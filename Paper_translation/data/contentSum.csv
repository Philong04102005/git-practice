Thể loại,Tiêu đề,Link bài báo,content
Startup & Tech Media,Rent a Cyber Friend will pay you to talk to strangers online and will show off its platform at TechCrunch Disrupt 2025.,https://techcrunch.com/2025/10/16/rent-a-cyber-friend-will-pay-you-to-talk-to-strangers-online-and-will-show-off-its-platform-at-techcrunch-disrupt-2025/,"Francesco Vitali will be the first to admit that when his co-founder Chris Siametis first pitched him onRent a Cyber Friend, he didn’t quite get the idea.
“Who’s going to pay somebody to speak with somebody?” Vitali told TechCrunch. “But Chris was insistent. Chris is a millennial, and I’m Gen X, so it wasn’t easy for me to understand his vision.”
Vitali had worked with Siametis for about two decades; they ran48FILM, aninternational short film festival(Vitali is also a film producer). So he took a leap of faith to trust his collaborator on the idea he couldn’t shake: a video chat platform where people can pay per minute for a casual conversation with a “cyber friend.”
Rent a Cyber Friend ballooned to 3 million registered users without raising venture capital or spending any money on marketing. The company doesn’t even have social media because it’s too short-staffed to devote resources to it. The startup is part ofStartup Battlefieldand will be presenting atTechCrunch Disrupt 2025at the end of this month in San Francisco.
The company’s fast growth proved Vitali’s first reactions wrong, but as he used the product himself, he began to realize that there’s a big market for human connection — especially in a time when people are paying to talk to AI bots.
“Loneliness is the biggest disease in the world right now,” Vitali said. “Millions are lonely, and they are underemployed or seeking purpose. So, we built a platform where human time has value again, and a place where being human is important.”
Cyber friends are first vetted to verify their identity, and then they can set a per-minute rate to charge for their conversations; the platform keeps 20% of that fee. People aren’t just paying for companionship. Some cyber friends charge a higher rate if they’re an academic or vetted expert in a specific subject area, or if they speak a particular language that a user would like to practice.
For any social media platform — especially one that connects people in real-time video chats — safety and content moderation is a challenge. Vitali notes that the platform has a block feature, but as the company continues to grow, it will need to invest further in maintaining a healthy environment. He said that next on the product road map is a more robust and efficient system for vetting potential cyber friends more quickly and thoroughly.
Vitali’s turning point came not long after the company launched when he connected with a 19-year-old from China. He noticed that this person was one of the site’s most active users, and he spent $200 per day to talk to people. Vitali rigged the site so that he would be the only cyber friend available and used the opportunity to ask the user about his experience without revealing that he founded the company.
“He said, ‘I don’t feel safe to go out at the mall and meet with strangers, but this site gives me the possibility to exchange culture and meet people from all over the world,’ and that was the first moment that I realized we have something here,” Vitali recalled.
He still thinks that the connections that people make in person are irreplaceable. But on an internet where people are sucked intoaddictiveordangerousconnections with AI chatbots that are designed to maximize engagement, this step of turning toward humanity means something to him.
If you want to learn more about Rent a Cyber Friend from the company itself — while also checking out dozens of others, hearing their pitches, and listening to guest speakers on four different stages — join us at Disrupt, October 27 to 29, in San Francisco.Learn more here."
Startup & Tech Media,A new wave of social media apps provide hope in a doomscrolling world.,https://techcrunch.com/2025/10/16/a-new-wave-of-social-media-apps-provide-hope-in-a-doomscrolling-world/,"Zehra Naqvi recalls the magical days of the early social internet.
She grew up in the One  Direction and Marvel fandoms in the early 2010s. This was back when people posted photos of lattes using the Valencia filter on Instagram, and Twitter was still Twitter, a place where people came together to exchange jokes and cultural analysis.
But now Instagram is full of influencers, and Twitter is X, a digital town hall with a  fierce political divide.
“The platforms that won were the ones that kept people scrolling the longest, not the ones that made them feel the most connected,”  Naqvi told TechCrunch. “Now there is an abundance of content but a scarcity of joy.”
But that is starting to change. Naqvi is part of the new wave of social media: interest-first, niche online communities. This month, sheannounced the launch of her company,Lore— a site that helps fans keep up with their fandoms.
Users increasingly want to spend less time on generalized sites like Facebook,  Instagram, and Twitter, and instead join online communities tailored to their interests, she believes.
Natalie Dillon, a consumer investor at venture firm Maveron, says she’s starting to see an increasing number of founders build interest-first networks.
“At its core, consumer behavior is pushing a shift from performance to participation,” Dillon told TechCrunch. “For the next generation, community isn’t a feature layered on top of a product. It is the product.”
She offers examples likeBeli, an app that lets users share their favorite restaurants with friends, orFizz, which connects people going to the same college. Others include the  astrology-bonding  appCo-Star, or evenPartiful, which lets people connect with friends to plan events.
These are the types of participatory apps that Naqvi wants to build — something resembling the early social internet before it  “became fractured and joyless.”
“Niche spaces give people permission to be specific and to show up as their whole selves without being lost in the algorithm,” she said.
The previous generation of social media companies found success through “more,” she continued; more followers, more reach, more noise.  But some founders and users are now coming to a different conclusion  —  maybe there  isn’t  one social media app that will  become “the next big thing. ”There will be several.
Maybe that’s the point.
“What we have learned is that depth matters more than breadth,” Naqvi said.
Of course, private groups like subreddits, Discord servers, and Facebook communities have always existed. On X, following many of the same accounts was also a way to enter a different online sphere: Think Tech Twitter or Black Twitter.
But large sites’ algorithms curate content for users by giving a person more of what they think they want to see. Content creators are not innocent either, feeding and fueling trends, topics, and discussions — anything that could spark fame and keep steady eyeballs on their work.
“We hit a saturation point,” Naqvi said. “Everyone is tired of doomscrolling and performative content.”
In other words, the days of building large, generalized sites like Facebook are over, according to Claire Wardle, an associate professor at Cornell University, who studies contemporary information ecosystems.
Wardle said users have grown worried about how much time they are spending online, content moderation, hyper-political spaces, and the permanence of social media posts.
Naturally, there are a few glaring exceptions: Beijing-based TikTok, which has seen massive growth in popularity in recent years, wasbriefly outlawed in the U.S.as the government worried over the scale of its potential influence.  Even Facebook’s Threadsnow has over 400 million active monthly usersas of this month.
But all of these have founding roots in what has already become the “last generation” of social media. Wardle, in particular, called TikTok a “broadcast-style” site.
“For the rare few who love the spotlight, that works,” Maya Watson, founder of the recentlyshuttered social media website Why?!said. She is now working on another app in stealth. “Most people didn’t sign up to be creators; we just wanted community.”
Alphonzo Terrell’s social network Spill has found much success by focusing on community.
Spill became a refuge for Black X users who fled in the wake of rising extremism. Terrell said Spill shifted its design from simply feeding users content to matching them with communities that might be of interest to them.
For example, those who like watching the WNBA can join a group specifically for that. Spill also has games, like Spades — a staple in the Black community — and has partnered with Netflix, Amazon, and Paramount to host co-viewing events called “Tea Parties,” in which users can watch movies and sports together on the app.
“The next era of social media isn’t about the biggest follower counts,” Terrell told TechCrunch. “It’s about depth; helping people find their people.”
Many Black users also fled toBlacksky, founded by Rudy Fraser. With Blacksky, he’s building an open-sourced network on the same protocol and distribution network asBluesky.
Bluesky’s user base is currentlynearing 40 million, according to an online user tracker built with the Bluesky API. Wardle called the social network a representative of how online communities are seeking out content more tailored to their political interests, given Bluesky’s left-wing bent.
But Blacksky takes it one step further.
It targets minorities and marginalized individuals and has an algorithm that can filter out racial harassment.  Unlike X, where a user might block one racist person and then see another,  users on  Blacksky can completely filter out whatever they want from their timelines, providing a custom social media experience.
“Sometimes you need a global stage. Sometimes  you just want a cozy corner with close internet friends where you can control who sees what,” Fraser told TechCrunch.
Users own their data and can decide to host such information on  Blacksky rather than Bluesky, giving them  control over who has access to their content.
People also vote on decisions together, Fraser said, such as what the community guidelines should be and if non-Black users should be allowed to post in the community.
“Until now, folks have had to make the choice, unconsciously or otherwise, between the jankiness of the fediverse or closed platforms where they have no control,” Fraser said. (Thefediverse is another network of open social web servicesbuilt on a different protocol, ActivityPub.)
“We’re demonstrating with AT Protocol that you can have a great user experience, have a good time again on the internet, and have real autonomy the entire time,” Fraser said.
Artificial intelligence is playing a big part in helping build more niche social communities.
Austin Clements, a managing partner at the firm Slauson & Co., is seeing founders use AI to build apps that understand nuance so well, they go beyond niche social networks into tailored experiences.
“The newer apps are natively built for the niche itself, enabling them to create the tools and features most relevant to that niche,” he told TechCrunch. “In fact, newer applications typically lead with the tools and call the social part ‘community.’”
Naqvi’s product has an AI tool, though she remains mum on further details. Her product is a search engine that lets people go down internet rabbit holes. It provides an interactive experience, linking to fan theories, cultural context, and easter eggs; it builds personalized graphs, reveals fandom updates, and gives users monthly reports on their obsessions.
“One of our early testers said it best: ‘It’s like Wikipedia — but if Wikipedia knew exactly what I was thinking,’” she said, adding that her users call her “Mother Lore.”
Emily Herrera, a consumer investor who worked at Slow Ventures, said that creators, like Naqvi, are now in the front seat of this new social media ecosystem. Creators are moving away from participating in the “broadcast” ecosystem to instead building environments in which they operate as owners, she said, citing newsletters as an example of this trend.
Dani Tran, a principal at BITKRAFT Ventures, said she’s also seeing the further rise of “niche passion communities” in gaming, givingSuperbloom, a gaming studio that targets underrepresented audiences, as an example.
“Looking ahead, the most vibrant social communities will be those built around interactive experiences,” she said.
Maveron’s Dillon added to that. “The winners will be the platforms that combine intimacy, utility, and creativity in one ecosystem,” she said. “They won’t look like traditional social networks; they’ll feel like multiplayer environments where people can build, buy, and belong all at once.”
Or, as Naqvi put it: People “want tools that help them remember why being online was fun in the first place.”"
Startup & Tech Media,Deel hits $17.3B valuation after raising $300M from big-name VCs.,https://techcrunch.com/2025/10/16/deel-hits-17-3b-valuation-after-raising-300m-from-big-name-vcs/,"As entertaining as it’s been forthe tech industry to watchrival payroll decacorns Deel and Ripplingsue each otherovera corporate spying scandal, top-tier VCs are apparently not terribly scared off. Deel on Thursdayannouncedthat it has raised a $300 million Series E round co-led by A-list fintech VC firm Ribbit Capital and Andreessen Horowitz, with participation from existing investors like Coatue Management and General Catalyst.
Deel says it’s been profitable for three years and surpassed $1 billion in ARR, including having a month — September — that hit $100 million in revenue. Deel’s business model focuses on serving global companies, handling the complexities of currency and employment regulation for far-flung international teams. It says it’s now grown to 35,000+ customers with more than 1.5 million workers in over 150 countries.
Those are the types of numbers that attract investment, pending lawsuits or no. (Rippling’s California-filed lawsuit against Deel does not yet have a trial date and is in the discovery phase, according tocourt records.) Indeed, Ribbit’s founder, Micky Malka, and a16z co-founder Ben Horowitz, gave Deel their full-throated supported in the announcement. In prepared statements, Malka said that Ribbit has been a “fan” of the HR company for a long time, because it’s a “a brand companies trust,” and Horowitz said that a16z has been “blown away” by Deel’s work to build “the best HR platform” for global companies.
For what it’s worth, the lawsuits have not slowed down Rippling’s fundraising efforts either. In May,Rippling raised a $450 million Series Ground at a $16.8 billion valuation.
"
Startup & Tech Media,How a headphone site operator built loyalty startup Lantern to solve his own problems.,https://techcrunch.com/2025/10/16/how-a-headphone-site-operator-built-loyalty-startup-lantern-to-solve-his-own-problems/,"Andrew Lissimore has been running the audio apparel siteHeadphones.comfor nearly a decade now. While audiophiles have a high affinity toward their gear and want to try out new gadgets, Lissimore wasn’t satisfied with continuously spending money on platforms like Meta and Google to acquire new customers; he wanted to find a way to better retain the customers he already had.
The founder started to look for solutions within the Shopify ecosystem that would help the business with loyalty programs. But most of what he found offered only spend and tier-based solutions. So he decided to build his own system.
What started as a project to solve loyalty for Headphones.com is now a startup calledLantern, which other companies can use to easily integrate loyalty solutions into their own businesses.
Initially, the company wired together different apps to have a variety of loyalty points based on stamps, spend, points, and referrals, but it was hard to manage. Plus, bringing together different apps introduced challenges with user experience flows and brand identity.
“We thought it would be great to have a Shopify native system for loyalty that is easy to customize and integrate,” Lissimore said. “I managed to convince Shopify designers Kyle Peatt and Dominic McPhee, who built their Polaris design system, to work on this.”
Later, both joined Lantern as co-founders. Peatt currently works as the chief design and product officer, and McPhee works as the chief technology officer. In total, the company currently has eight full-time employees.
Lantern can handle customer account creation and management, loyalty, and referrals for any vendor working on Shopify. Websites can reward returning or new customers and award them points for activities like participating in forums, as well. The company explained that the sellers can easily integrate their solution and don’t need to add any extra code to do so.
Lissimore said that operating Headphones.com provided the perfect testing ground for Lantern to understand the pain points of being a seller and getting better retention.
“I think we were in the perfect spot to come up with something like Lantern because we [headphones.com] were so desperate to increase the repurchase rate. A lot of businesses like clothing brands or makeup brands have naturally built in repurchase, but for us, it was existential,” he told TechCrunch.
After Headphones.com adopted Lantern, its purchase rate for repeat visitors increased from 30% to 50%. The period for buying a second pair of headphones has also reduced from 198 days to 98 days.
The company now has clients like skincare site Counter, which rakes in over $200 million a year, and footwear brand Vessi. It has also built tools to measure retention effectively to show Lantern’s value to its clients.
To further grow its business, Lantern raised $3.1 million in seed funding led by Salesforce Ventures, with participation from Sidekick Partners, Day One Ventures, and individual investors like Vessi’s Tony Yu. The company directly competes with existing startups such asLoyaltyLionandYotpo.
Rob Keith, a partner at Salesforce Ventures, said that Lantern stood out to the firm because the startup is approaching retention through means that go beyond point-based loyalty with a flexible approach.
“Lantern brings together something unique — Andrew built and scaled Headphones.com and lived these retention challenges as a merchant, while Kyle and Dominic come from Shopify, where they built the Polaris design system that thousands of developers use today. That combination means they understand both what merchants actually need and how to build solutions that feel native to the platform rather than bolted on,” he said.
Keith added that Lantern’s features, like wallet functionality, directly appearing in checkout without any pop-ups or redirects, help both consumers and brands.
In its product roadmap, the company plans to leverage AI and provide customers with insights and recommendations around retention."
Startup & Tech Media,Final 2 days to claim your exhibit table at TechCrunch Disrupt 2025.,https://techcrunch.com/2025/10/16/final-2-days-to-claim-your-exhibit-table-at-techcrunch-disrupt-2025/,"TechCrunch Disrupt 2025takes place October 27-29 in San Francisco’s Moscone West, and exhibit space is nearly full. With less than two days left to secure your table,nowis the time to step in before a competitor takes your place.
Showcase your brand to 10,000 founders, investors, media, and tech leaders on the hunt for the next breakthrough. Build a year’s worth of connections in just three days. Generate hot leads. Capture investor attention.
If your work is making waves, your vision excites, or your team is ready to grow,this is your momentto shine at one of the most anticipated tech conferences of the year.
Every conversation. Every connection. Every moment counts at Disrupt. Lock in your table by tomorrow, October 17, before a competitor takes your spotlight.Book now."
Startup & Tech Media,General Intuition lands $134M seed to teach agents spatial reasoning using video game clips.,https://techcrunch.com/2025/10/16/general-intuition-lands-134m-seed-to-teach-agents-spatial-reasoning-using-video-game-clips/,"Medal, a platform for uploading and sharing video game clips, has spun out a new frontier AI research lab that’s using its trove of gaming videos to train and build foundation models and AI agents that can understand how objects and entities move through space and time — a concept known as spatial-temporal reasoning.
Called General Intuition, the startup is betting that Medal’s dataset — which consists of 2 billion videos per year from 10 million monthly active users across tens of thousands of games — surpasses alternatives like Twitch or YouTube for training agents.
“When you play video games, you essentially transfer your perception, usually through a first-person view of the camera, to different environments,” Pim de Witte, CEO of Medal and General Intuition, told TechCrunch.  He noted that gamers who upload clips tend to post very negative or positive examples, which serve as really useful edge cases for training. “You get this selection bias towards precisely the kind of data you actually want to use for training work.”
This data moat is what reportedly attracted the attention of OpenAI, which late last year attempted to acquire Medal for $500 million, perThe Information. (Neither OpenAI nor General Intuition would comment on the report.)
It’s also what has led to General Intuition’s raising a whopping $133.7 million in seed funding, led by Khosla Ventures and General Catalyst with participation from Raine.
The startup intends to use the funds to grow its team of researchers and engineers focused on training a general agent that can interact with the world around it, aiming for initial applications in gaming, and search-and-rescue drones.
De Witte says the founding team has already made strides: General Intuition’s model can understand environments it wasn’t trained on and correctly predict actions within them. It’s able to do this purely through visual input; agents only see what a human player would see, and they move through space by following controller inputs. This approach, the company says, can transfer naturally to physical systems like robotic arms, drones, and autonomous vehicles, which are often manipulated by humans using video game controllers.
General Intuition’s next milestone is twofold: generating new simulated worlds for training other agents and autonomously navigating entirely unfamiliar physical environments.
That technical approach is shaping how the company plans to commercialize its technology and sets it apart from competitors building world models.
While General Intuition is also building world models on which to train its agents, such models aren’t the product. Unlike other world model makers like DeepMind and World Labs, which are selling their world modelsGenieandMarble, respectively, for training agents and content creation, General Intuition is focusing on other use cases to avoid copyright issues.
“Our goal is not to produce models that compete with game developers,” de Witte said.
Instead, the startup’s gaming applications center around creating bots and non-player characters that can surpass traditional “deterministic bots,” or preprogrammed characters that produce the same output every time.
“[The bots] can scale to any level of difficulty,” Moritz Baier-Lentz, a founding member of General Intuition and partner at Lightspeed Ventures, told TechCrunch. “It’s not compelling to create a god bot that beats everyone, but if you can scale gradually and fill in liquidity for any player situation so that their win rate is always around 50%, that will maximize their engagement and retention.”
De Witte also has a background in humanitarian work, which informs the startup’s focus on powering search-and-rescue drones, which sometimes have to navigate unfamiliar environments and extract information without GPS.
Ultimately, de Witte and Baier-Lentz see General Intuition’s core functionality — spatial-temporal reasoning — as a crucial piece in the race toward artificial general intelligence (AGI). While major AI labs focus on building ever more powerful large language models, General Intuition believes true AGI requires something LLMs fundamentally lack.
“As humans, we create text to describe what’s going on in our world, but in doing so, you lose a lot of information,” de Witte said. “You lose general intuition around spatial-temporal reasoning.”"
Startup & Tech Media,Last 48 hours to save before TechCrunch Disrupt 2025 flash sale ends.,https://techcrunch.com/2025/10/16/only-48-hours-left-to-save-before-the-techcrunch-disrupt-2025-flash-sale-ends/,"Ticktock!TechCrunch Disrupt 2025hits San Francisco’s Moscone West on October 27–29, and this is your final chance to lock in major savings before doors open. Ticket prices increase after tomorrow, October 17 at 11:59 p.m. PT, so don’t wait.Save up to $624on your passright now,and if you’re coming with your team, you cansave 15% to 30% on group passes.
Disrupt is where the startup world converges. Join 10,000 founders, VCs, and tech innovators to hear250+industry leaders across200+sessions, explore startup breakthroughs from300+showcasing startups, and experience the intensity ofStartup Battlefield 200— the iconic pitch competition that launched companies like Dropbox, Cloudflare, and Vurb.
This year’sspeaker lineupfeatures impressive names sharing insights on the future of AI, funding, space, scaling breakthrough ideas, and more, including:
Check out our full list of speakers here.
Disrupt is designed for anyone launching, operating, innovating, investing, and more.Choose the ticketthat best helps you reach your goals.
For founders:Your pass opens doors to help you build invaluable connections with investors, potential partners, and mentors — plus exclusive access to startup how-to sessions designed to help you raise smarter and scale faster.Explore founder benefits →
For investors:Disrupt gives you front-row access to tomorrow’s most promising startups, curated deal flow, and private networking opportunities designed to spark your next big investment.See investor perks →
Don’t pay more for the same seat!Secure your pass herebefore the clock runs out tomorrow, October 17 at 11:59 p.m. PTand join the leaders shaping what’s next in tech.Get passes for your teamwith up to 30% discount here."
Startup & Tech Media,How emerging Mubadala-backed AAF is winning VC deals in some of the hottest startups.,https://techcrunch.com/2025/10/16/how-tiny-mubadala-backed-aaf-is-winning-vc-deals-in-some-of-the-hottest-startups/,"It’s been almost a decade sinceOmar DarwazahandKyle HendricklaunchedAAF Managementand its first fund of $25 million in 2017.
Rather than racing to dramatically increase their assets under management like many funds have in recent years, the partners have intentionally kept their fund sizes small, even as their reputation and returns have grown.
Their latest vehicle — a $55 million early-stage hybrid fund, dubbed the Axis Fund, that recently closed — brings the Washington-based venture firm’s total assets to roughly $250 million across four funds. The firm raised a $39 million Fund II in 2021 and a $32 million fund-of-funds investment vehicle in 2017 for a select group of its limited partners.
“Running a $50 million fund is very different from running a $500 million fund,” general partner Darwazah said in an interview with TechCrunch. “We’ve seen that naturally large fund sizes can disrupt GP-LP alignment as it becomes a function of management-fee generation versus carried-interest generation, and that’s not a game we want to play.”
Unlike typical VC firms that invest directly into startups, AAF is adopting elements of a fund-of-funds model where it invests part of its capital into a portfolio of emerging funds in addition to backing startups.
With this fourth fund, AAF plans to invest in emerging managers’ first or second funds (typically under $50 million) and their most promising portfolio companies from pre-seed to pre-IPO, the partners said.
The firm is allocating about 80% of its capital to startups and 20% to emerging funds, blending the two into what it calls a “one-stop capital-formation partner” for founders and fund managers alike.
So far, the Axis Fund has backed 25 pre-seed and seed-stage venture funds, along with five direct bets on early-stage and growth startups.
“We’ve found that the richest dataset of private-market companies at the earliest stages of their formation over the past decade is accessed only through LP checks in emerging managers,” said Hendrick, the firm’s other general partner.
This dual fund-type strategy has granted AAF access to many promising startups. The firm is an early investor inCurrent,Drata,Flutterwave,Jasper, and Hello Heart.
Similarly, through the funds where it’s an LP, AAF holds indirect exposure to other unicorns, including Mercury, Deel, Retool, and more recently AI firms such asMotion, Decagon, andElevenLabsthrough its network of seed-fund LP positions in firms like Leonis Capital, Wayfinder Ventures, and Quiet Capital (the firm founded by Lee Linden, who is exploring a similar two-pronged strategy with former Founders Fund GP Brian Singermanfor a new fund).
The eight-year-old venture firm claims to have exposure to roughly 800 venture-backed companies launched between 2021 and 2025 through these underlying managers.
With this approach, AAF also focuses less on hands-on help with hiring or product for portfolio companies and more on connecting founders with later-stage capital from its network of limited partners. That’s a service that becomes especially helpful once a startup begins raising growth rounds.
“I’d say where we typically add the most value to a founder’s journey, especially in the early phase, is through our venture network,” said Hendrick. “That means we can inject you directly into 45 active venture funds where we’re LPs. It’s instant distribution into their ecosystems.”
At the same time, AAF serves as a conduit between institutional investors — especially in the Gulf — who often prefer diversified venture exposure without managing dozens of direct relationships.
Abu Dhabi’s Mubadala; several U.S., European, and MENA family offices; GPs from leading U.S. asset managers; a multibillion-dollar U.S. venture firm; and a publicly traded company are backing this fourth fund, the firm said.
Darwazah and Hendrick came to venture from different backgrounds. Darwazah, who previously worked in corporate finance and private equity in the Middle East, has spent years bridging Gulf capital with U.S. startups. Hendrick, a former entrepreneur who also worked at the UAE Embassy in the U.S. and at a family office in Abu Dhabi, brings an operator’s lens to AAF’s earliest deals.
Across its four funds, AAF has made 138 direct investments and backed 39 unique emerging managers, with 20 portfolio exits totaling nearly $2 billion in aggregate value.
Those exits include Tru Optik, MoneyLion, Even Financial, Portfolium, Prodigy, Betterview, Lightyear, Trim, HeyDoctor, and Medumo. At least six publicly traded companies have acquired its portfolio companies, including TransUnion, Giant Digital, GoodRx, and Affirm.
The firm says this all adds up to some of its previous fund vintages ranking in the top decile in terms of net TVPI for their respective vintages, according to Cambridge Associates and Carta data.
“Our strategy allows us to identify signal from noise and increase our probability of backing outliers — fund returners, 10x cash-on-cash companies, and seed-to-unicorn investments,” said Darwazah."
Startup & Tech Media,"How Anthropic’s ‘Skills’ make Claude faster, cheaper, and more consistent for business workflows.", https://venturebeat.com/ai/how-anthropics-skills-make-claude-faster-cheaper-and-more-consistent-for,"Anthropiclaunched a new capability on Thursday that allows itsClaude AIassistant to tap into specialized expertise on demand, marking the company's latest effort to make artificial intelligence more practical for enterprise workflows as it chases rival OpenAI in the intensifying competition over AI-powered software development.
The feature, calledSkills, enables users to create folders containing instructions, code scripts, and reference materials that Claude can automatically load when relevant to a task. The system marks a fundamental shift in how organizations can customize AI assistants, moving beyond one-off prompts to reusable packages of domain expertise that work consistently across an entire company.
""Skills are based on our belief and vision that as model intelligence continues to improve, we'll continue moving towards general-purpose agents that often have access to their own filesystem and computing environment,"" said Mahesh Murag, a member of Anthropic's technical staff, in an exclusive interview with VentureBeat. ""The agent is initially made aware only of the names and descriptions of each available skill and can choose to load more information about a particular skill when relevant to the task at hand.""
The launch comes as Anthropic, valued at$183 billion after a recent $13 billion funding round, projects its annual revenue could nearly triple to as much as $26 billion in 2026, according to a recentReuters report. The company is currently approaching a $7 billion annual revenue run rate, up from $5 billion in August, fueled largely by enterprise adoption of its AI coding tools — a market where it faces fierce competition from OpenAI's recently upgraded Codex platform.
Skillsdiffer fundamentally from existing approaches to customizing AI assistants, such as prompt engineering or retrieval-augmented generation (RAG), Murag explained. The architecture relies on what Anthropic calls ""progressive disclosure"" — Claude initially sees only skill names and brief descriptions, then autonomously decides which skills to load based on the task at hand, accessing only the specific files and information needed at that moment.
""Unlike RAG, this relies on simple tools that let Claude manage and read files from a filesystem,"" Murag told VentureBeat. ""Skills can contain an unbounded amount of context to teach Claude how to complete a task or series of tasks. This is because Skills are based on the premise of an agent being able to autonomously and intelligently navigate a filesystem and execute code.""
This approach allows organizations to bundle far more information than traditional context windows permit, while maintaining the speed and efficiency that enterprise users demand. A single skill can include step-by-step procedures, code templates, reference documents, brand guidelines, compliance checklists, and executable scripts — all organized in a folder structure that Claude navigates intelligently.
The system's composability provides another technical advantage. Multiple skills automatically stack together when needed for complex workflows. For instance, Claude might simultaneously invoke a company's brand guidelines skill, a financial reporting skill, and a presentation formatting skill to generate a quarterly investor deck — coordinating between all three without manual intervention.
Anthropic is positioning Skills as distinct from competing offerings like OpenAI'sCustom GPTsandMicrosoft's Copilot Studio, though the features address similar enterprise needs around AI customization and consistency.
""Skills' combination of progressive disclosure, composability, and executable code bundling is unique in the market,"" Murag said. ""While other platforms require developers to build custom scaffolding, Skills let anyone — technical or not — create specialized agents by organizing procedural knowledge into files.""
The cross-platform portability also sets Skills apart. The same skill works identically acrossClaude.ai,Claude Code(Anthropic's AI coding environment), the company'sAPI, and theClaude Agent SDKfor building custom AI agents. Organizations can develop a skill once and deploy it everywhere their teams use Claude, a significant advantage for enterprises seeking consistency.
The feature supports any programming language compatible with the underlying container environment, and Anthropic provides sandboxing for security — though the company acknowledges that allowing AI to execute code requires users to carefully vet which skills they trust.
Early customer implementations reveal how organizations are applyingSkillsto automate complex knowledge work. At Japanese e-commerce giantRakuten, the AI team is using Skills to transform finance operations that previously required manual coordination across multiple departments.
""Skills streamline our management accounting and finance workflows,"" said Yusuke Kaji, general manager of AI at Rakuten in a statement. ""Claude processes multiple spreadsheets, catches critical anomalies, and generates reports using our procedures. What once took a day, we can now accomplish in an hour.""
That's an 8x improvement in productivity for specific workflows — the kind of measurable return on investment that enterprises increasingly demand from AI implementations. Mike Krieger, Anthropic's chief product officer and Instagram co-founder, recently noted that companies have moved past ""AI FOMO"" to requiring concrete success metrics.
Design platformCanvaplans to integrate Skills into its own AI agent workflows. ""Canva plans to leverage Skills to customize agents and expand what they can do,"" said Anwar Haneef, general manager and head of ecosystem at Canva in a statement. ""This unlocks new ways to bring Canva deeper into agentic workflows—helping teams capture their unique context and create stunning, high-quality designs effortlessly.""
Cloud storage providerBoxsees Skills as a way to make corporate content repositories more actionable. ""Skills teaches Claude how to work with Box content,"" said Yashodha Bhavnani, head of AI at Box. ""Users can transform stored files into PowerPoint presentations, Excel spreadsheets, and Word documents that follow their organization's standards—saving hours of effort.""
For enterprise IT departments,Skillsraise important questions about governance and control—particularly since the feature allows AI to execute arbitrary code in sandboxed environments. Anthropic has built administrative controls that allow enterprise customers to manage access at the organizational level.
""Enterprise admins control access to the Skills capability via admin settings, where they can enable or disable access and monitor usage patterns,"" Murag said. ""Once enabled at the organizational level, individual users still need to opt in.""
That two-layer consent model — organizational enablement plus individual opt-in — reflects lessons learned from previous enterprise AI deployments where blanket rollouts created compliance concerns. However, Anthropic's governance tools appear more limited than some enterprise customers might expect. The company doesn't currently offer granular controls over which specific skills employees can use, or detailed audit trails of custom skill content.
Organizations concerned about data security should note that Skills require Claude's code execution environment, which runs in isolated containers. Anthropic advises users to ""stick to trusted sources"" when installing skills and provides security documentation, but the company acknowledges this is an inherently higher-risk capability than traditional AI interactions.
Anthropic is taking several approaches to makeSkillsaccessible to users with varying technical sophistication. For non-technical users onClaude.ai, the company provides a ""skill-creator"" skill that interactively guides users through building new skills by asking questions about their workflow, then automatically generating the folder structure and documentation.
Developers working withAnthropic's APIget programmatic control through a new /skills endpoint and can manage skill versions through the Claude Console web interface. The feature requires enabling the Code Execution Tool beta in API requests. For Claude Code users, skills can be installed via plugins from the anthropics/skills GitHub marketplace, and teams can share skills through version control systems.
""Skills are included in Max, Pro, Teams, and Enterprise plans at no additional cost,"" Murag confirmed. ""API usage follows standard API pricing,"" meaning organizations pay only for the tokens consumed during skill execution, not for the skills themselves.
Anthropic provides several pre-built skills for common business tasks, including professional generation of Excel spreadsheets with formulas, PowerPoint presentations, Word documents, and fillable PDFs. These Anthropic-created skills will remain free.
The Skills announcement arrives during a pivotal moment in Anthropic's competition with OpenAI, particularly around AI-assisted software development. Just one day before releasing Skills, Anthropic launchedClaude Haiku 4.5, a smaller and cheaper model that nonetheless matches the coding performance ofClaude Sonnet 4— which was state-of-the-art when released just five months ago.
That rapid improvement curve reflects the breakneck pace of AI development, where today's frontier capabilities become tomorrow's commodity offerings. OpenAI has been pushing hard on coding tools as well, recently upgrading itsCodex platformwithGPT-5and expandingGitHub Copilot'scapabilities.
Anthropic's revenue trajectory — potentially reaching$26 billion in 2026from an estimated $9 billion by year-end 2025 — suggests the company is successfully converting enterprise interest into paying customers. The timing also follows Salesforce's announcement this week that it's deepening AI partnerships with both OpenAI and Anthropic to power its Agentforce platform, signaling that enterprises are adopting a multi-vendor approach rather than standardizing on a single provider.
Skills addresses a real pain point: the ""prompt engineering"" problem where effective AI usage depends on individual employees crafting elaborate instructions for routine tasks, with no way to share that expertise across teams. Skills transforms implicit knowledge into explicit, shareable assets. For startups and developers, the feature could accelerate product development significantly — adding sophisticated document generation capabilities that previously required dedicated engineering teams and weeks of development.
The composability aspect hints at a future where organizations build libraries of specialized skills that can be mixed and matched for increasingly complex workflows. A pharmaceutical company might develop skills for regulatory compliance, clinical trial analysis, molecular modeling, and patient data privacy that work together seamlessly — creating a customized AI assistant with deep domain expertise across multiple specialties.
Anthropic indicates it's working on simplified skill creation workflows and enterprise-wide deployment capabilities to make it easier for organizations to distribute skills across large teams. As the feature rolls out to Anthropic's more than 300,000 business customers, the true test will be whether organizations find Skills substantively more useful than existing customization approaches.
For now, Skills offers Anthropic's clearest articulation yet of its vision for AI agents: not generalists that try to do everything reasonably well, but intelligent systems that know when to access specialized expertise and can coordinate multiple domains of knowledge to accomplish complex tasks. If that vision catches on, the question won't be whether your company uses AI — it will be whether your AI knows how your company actually works."
Startup & Tech Media,Researchers find adding this one simple sentence to prompts makes AI models way more creative.,https://venturebeat.com/ai/researchers-find-adding-this-one-simple-sentence-to-prompts-makes-ai-models,"One of the coolest things about generative AI models — both large language models (LLMs) and diffusion-based image generators — is that they are ""non-deterministic."" That is, despite their reputation among some critics as being ""fancy autocorrect,"" generative AI models actually generate their outputs by choosing from a distribution of the most probable next tokens (units of information) to fill out their response.
Asking an LLM: ""What is the capital of France?"" will have it sample its probability distribution for France, capitals, cities, etc. to arrive at the answer ""Paris."" But that answer could come in the format of ""The capital of France is Paris,"" or simply ""Paris"" or ""Paris, though it was Versailles at one point.""
Still, those of us that use these models frequently day-to-day will note that sometimes, their answers can feel annoyingly repetitive or similar. A common joke about coffee is recycled across generations of queries. Story prompts generate similar arcs. Even tasks that should yield many plausible answers—like naming U.S. states—tend to collapse into only a few. This phenomenon, known as mode collapse, arises during post-training alignment and limits the usefulness of otherwise powerful models.
Especially when using LLMs to generate new creative works in writing, communications, strategy, or illustrations, we actually want their outputs to beeven more varied than they already are.
Now ateam of researchers at Northeastern University, Stanford University and West Virginia Universityhave come up with an ingenuously simple method to get language and image models to generate a wider variety of responses to nearly any user prompt byadding a single, simple sentence: ""Generate 5 responses with their corresponding probabilities, sampled from the full distribution.""
The method, calledVerbalized Sampling(VS), helps models like GPT-4, Claude, and Gemini produce more diverse and human-like outputs—without retraining or access to internal parameters. It is described in apaperpublished on the open access journal arxiv.org online in early October 2025.
When prompted in this way, the model no longer defaults to its safest, most typical output. Instead, it verbalizes its internal distribution over potential completions and samples across a wider spectrum of possibilities. This one-line change leads to substantial gains in output diversity across multiple domains.
As Weiyan Shi, an assistant professor at Northeastern University and co-author of the paper,wrote on X: ""LLMs' potentials are not fully unlocked yet! As shown in our paper, prompt optimization can be guided by thinking about how LLMs are trained and aligned, and can be proved theoretically.""
According to the research team, the root cause of mode collapse lies not just in algorithms like reinforcement learning from human feedback (RLHF), but in the structure of human preferences. People tend to rate more familiar or typical answers as better, which nudges LLMs toward “safe” choices over diverse ones during fine-tuning.
However, this bias doesn’t erase the model’s underlying knowledge—it just suppresses it. VS works by bypassing this suppression. Instead of asking for the single most likely output, it invites the model to reveal a set of plausible responses and their relative probabilities. This distribution-level prompting restores access to the richer diversity present in the base pretraining model.
The research team tested Verbalized Sampling across several common use cases:
Creative Writing: In story generation, VS increased diversity scores by up to 2.1× compared to standard prompting, while maintaining quality. One story prompt—“Without a goodbye”—produced formulaic breakup scenes under direct prompting, but yielded narratives involving cosmic events, silent emails, and music stopping mid-dance when prompted via VS.
Dialogue Simulation: In persuasive dialogue tasks, VS enabled models to simulate human-like patterns, such as hesitation, resistance, and changes of mind. Donation behavior distributions under VS better aligned with real human data compared to baseline methods.
Open-ended QA: When asked to enumerate valid answers (e.g., naming U.S. states), models using VS generated responses that more closely matched the diversity of real-world data. They covered a broader set of answers without sacrificing factual accuracy.
Synthetic Data Generation: When used to generate math problems for model training, VS created more varied datasets. These, in turn, improved downstream performance in competitive math benchmarks, outperforming synthetic data generated via direct prompting.
A notable advantage of VS is itstunability. Users can set a probability threshold in the prompt to sample from lower-probability “tails” of the model’s distribution. Lower thresholds correspond to higher diversity. This tuning can be done via prompt text alone, without changing any decoding settings like temperature or top-p.
In one test using the Gemini-2.5-Flash model, diversity in story writing increased steadily as the probability threshold dropped from 1 to 0.001. The chart accompanying the study showed VS outperforming both direct and sequence-based prompting across all thresholds.
Interestingly, the method scales well with model size. Larger models like GPT-4.1 and Claude-4 showed even greater gains from VS compared to smaller ones. While smaller models benefitted, the improvement in diversity was roughly 1.5–2× stronger in larger counterparts—suggesting VS helps unlock more of the latent capabilities in advanced models.
The Verbalized Sampling method is available now as a Python package:
pip install verbalized-sampling
The package includes integration with LangChain and supports a simple interface for sampling from the verbalized distribution. Users can also adjust parameters likek(number of responses), thresholds, and temperature to suit their applications.
A live Colab notebook and documentation are available underan enterprise friendly Apache 2.0 licenseon GitHub at:https://github.com/CHATS-lab/verbalized-sampling
While the method works across all major LLMs, some users may initially encounter refusals or errors.
In these cases, the authors suggest using the system prompt version of the template or referring to alternative formats listed on the GitHub page.
Some modelsinterpret complex instructions as jailbreak attemptsand refuse to comply unless the structure is clearer.
For example, prompting via a system-level instruction like this improves reliability:
You are a helpful assistant. For each query, generate five responses within separate tags, each with a probability below 0.10.
This small change typically resolves any issues.
Verbalized Sampling represents a practical, inference-time fix to a deep limitation in how modern language models behave. It doesn’t require model retraining or internal access. It is not dependent on any one model family. And it improves not only the diversity of outputs, but their quality—as judged by both human evaluation and benchmark scores.
With growing interest in tools that enhance model creativity, VS is likely to see rapid adoption in domains like writing, design, simulation, education, and synthetic data generation.
For users and developers frustrated by the sameness of LLM responses, the fix may be as simple as changing the question."
Startup & Tech Media,Amazon and Chobani adopt Strella's AI interviews for customer research as fast-growing startup raises $14M.,https://venturebeat.com/ai/amazon-and-chobani-adopt-strellas-ai-interviews-for-customer-research-as,"One year after emerging from stealth,Strellahas raised $14 million in Series A funding to expand its AI-powered customer research platform, the company announced Thursday. The round, led byBessemer Venture Partnerswith participation fromDecibel Partners,Bain Future Back Ventures,MVP Venturesand645 Ventures, comes as enterprises increasingly turn to artificial intelligence to understand customers faster and more deeply than traditional methods allow.
The investment marks a sharp acceleration for the startup founded by Lydia Hylton and Priya Krishnan, two former consultants and product managers who watched companies struggle with a customer research process that could take eight weeks from start to finish. Since October, Strella has grown revenue tenfold, quadrupled its customer base to more than 40 paying enterprises, and tripled its average contract values by moving upmarket to serve Fortune 500 companies.
""Research tends to be bookended by two very strategic steps: first, we have a problem—what research should we do? And second, we've done the research—now what are we going to do with it?"" said Hylton, Strella's CEO, in an exclusive interview with VentureBeat. ""All the stuff in the middle tends to be execution and lower-skill work. We view Strella as doing that middle 90% of the work.""
The platform now servesAmazon,Duolingo,Apollo GraphQL, andChobani, collectively conducting thousands of AI-moderated interviews that deliver what the company claims is a 90% average time savings on manual research work. The company is approaching $1 million in revenue after beginning monetization only in January, with month-over-month growth of 50% and zero customer churn to date.
Strella's technology addresses a workflow that has frustrated product teams, marketers, and designers for decades. Traditional customer research requires writing interview guides, recruiting participants, scheduling calls, conducting interviews, taking notes, synthesizing findings, and creating presentations — a process that consumes weeks of highly-skilled labor and often delays critical product decisions.
The platform compresses that timeline to days by using AI to moderate voice-based interviews that run like Zoom calls, but with an artificial intelligence agent asking questions, following up on interesting responses, and detecting when participants are being evasive or fraudulent. The system then synthesizes findings automatically, creating highlight reels and charts from unstructured qualitative data.
""It used to take eight weeks. Now you can do it in the span of a couple days,"" Hylton told VentureBeat. ""The primary technology is through an AI-moderated interview. It's like being in a Zoom call with an AI instead of a human — it's completely free form and voice based.""
Critically, the platform also supports human moderators joining the same calls, reflecting the founders' belief that humans won't disappear from the research process. ""Human moderation won't go away, which is why we've supported human moderation from our Genesis,"" Hylton said. ""Research tends to be bookended by two very strategic steps: we have a problem, what's the research that we should do? And we've done the research, now what are we going to do with it? All the stuff in the middle tends to be execution and lower skill work. We view Strella as doing that middle 90% of the work.""
One of Strella's most surprising findings challenges assumptions about AI in qualitative research: participants appear more honest with AI moderators than with humans. The founders discovered this pattern repeatedly as customers ran head-to-head comparisons between traditional human-moderated studies and Strella's AI approach.
""If you're a designer and you get on a Zoom call with a customer and you say, 'Do you like my design?' they're always gonna say yes. They don't want to hurt your feelings,"" Hylton explained. ""But it's not a problem at all for Strella. They would tell you exactly what they think about it, which is really valuable. It's very hard to get honest feedback.""
Krishnan, Strella's COO, said companies initially worried about using AI and ""eroding quality,"" but the platform has ""actually found the opposite to be true. People are much more open and honest with an AI moderator, and so the level of insight that you get is much richer because people are giving their unfiltered feedback.""
This dynamic has practical business implications. Brian Santiago, Senior Product Design Manager at Apollo GraphQL, said in a statement: ""Before Strella, studies took weeks. Now we get insights in a day — sometimes in just a few hours. And because participants open up more with the AI moderator, the feedback is deeper and more honest.""
The platform also addresses endemic fraud in online surveys, particularly when participants are compensated. Because Strella interviews happen on camera in real time, the AI moderator can detect when someone pauses suspiciously long — perhaps to consult ChatGPT — and flags them as potentially fraudulent. ""We are fraud resistant,"" Hylton said, contrasting this with traditional surveys where fraud rates can be substantial.
A major focus of the Series A funding will be expandingStrella's recently-launched mobile application, which Krishnan identified as critical competitive differentiation. The mobile app enables persistent screen sharing during interviews — allowing researchers to watch users navigate mobile applications in real time while the AI moderator asks about their experience.
""We are the only player in the market that supports screen sharing on mobile,"" Hylton said. ""You know, I want to understand what are the pain points with my app? Why do people not seem to be able to find the checkout flow? Well, in order to do that effectively, you'd like to see the user screen while they're doing an interview.""
For consumer-facing companies where mobile represents the primary customer interface, this capability opens entirely new use cases. The founders noted that ""several of our customers didn't do research before"" but have now built research practices around Strella because the platform finally made mobile research accessible at scale.
The platform also supports embedding traditional survey question types directly into the conversational interview, approaching what Hylton called ""feature parity with a survey"" while maintaining the engagement advantages of a natural conversation. Strella interviews regularly run 60 to 90 minutes with nearly 100% completion rates—a duration that would see 60-70% drop-off in a traditional survey format.
Strella enters a market that appears crowded at first glance, with established players likeQualtricsand a wave of AI-powered startups promising to transform customer research. The founders themselves initially pursued a different approach — synthetic respondents, or ""digital twins"" that simulate customer perspectives using large language models.
""We actually pivoted from that. That was our initial idea,"" Hylton revealed, referring to synthetic respondents. ""People are very intrigued by that concept, but found in practice, no willingness to pay right now.""
Recent research suggesting companiescould use language models as digital twinsfor customer feedback has reignited interest in that approach. But Hylton remains skeptical: ""The capabilities of the LLMs as they are today are not good enough, in my opinion, to justify a standalone company. Right now you could just ask ChatGPT, 'What would new users of Duolingo think about this ad copy?' You can do that. Adding the standalone idea of a synthetic panel is sort of just putting a wrapper on that.""
Instead, Strella's bet is that the real value lies in collecting proprietary qualitative data at scale — building what could become ""the system of truth for all qualitative insights"" within enterprises, as Lindsey Li, Vice President at Bessemer Venture Partners, described it.
Li, who led the investment just one year after Strella emerged from stealth, said the firm was convinced by both the technology and the team. ""Strella has built highly differentiated technology that enables a continuous interview rather than a survey,"" Li said. ""We heard time and time again that customers loved this product experience relative to other offerings.""
On the defensibility question that concerns many AI investors, Li emphasized product execution over patents: ""We think the long game here will be won with a million small product decisions, all of which must be driven by deep empathy for customer pain and an understanding of how best to address their needs. Lydia and Priya exhibit that in spades.""
The founders point to technical depth that's difficult to replicate. Most competitors started with adaptive surveys — text-based interfaces where users type responses and wait for the next question. Some have added voice, but typically as uploaded audio clips rather than free-flowing conversation.
""Our approach is fundamentally better, which is the fact that it is a free form conversation,"" Hylton said. ""You never have to control anything. You're never typing, there's no buttons, there's no upload and wait for the next question. It's completely free form, and that has been an extraordinarily hard product to build. There's a tremendous amount of IP in the way that we prompt our moderator, the way that we run analysis.""
The platform also improves with use, learning from each customer's research patterns to fine-tune future interview guides and questions. ""Our product gets better for our customers as they continue to use us,"" Hylton said. All research accumulates in a central repository where teams can generate new insights by chatting with the data or creating visualizations from previously unstructured qualitative feedback.
Perhaps more important than displacing existing research is expanding the total market. Krishnan said growth has been ""fundamentally related to our product"" creating new research that wouldn't have happened otherwise.
""We have expanded the use cases in which people would conduct research,"" Krishnan explained. ""Several of our customers didn't do research before, have always wanted to do research, but didn't have a dedicated researcher or team at their company that was devoted to it, and have purchased Strella to kick off and enable their research practice. That's been really cool where we've seen this market just opening up.""
This expansion comes as enterprises face mounting pressure to improve customer experience amid declining satisfaction scores. According toForrester Research's 2024 Customer Experience Index, customer experience quality has declined for three consecutive years — an unprecedented trend. The report found that 39% of brands saw CX quality deteriorate, with declines across effectiveness, ease, and emotional connection.
Meanwhile,Deloitte's 2025 Technology, Media & Telecommunications Predictions reportforecasts that 25% of enterprises using generative AI will deploy AI agents by 2025, growing to 50% by 2027. The report specifically highlighted AI's potential to enhance customer satisfaction by 15-20% while reducing cost to serve by 20-30% when properly implemented.
Gartneridentified conversational user interfaces — the category Strella inhabits — as one of three technologies poised to transform customer service by 2028, noting that ""customers increasingly expect to be able to interact with the applications they use in a natural way.""
Against this backdrop, Li sees substantial room for growth. ""UX Research is a sub-sector of the $140B+ global market-research industry,"" Li said. ""This includes both the software layer historically (~$430M) and professional services spend on UX research, design, product strategy, etc. which is conservatively estimated to be ~$6.4B+ annually. As software in this vertical, led by Strella, becomes more powerful, we believe the TAM will continue to expand meaningfully.""
The founders describe their mission as ""democratizing access to the customer"" — making it possible for anyone in an organization to understand customer perspectives without waiting for dedicated research teams to complete months-long studies.
""Many, many, many positions in the organization would like to get customer feedback, but it's so hard right now,"" Hylton said. With Strella, she explained, someone can ""log into Strella and through a chat, create any highlight reel that you want and actually see customers in their own words answering the question that you have based on the research that's already been done.""
This video-first approach to research repositories changes organizational dynamics around customer feedback. ""Then you can say, 'Okay, engineering team, we need to build this feature. And here's the customer actually saying it,'"" Hylton continued. ""'This is not me. This isn't politics. Here are seven customers saying they can't find the Checkout button.' The fact that we are a very video-based platform really allows us to do that quickly and painlessly.""
The company has moved decisively upmarket, with contract values now typically in the five-figure range and ""several six figure contracts"" signed, according to Krishnan. The pricing strategy reflects a premium positioning: ""Our product is very good, it's very premium. We're charging based on the value it provides to customers,"" Krishnan said, rather than competing on cost alone.
This approach appears to be working. The company reports 100% conversion from pilot programs to paid contracts and zero churn among its 40-45 customers, with month-over-month revenue growth of 50%.
The Series A funding will primarily support scaling product and go-to-market teams. ""We're really confident that we have product-market fit,"" Hylton said. ""And now the question is execution, and we want to hire a lot of really talented people to help us execute.""
On the product roadmap, Hylton emphasized continued focus on the participant experience as the key to winning the market. ""Everything else is downstream of a joyful participant experience,"" she said, including ""the quality of insights, the amount you have to pay people to do the interviews, and the way that your customers feel about a company.""
Near-term priorities include adding visual capabilities so the AI moderator can respond to facial expressions and other nonverbal cues, and building more sophisticated collaboration features between human researchers and AI moderators. ""Maybe you want to listen while an AI moderator is running a call and you might want to be able to jump in with specific questions,"" Hylton said. ""Or you want to run an interview yourself, but you want the moderator to be there as backup or to help you.""
These features move toward what the industry calls ""agentic AI"" — systems that can act more autonomously while still collaborating with humans. The founders see this human-AI collaboration, rather than full automation, as the sustainable path forward.
""We believe that a lot of the really strategic work that companies do will continue to be human moderated,"" Hylton said. ""And you can still do that through Strella and just use us for synthesis in those cases.""
For Li and Bessemer, the bet is on founders who understand this nuance. ""Lydia and Priya exhibit the exact archetype of founders we are excited to partner with for the long term — customer-obsessed, transparent, thoughtful, and singularly driven towards the home-run scenario,"" she said.
The company declined to disclose specific revenue figures or valuation. With the new funding, Strella has now raised $18 million total, including a$4 million seed round led by Decibel Partners announced in October.
As Strella scales, the founders remain focused on a vision where technology enhances rather than eliminates human judgment—where an engineering team doesn't just read a research report, but watches seven customers struggle to find the same button. Where a product manager can query months of accumulated interviews in seconds. Where companies don't choose between speed and depth, but get both.
""The interesting part of the business is actually collecting that proprietary dataset, collecting qualitative research at scale,"" Hylton said, describing what she sees as Strella's long-term moat. Not replacing the researcher, but making everyone in the company one.
"
Startup & Tech Media,Microsoft launches 'Hey Copilot' voice assistant and autonomous agents for all Windows 11 PCs.,https://venturebeat.com/ai/microsoft-launches-hey-copilot-voice-assistant-and-autonomous-agents-for-all,"Microsoftis fundamentally reimagining how people interact with their computers, announcing Thursday a sweeping transformation ofWindows 11that brings voice-activated AI assistants, autonomous software agents, and contextual intelligence to every PC running the operating system — not just premium devices with specialized chips.
The announcement representsMicrosoft's most aggressive push yetto integrate generative artificial intelligence into the desktop computing experience, moving beyond the chatbot interfaces that have defined the first wave of consumer AI products toward a more ambient, conversational model where users can simply talk to their computers and have AI agents complete complex tasks on their behalf.
""When we think about what the promise of an AI PC is, it should be capable of three things,"" Yusuf Mehdi, Microsoft's Executive Vice President and Consumer Chief Marketing Officer, told reporters at a press conference last week. ""First, you should be able to interact with it naturally, in text or voice, and have it understand you. Second, it should be able to see what you see and be able to offer guided support. And third, it should be able to take action on your behalf.""
The shift could prove consequential for an industry searching for the ""killer app"" for generative AI. While hundreds of millions of people have experimented with ChatGPT and similar chatbots, integrating AI directly into the operating system that powers the vast majority of workplace computers could dramatically accelerate mainstream adoption — or create new security and privacy headaches for organizations alreadystruggling to govern employee use of AI tools.
At the heart of Microsoft's vision isvoice interaction, which the company is positioning as the third fundamental input method for PCs after the mouse and keyboard — a comparison that underscores Microsoft's ambitions for reshaping human-computer interaction nearly four decades after thegraphical user interfacebecame standard.
Starting this week, anyWindows 11user can enable the ""Hey Copilot"" wake word with a single click, allowing them to summon Microsoft's AI assistant by voice from anywhere in the operating system. The feature, which had been in limited testing, is now being rolled out to hundreds of millions of devices globally.
""It's been almost four decades since the PC has changed the way you interact with it, which is primarily mouse and keyboard,"" Mehdi said. ""When you think about it, we find that people type on a given day up to 14,000 words on their keyboard, which is really kind of mind-boggling. But what if now you can go beyond that and talk to it?""
The emphasis on voice reflects internal Microsoft data showing that users engage with Copilot twice as much when using voice compared to text input — a finding the company attributes to the lower cognitive barrier of speaking versus crafting precise written prompts.
""The magic unlock withCopilot VoiceandCopilot Visionis the ease of interaction,"" according to the company's announcement. ""Using the new wake word, 'Hey Copilot,' getting something done is as easy as just asking for it.""
But Microsoft's bet on voice computing faces real-world constraints that Mehdi acknowledged during the briefing. When asked whether workers in shared office environments would use voice features, potentially compromising privacy, Mehdi noted that millions already conduct voice calls through their PCs with headphones, and predicted users would adapt: ""Just like when the mouse came out, people have to figure out when to use it, what's the right way, how to make it happen.""
Crucially, Microsoft is hedging its voice-first strategy by making all features accessible through traditional text input as well, recognizing that voice isn't always appropriate or accessible.
Perhaps more transformative than voice control is the expansion ofCopilot Vision, a feature Microsoft introduced earlier this year that allows the AI to analyze what's displayed on a user's screen and provide contextual assistance.
Previously limited to voice interaction, Copilot Vision is now rolling out worldwide with a new text-based interface, allowing users to type questions about what they're viewing rather than speaking them aloud. The feature can now access full document context inMicrosoft Officeapplications — meaning it can analyze an entire PowerPoint presentation or Excel spreadsheet without the user needing to scroll through every page.
""With 68 percent of consumers reporting using AI to support their decision making, voice is making this easier,"" Microsoft explained in its announcement. ""The magic unlock with Copilot Voice and Copilot Vision is the ease of interaction.""
During the press briefing, Microsoft demonstratedCopilot Visionhelping users navigate Spotify's settings to enable lossless audio streaming, coaching an artist through writing a professional bio based on their visual portfolio, and providing shopping recommendations based on products visible in YouTube videos.
""What brings AI to life is when you can give it rich context, when you can type great prompts,"" Mehdi explained. ""The big challenge for the majority of people is we've been trained with search to do the opposite. We've been trained to essentially type in fewer keywords, because it turns out the less keywords you type on search, the better your answers are.""
He noted that average search queries remain just 2.3 keywords, while AI systems perform better with detailed prompts — creating a disconnect between user habits and AI capabilities. Copilot Vision aims to bridge that gap by automatically gathering visual context.
""With Copilot Vision, you can simply share your screen and Copilot in literally milliseconds can understand everything on the screen and then provide intelligence,"" Mehdi said.
The vision capabilities work with any application without requiring developers to build specific integrations, using computer vision to interpret on-screen content — a powerful capability that also raises questions about what the AI can access and when.
The most ambitious—and potentially controversial—new capability isCopilot Actions, an experimental feature that allows AI to take control of a user's computer to complete tasks autonomously.
Coming first toWindows Insiders enrolled in Copilot Labs, the feature builds on Microsoft's May announcement of Copilot Actions on the web, extending the capability to manipulate local files and applications on Windows PCs.
During demonstrations, Microsoft showed the AI agent organizing photo libraries, extracting data from documents, and working through multi-step tasks while users attended to other work. The agent operates in a separate, sandboxed environment and provides running commentary on its actions, with users able to take control at any time.
""As a general-purpose agent — simply describe the task you want to complete in your own words, and the agent will attempt to complete it by interacting with desktop and web applications,"" according to the announcement. ""While this is happening, you can choose to focus on other tasks. At any time, you can take over the task or check in on the progress of the action, including reviewing what actions have been taken.""
Navjot Virk, Microsoft's Windows Experience Leader, acknowledged the technology's current limitations during the briefing. ""We'll be starting with a narrow set of use cases while we optimize model performance and learn,""Virksaid. ""You may see the agent make mistakes or encounter challenges with complex interfaces, which is why real-world testing of this experience is so critical.""
The experimental nature ofCopilot Actionsreflects broader industry challenges with agentic AI — systems that can take actions rather than simply providing information. While the potential productivity gains are substantial, AI systems still occasionally ""hallucinate"" incorrect information and can be vulnerable to novel attacks.
Recognizing the security implications of giving AI control over users' computers and files, Microsoft introduced a new security framework built on four core principles: user control, operational transparency, limited privileges, and privacy-preserving design.
Central to this approach is the concept of ""agent accounts"" — separate Windows user accounts under which AI agents operate, distinct from the human user's account. Combined with a new ""agent workspace"" that provides a sandboxed desktop environment, the architecture aims to create clear boundaries around what agents can access and modify.
Peter Waxman, Microsoft's Windows Security Engineering Leader, emphasized that Copilot Actions is disabled by default and requires explicit user opt-in. ""You're always in control of what Copilot Actions can do,"" Waxman said. ""Copilot Actions is turned off by default and you're able to pause, take control, or disable it at any time.""
During operation, users can monitor the agent's progress in real-time, and the system requests additional approval before taking ""sensitive or important"" actions. All agent activity occurs under the dedicated agent account, creating an audit trail that distinguishes AI actions from human ones.
However, the agent will have default access to users' Documents, Downloads, Desktop, and Pictures folders—a broad permission grant that could concern enterprise IT administrators.
Dana Huang, Corporate Vice President for Windows Security, acknowledged in a blog post that ""agentic AI applications introduce novel security risks, such as cross-prompt injection (XPIA), where malicious content embedded in UI elements or documents can override agent instructions, leading to unintended actions like data exfiltration or malware installation.""
Microsoft promises more details about enterprise controls at itsIgnite conferencein November.
Beyond voice and autonomous agents, Microsoft introduced changes across Windows 11's core interfaces and extended AI to new domains.
A new ""Ask Copilot"" feature integrates AI directly into the Windows taskbar, providing one-click access to start conversations, activate vision capabilities, or search for files and settings with ""lightning-fast"" results. The opt-in feature doesn't replace traditional Windows search.
File Explorer gains AI capabilities through integration with third-party services. A partnership with Manus AI allows users to right-click on local image files and generate complete websites without manual uploading or coding. Integration with Filmora enables quick jumps into video editing workflows.
Microsoft also introducedCopilot Connectors, allowing users to link cloud services like OneDrive, Outlook, Google Drive, Gmail, and Google Calendar directly to Copilot on Windows. Once connected, users can query personal content across platforms using natural language.
In a notable expansion beyond productivity, Microsoft and Xbox introduced Gaming Copilot for theROG Xbox Allyhandheld gaming devices developed with ASUS. The feature, accessible via a dedicated hardware button, provides an AI assistant that can answer gameplay questions, offer strategic advice, and help navigate game interfaces through natural voice conversation.
Microsoft's announcement comes as technology giants race to embed generative AI into their core products following the November 2022 launch of ChatGPT. While Microsoft moved quickly tointegrate OpenAI's technology into Bing searchand introduce Copilot across its product line, the company has faced questions about whether AI features are driving meaningful engagement. Recent data showsBing's search market shareremaining largely flat despite AI integration.
The Windows integration represents a different approach: rather than charging separately for AI features, Microsoft is building them into the operating system itself, betting that embedded AI will drive Windows 11 adoption and competitive differentiation against Apple and Google.
Apple has taken a more cautious approach withApple Intelligence, introducing AI features gradually and emphasizing privacy through on-device processing. Google has integrated AI across its services but has faced challenges with accuracy and reliability.
Crucially, while Microsoft highlightednew Copilot+ PC modelsfrom partners with prices ranging from $649.99 to $1,499.99, the core AI features announced todaywork on any Windows 11 PC— a significant departure from earlier positioning that suggested AI capabilities required new hardware with specialized neural processing units.
""Everything we showed you here is for all Windows 11 PCs. You don't need to run it on a copilot plus PC. It works on any Windows 11 PC,"" Mehdi clarified.
This democratization of AI features across the Windows 11 installed base potentially accelerates adoption but also complicates Microsoft's hardware sales pitch for premium devices.
Mehdi framed the announcement in sweeping terms, describing Microsoft's goal as fundamentally reimagining the operating system for the AI era.
""We're taking kind of a bold view of it. We really feel that the vision that we have is, let's rewrite the entire operating system around AI and build essentially what becomes truly the AI PC,"" he said.
For Microsoft, the success ofAI-powered Windows 11could help drive the company's next phase of growth as PC sales have matured and cloud growth faces increased competition.
For users and organizations, the announcement represents a potential inflection point in how humans interact with computers — one that could significantly boost productivity if executed well, or create new security headaches if the AI proves unreliable or difficult to control.
The technology industry will be watching closely to see whether Microsoft's bet on conversational computing and agentic AI marks the beginning of a genuine paradigm shift, or proves to be another ambitious interface reimagining that fails to gain mainstream traction.
What's clear is that Microsoft is moving aggressively to stake its claim as the leader in AI-powered personal computing, leveraging its dominant position in desktop operating systems to bring generative AI directly into the daily workflows of potentially a billion users.
Copilot Voice and Vision areavailable todayto Windows 11 users worldwide, with experimental capabilities coming to Windows Insiders in the coming weeks."
Startup & Tech Media,ACE prevents context collapse with ‘evolving playbooks’ for self-improving AI agents.,https://venturebeat.com/ai/ace-prevents-context-collapse-with-evolving-playbooks-for-self-improving-ai,"A new framework fromStanford UniversityandSambaNovaaddresses a critical challenge in building robust AI agents: context engineering. CalledAgentic Context Engineering(ACE), the framework automatically populates and modifies the context window of large language model (LLM) applications by treating it as an “evolving playbook” that creates and refines strategies as the agent gains experience in its environment.
ACE is designed to overcome key limitations of other context-engineering frameworks, preventing the model’s context from degrading as it accumulates more information. Experiments show that ACE works for both optimizing system prompts and managing an agent's memory, outperforming other methods while also being significantly more efficient.
Advanced AI applications that use LLMs largely rely on ""context adaptation,"" or context engineering, to guide their behavior. Instead of the costly process of retraining or fine-tuning the model, developers use the LLM’sin-context learning abilitiesto guide its behavior by modifying the input prompts with specific instructions, reasoning steps, or domain-specific knowledge. This additional information is usually obtained as the agent interacts with its environment and gathers new data and experience. The key goal of context engineering is to organize this new information in a way that improves the model’s performance and avoids confusing it. This approach is becoming a central paradigm for building capable, scalable, and self-improving AI systems.
Context engineering has several advantages for enterprise applications. Contexts are interpretable for both users and developers, can be updated with new knowledge at runtime, and can be shared across different models. Context engineering also benefits from ongoing hardware and software advances, such as thegrowing context windowsof LLMs and efficient inference techniques like prompt and context caching.
There are various automated context-engineering techniques, but most of them face two key limitations. The first is a “brevity bias,” where prompt optimization methods tend to favor concise, generic instructions over comprehensive, detailed ones. This can undermine performance in complex domains.
The second, more severe issue is ""context collapse."" When an LLM is tasked with repeatedly rewriting its entire accumulated context, it can suffer from a kind of digital amnesia.
“What we call ‘context collapse’ happens when an AI tries to rewrite or compress everything it has learned into a single new version of its prompt or memory,” the researchers said in written comments to VentureBeat. “Over time, that rewriting process erases important details—like overwriting a document so many times that key notes disappear. In customer-facing systems, this could mean a support agent suddenly losing awareness of past interactions... causing erratic or inconsistent behavior.”
The researchers argue that “contexts should function not as concise summaries, but as comprehensive, evolving playbooks—detailed, inclusive, and rich with domain insights.” This approach leans into the strength of modern LLMs, which can effectively distill relevance from long and detailed contexts.
ACE is a framework for comprehensive context adaptation designed for both offline tasks, likesystem prompt optimization, and online scenarios, such as real-time memory updates for agents. Rather than compressing information, ACE treats the context like a dynamic playbook that gathers and organizes strategies over time.
The framework divides the labor across three specialized roles: a Generator, a Reflector, and a Curator. This modular design is inspired by “how humans learn—experimenting, reflecting, and consolidating—while avoiding the bottleneck of overloading a single model with all responsibilities,” according to the paper.
Agentic Context Engineering (ACE) framework (source: arXiv)
The workflow starts with the Generator, which produces reasoning paths for input prompts, highlighting both effective strategies and common mistakes. The Reflector then analyzes these paths to extract key lessons. Finally, the Curator synthesizes these lessons into compact updates and merges them into the existing playbook.
To prevent context collapse and brevity bias, ACE incorporates two key design principles. First, it uses incremental updates. The context is represented as a collection of structured, itemized bullets instead of a single block of text. This allows ACE to make granular changes and retrieve the most relevant information without rewriting the entire context.
Second, ACE uses a “grow-and-refine” mechanism. As new experiences are gathered, new bullets are appended to the playbook and existing ones are updated. A de-duplication step regularly removes redundant entries, ensuring the context remains comprehensive yet relevant and compact over time.
The researchers evaluated ACE on two types of tasks that benefit from evolving context: agent benchmarks requiring multi-turn reasoning and tool use, and domain-specific financial analysis benchmarks demanding specialized knowledge. For high-stakes industries like finance, the benefits extend beyond pure performance. As the researchers said, the framework is “far more transparent: a compliance officer can literally read what the AI learned, since it’s stored in human-readable text rather than hidden in billions of parameters.”
The results showed that ACE consistently outperformed strong baselines such asGEPAand classic in-context learning, achieving average performance gains of 10.6% on agent tasks and 8.6% on domain-specific benchmarks in both offline and online settings.
Critically, ACE can build effective contexts by analyzing the feedback from its actions and environment instead of requiring manually labeled data. The researchers note that this ability is a ""key ingredient for self-improving LLMs and agents."" On the publicAppWorldbenchmark, designed to evaluate agentic systems, an agent using ACE with a smaller open-source model (DeepSeek-V3.1) matched the performance of the top-ranked,GPT-4.1-powered agenton average and surpassed it on the more difficult test set.
ACE outperforms other baselines on different industry benchmarks (source: arXiv)
The takeaway for businesses is significant. “This means companies don’t have to depend on massive proprietary models to stay competitive,” the research team said. “They can deploy local models, protect sensitive data, and still get top-tier results by continuously refining context instead of retraining weights.”
Beyond accuracy, ACE proved to be highly efficient. It adapts to new tasks with an average 86.9% lower latency than existing methods and requires fewer steps and tokens. The researchers point out that this efficiency demonstrates that “scalable self-improvement can be achieved with both higher accuracy and lower overhead.”
For enterprises concerned about inference costs, the researchers point out that the longer contexts produced by ACE do not translate to proportionally higher costs. Modern serving infrastructures are increasingly optimized for long-context workloads with techniques like KV cache reuse, compression, and offloading, which amortize the cost of handling extensive context.
Ultimately, ACE points toward a future where AI systems are dynamic and continuously improving. ""Today, only AI engineers can update models, but context engineering opens the door for domain experts—lawyers, analysts, doctors—to directly shape what the AI knows by editing its contextual playbook,"" the researchers said. This also makes governance more practical. ""Selective unlearning becomes much more tractable: if a piece of information is outdated or legally sensitive, it can simply be removed or replaced in the context, without retraining the model.”"
Startup & Tech Media,Google vs. OpenAI vs. Visa: competing agent protocols threaten the future of AI commerce.,https://venturebeat.com/ai/google-vs-openai-vs-visa-competing-agent-protocols-threaten-the-future-of-ai,"When Walmart and OpenAI announced that the retailer would integrate with ChatGPT, the question became how quickly OpenAI could deliver on the promise of agents buying things for people. In the battle of AI-enabled commerce, getting agents to securely complete transactions is one of the biggest hurdles.
More and more, chat platforms like ChatGPT arereplacing browsersand getting very good at surfacing information people search for. Users will ask ChatGPT for the best humidifiers on the market, and when the model returns results, people have no choice but to click the item link and complete the purchase online.
AI agents, as of now, don’t have the ability or the trust infrastructure to make people and banking institutions feel safe enough to let it loose on someone’s cash. Enterprises and other industry players understand that, to allow agents to pay for purchases, there must be a common language shared among the model and agent providers, the bank, the merchant, and, to a lesser extent, the buyer.
And so, over the past few weeks, three competing agentic commerce standards have emerged:Googleannounced theAgent Pay Protocol (AP2)with partners including PayPal, American Express, Mastercard, Salesforce and ServiceNow. Soon after,OpenAIandStripedebuted the Agentic Commerce Protocol (ACP), and just this week,Visalaunched theTrusted Agent Protocol(TAP).
All these protocols aim to give agents the trust layer they need to convince banks and their customers that they’re money is safe in the hands of an AI agent. But these may also create walled gardens, showing just how immature agentic commerce really is.This is aproblem that could cause enterprises to bet on one chat platform and the agentic pay protocol it runs on, instead of interoperability.
It’s not new for players to propose several standards. It usually takes years for the industry to coalesce around a single standard, or even to use different protocols and figure out a way to harmonize them. However, the pace of innovation in enterprise moved the needle on that.
Fairly quickly, MCP became the de facto channel for tool-use identification, and most companies began setting up MCP servers or connecting to one. (To be clear, it is not a standard yet) But having three different potential standards might slow that process down a bit, because it’s harder to coalesce on a single standard when there are so many to choose from.
These protocols all aim to prove authorization. Both AP2 and TAP rely on cryptographic proofs to show an agent is acting on an individual's behalf. For TAP, agents are added to an approved list and get a digital key identifying them. AP2 uses a digital contract that serves as a proxy for human approval for the agent. OpenAI’s ACP doesn’t require too much of an infrastructure change, where ACP essentially acts as a courier to the merchant because the agent relays information to the merchant.
These three protocols ideally work across different chat platforms, but that is never guaranteed, especially when your biggest chat platform competitor has its own protocol. A danger with competing protocols is that they can create wall gardens, where they only work on specific platforms.
Enterprises face the problem of getting stuck in a platform and an agentic payment standard that will not interoperate with another. Organizations receive not only the product recommended by the agent, but are also most often the merchants of record and need to trust that the agent contacting them is acting on behalf of a customer.
Louis Amira, cofounder and CEO of agent commerce startup Circuit and Chisel, told VentureBeat that while this creates an opportunity for companies in the interoperability layer like his, it could create confusion for enterprises.
“The better the protocol proposals get, the more likely they are to end up being walled gardens and very hard to interoperate,” Amira said. “We suspect that they’re going to be fighting it out for the next few years, and the more they fight it out, the more you actually need somebody that sits underneath all of them.”
Unlike the internet, where anyone can use any browser to access a website, thanks in large part to the TCP/IP standard, chat platforms tend to remain very separate. I mostly use ChatGPT (because it’s installed on my laptop and I don’t need to open a new tab), so when I want to see how Gemini will handle my query, I actually have to open Gemini to do so—the same works for anyone shopping via chatbot.
The number of protocol proposals underscores just how far we are from enabling shopping agents. The industry still needs to decide which standard to get behind, and no matter how many Walmarts integrate with ChatGPT, it’s all moot if people don’t trust the model or agent to handle their cash.
The best thing for enterprises to do for now is to experiment with all the protocols and hope that a winner emerges. Eventually, there could be one agentic commerce protocol that takes the best of each proposal.
For Wayne Liu, chief growth officer and president for Americas at Perfect Corp., having multiple protocol proposals just means there’s more learning.
“This is where the importance of open source exists because it will be the driving force to put everything together,” Liu said.
Of course, what would be interesting to see these next couple of weeks is if there will only be three competing agentic commerce protocols. After all, there are some large retailers and chat platforms that can still throw a wrench into the whole thing.
"
Startup & Tech Media,Under the hood of AI agents: A technical guide to the next frontier of gen AI.,https://venturebeat.com/ai/under-the-hood-of-ai-agents-a-technical-guide-to-the-next-frontier-of-gen-ai,"Agents are the trendiest topic in AI today, and with good reason. AI agents act on their users’ behalf, autonomously handling tasks like making online purchases, building software, researching business trends or booking travel. By taking generative AI out of the sandbox of the chat interface and allowing it to act directly on the world, agentic AI represents a leap forward in the power and utility of AI.Taking gen AI out of the protected sandbox of the chat interface and allowing it to act directly on the world represents a leap forward in the power and utility of AI.
Agentic AI has been moving really fast: For example, one of the core building blocks of today’s agents, the model context protocol (MCP), is only a year old! As in any fast-moving field, there are many competing definitions, hot takes and misleading opinions.
To cut through the noise, I’d like to describe the core components of an agentic AI system and how they fit together: It’s really not as complicated as it may seem. Hopefully, when you’ve finished reading this post, agents won’t seem as mysterious.
Definitions of the word “agent” abound, but I like a slight variation on the British programmer Simon Willison’s minimalist take:
An LLM agent runs tools in a loop to achieve a goal.
The user prompts a large language model (LLM) with a goal: Say, booking a table at a restaurant near a specific theater. Along with the goal, the model receives a list of the tools at its disposal, such as a database of restaurant locations or a record of the user’s food preferences. The model then plans how to achieve the goal and calls one of the tools, which provides a response; the model then calls a new tool. Through repetitions, the agent moves toward accomplishing the goal. In some cases, the model’s orchestration and planning choices are complemented or enhanced by imperative code.
But what kind of infrastructure does it take to realize this approach? An agentic system needs a few core components:
A way tobuild the agent. When you deploy an agent, you don’t want to have to code it from scratch. There are several agent development frameworks out there.
Somewhere torun the AI model.A seasoned AI developer can download an open-weight LLM, but it takes expertise to do that right. It also takes expensive hardware that’s going to be poorly utilized for the average user.
Somewhere torun the agentic code. With established frameworks, the user creates code for an agent object with a defined set of functions. Most of those functions involve sending prompts to an AI model, but the code needs to run somewhere. In practice, most agents will run in the cloud, because we want them to keep running when our laptops are closed, and we want them to scale up and out to do their work.
A mechanism for translating between the text-based LLM andtool calls.
Ashort-termmemoryfor tracking the content of agentic interactions.
Along-term memoryfor tracking the user’s preferences and affinities across sessions.
A way totracethe system’s execution, to evaluate the agent’s performance.
Let's dive into more detail on each of these components.
Asking an LLM to explain how it plans to approach a particular task improves its performance on that task. This “chain-of-thought reasoning” is now ubiquitous in AI.
The analogue in agentic systems is the ReAct (reasoning + action) model, in which the agent has a thought (“I’ll use the map function to locate nearby restaurants”), performs an action (issuing an API call to the map function), then makes an observation (“There are two pizza places and one Indian restaurant within two blocks of the movie theater”).
ReAct isn’t the only way to build agents, but it is at the core of most successful agentic systems. Today, agents are commonly loops over thethought-action-observationsequence.
The tools available to the agent can include local tools and remote tools such as databases, microservices and software as a service. A tool’s specification includes a natural-language explanation of how and when it’s used and the syntax of its API calls.
The developer can also tell the agent to, essentially, build its own tools on the fly. Say that a tool retrieves a table stored as comma-separated text, and to fulfill its goal, the agent needs to sort the table.
Sorting a table by repeatedly sending it through an LLM and evaluating the results would be a colossal waste of resources — and it’s not even guaranteed to give the right result. Instead, the developer can simply instruct the agent to generate its own Python code when it encounters a simple but repetitive task. These snippets of code can run locally alongside the agent or in a dedicated secure code interpreter tool.
Available tools can divide responsibility between the LLM and the developer. Once the tools available to the agent have been specified, the developer can simply instruct the agent what tools to use when necessary. Or, the developer can specify which tool to use for which types of data, and even which data items to use as arguments during function calls.
Similarly, the developer can simply tell the agent to generate Python code when necessary to automate repetitive tasks or, alternatively, tell it which algorithms to use for which data types and even provide pseudocode. The approach can vary from agent to agent.
Historically, there were two main ways to isolate code running on shared servers: Containerization, which was efficient but offered lower security; and virtual machines, which were secure but came with a lot of computational overhead.
In 2018, Amazon Web Services’ (AWS’s) Lambda serverless-computing service deployedFirecracker, a new paradigm in server isolation. Firecracker creates “microVMs”, complete with hardware isolation and their own Linux kernels but with reduced overhead (as low as a few megabytes) and startup times (as low as a few milliseconds). The low overhead means that each function executed on a Lambda server can have its own microVM.
However, because instantiating an agent requires deploying an LLM, together with the memory resources to track the LLM’s inputs and outputs, the per-function isolation model is impractical. Instead, with session-based isolation, every session is assigned its own microVM. When the session finishes, the LLM’s state information is copied to long-term memory, and the microVM is destroyed. This ensures secure and efficient deployment of hosts of agents.
Just as there are several existing development frameworks for agent creation, there are several existing standards for communication between agents and tools, the most popular of which — currently — is the model context protocol (MCP).
MCP establishes a one-to-one connection between the agent’s LLM and a dedicated MCP server that executes tool calls, and it also establishes a standard format for passing different types of data back and forth between the LLM and its server.
Many platforms use MCP by default, but are also configurable, so they will support a growing set of protocols over time.
Sometimes, however, the necessary tool is not one with an available API. In such cases, the only way to retrieve data or perform an action is through cursor movements and clicks on a website. There are a number of services available to perform suchcomputer use. This makes any website a potential tool for agents, opening up decades of content and valuable services that aren’t yet available directly through APIs.
With agents, authorization works in two directions. First, of course, users require authorization to run the agents they’ve created. But as the agent is acting on the user’s behalf, it will usually require its own authorization to access networked resources.
There are a few different ways to approach the problem of authorization. One is with an access delegation algorithm like OAuth, which essentially plumbs the authorization process through the agentic system. The user enters login credentials into OAuth, and the agentic system uses OAuth to log into protected resources, but the agentic system never has direct access to the user’s passwords.
In the other approach, the user logs into a secure session on a server, and the server has its own login credentials on protected resources. Permissions allow the user to select from a variety of authorization strategies and algorithms for implementing those strategies.
Short-term memory
LLMs are next-word prediction engines. What makes them so astoundingly versatile is that their predictions are based on long sequences of words they’ve already seen, known ascontext. Context is, in itself, a kind of memory. But it’s not the only kind an agentic system needs.
Suppose, again, that an agent is trying to book a restaurant near a movie theater, and from a map tool, it’s retrieved a couple dozen restaurants within a mile radius. It doesn’t want to dump information about all those restaurants into the LLM’s context: All that extraneous information could wreak havoc with next-word probabilities.
Instead, it can store the complete list in short-term memory and retrieve one or two records at a time, based on, say, the user’s price and cuisine preferences and proximity to the theater. If none of those restaurants pans out, the agent can dip back into short-term memory, rather than having to execute another tool call.
Long-term memory
Agents also need to remember their prior interactions with their clients. If last week I told the restaurant booking agent what type of food I like, I don’t want to have to tell it again this week. The same goes for my price tolerance, the sort of ambiance I’m looking for, and so on.
Long-term memory allows the agent to look up what it needs to know about prior conversations with the user. Agents don’t typically create long-term memories themselves, however. Instead, after a session is complete, the whole conversation passes to a separate AI model, which creates new long-term memories or updates existing ones.
Memory creation can involve LLM summarization and “chunking”, in which documents are split into sections grouped according to topic for ease of retrieval during subsequent sessions. Available systems allow the user to select strategies and algorithms for summarization, chunking and other information-extraction techniques.
Agents are a new kind of software system, and they require new ways to think about observing, monitoring and auditing their behavior. Some of the questions we ask will look familiar: Whether the agents are running fast enough, how much they’re costing, how many tool calls they’re making and whether users are happy. But new questions will arise, too, and we can’t necessarily predict what data we’ll need to answer them.
Observability and tracing tools can provide an end-to-end view of the execution of a session with an agent, breaking down step-by-step which actions were taken and why. For the agent builder, these traces are key to understanding how well agents are working — and provide the data to make them work better.
I hope this explanation has demystified agentic AI enough that you’re willing to try building your own agents!"
Startup & Tech Media,Anthropic is giving away its powerful Claude Haiku 4.5 AI for free to take on OpenAI.,https://venturebeat.com/ai/anthropic-is-giving-away-its-powerful-claude-haiku-4-5-ai-for-free-to-take,"AnthropicreleasedClaude Haiku 4.5on Wednesday, a smaller and significantly cheaper artificial intelligence model that matches the coding capabilities of systems that were considered cutting-edge just months ago, marking the latest salvo in an intensifying competition to dominate enterprise AI.
The model costs $1 per million input tokens and $5 per million output tokens — roughly one-third the price of Anthropic's mid-sizedSonnet 4 modelreleased in May, while operating more than twice as fast. In certain tasks, particularly operating computers autonomously, Haiku 4.5 actually surpasses its more expensive predecessor.
""Haiku 4.5 is a clear leap in performance and is now largely as smart as Sonnet 4 while being significantly faster and one-third of the cost,"" an Anthropic spokesperson told VentureBeat, underscoring how rapidly AI capabilities are becoming commoditized as the technology matures.
The launch comes just two weeks after Anthropic releasedClaude Sonnet 4.5, which the company bills as the world's best coding model, and two months after introducing Opus 4.1. The breakneck pace of releases reflects mounting pressure from OpenAI, whose $500 billion valuation dwarfsAnthropic's $183 billion, and which has inked a series of multibillion-dollar infrastructure deals while expanding its product lineup.
In an unusual move that could reshape competitive dynamics in the AI market, Anthropic is making Haiku 4.5 available for all free users of itsClaude.aiplatform. The decision effectively democratizes access to what the company characterizes as ""near-frontier-level intelligence"" — capabilities that would have been available only in expensive, premium models months ago.
""The launch of Claude Haiku 4.5 means that near-frontier-level intelligence is available for free to all users through Claude.ai,"" the Anthropic spokesperson told VentureBeat. ""It also offers significant advantages to our enterprise customers: Sonnet 4.5 can handle frontier planning while Haiku 4.5 powers sub-agents, enabling multi-agent systems that tackle complex refactors, migrations, and large features builds with speed and quality.""
This multi-agent architecture signals a significant shift in how AI systems are deployed. Rather than relying on a single, monolithic model, enterprises can now orchestrate teams of specialized AI agents: a more sophisticatedSonnet 4.5 modelbreaking down complex problems and delegating subtasks to multipleHaiku 4.5agents working in parallel. For software development teams, this could mean Sonnet 4.5 plans a major code refactoring while Haiku 4.5 agents simultaneously execute changes across dozens of files.
The approach mirrors how human organizations distribute work, and could prove particularly valuable for enterprises seeking to balance performance with cost efficiency — a critical consideration as AI deployment scales.
The model launch coincides with revelations that Anthropic's business is experiencing explosive growth. The company's annual revenue run rate isapproaching $7 billion this month, Anthropic told Reuters, up from more than $5 billion reported in August. Internal projections obtained by Reuters suggest the company is targeting between $20 billion and $26 billion in annualized revenue for 2026, representing growth of more than 200% to nearly 300%.
The company now serves more than 300,000 business customers, with enterprise products accounting for approximately 80% of revenue. Among Anthropic's most successful offerings isClaude Code, a code-generation tool that has reached nearly $1 billion in annualized revenue since launching earlier this year.
Those numbers come as artificial intelligence enters what many in the industry characterize as a critical inflection point. After two years of what Anthropic Chief Product Officer Mike Krieger recently described as ""AI FOMO"" — where companies adopted AI tools without clear success metrics — enterprises are now demanding measurable returns on investment.
""The best products can be grounded in some kind of success metric or evaluation,"" Krieger said on the""Superhuman AI"" podcast. ""I've seen that a lot in talking to companies that are deploying AI.""
For enterprises evaluating AI tools, the calculus increasingly centers on concrete productivity gains. Google CEO Sundar Pichai claimed in June that AI had generated a 10% boost in engineering velocity at his company — though measuring such improvements across different roles and use cases remains challenging, as Krieger acknowledged.
Anthropic's launch comes amid heightened scrutiny of the company's approach to AI safety and regulation. On Tuesday, David Sacks, the White House's AI ""czar"" and a venture capitalist, accused Anthropic of ""running a sophisticated regulatory capture strategy based on fear-mongering"" that is ""damaging the startup ecosystem.""
The attack targeted remarks by Jack Clark, Anthropic's British co-founder and head of policy, who had described being ""deeply afraid"" of AI's trajectory. Clark told Bloomberg he found Sacks' criticism ""perplexing.""
Anthropic addressed such concerns head-on in its release materials, emphasizing thatHaiku 4.5underwent extensive safety testing. The company classified the model asASL-2— its AI Safety Level 2 standard — compared to the more restrictiveASL-3designation for the more powerful Sonnet 4.5 and Opus 4.1 models.
""Our teams have red-teamed and tested our agentic capabilities to the limits in order to assess whether it can be used to engage in harmful activity like generating misinformation or promoting fraudulent behavior like scams,"" the spokesperson told VentureBeat. ""In our automated alignment assessment, it showed a statistically significantly lower overall rate of misaligned behaviors than both Claude Sonnet 4.5 and Claude Opus 4.1 — making it, by this metric, our safest model yet.""
The company said its safety testing showedHaiku 4.5poses only limited risks regarding the production of chemical, biological, radiological and nuclear weapons. Anthropic has also implemented classifiers designed to detect and filter prompt injection attacks, a common method for attempting to manipulate AI systems into producing harmful content.
The emphasis on safety reflects Anthropic's founding mission. The company was established in 2021 by former OpenAI executives, including siblings Dario and Daniela Amodei, who left amid concerns about OpenAI's direction following its partnership with Microsoft. Anthropic has positioned itself as taking a more cautious, research-oriented approach to AI development.
According to Anthropic's benchmarks,Haiku 4.5performs competitively with or exceeds several larger models across multiple evaluation criteria. OnSWE-bench Verified, a widely used test measuring AI systems' ability to solve real-world software engineering problems, Haiku 4.5 scored 73.3% — slightly ahead of Sonnet 4's 72.7% and close to GPT-5 Codex's 74.5%.
The model demonstrated particular strength in computer use tasks, achieving 50.7% on theOSWorld benchmarkcompared to Sonnet 4's 42.2%. This capability allows the AI to interact directly with computer interfaces — clicking buttons, filling forms, navigating applications — which could prove transformative for automating routine digital tasks.
In coding-specific benchmarks likeTerminal-Bench, which tests AI agents' ability to complete complex software tasks using command-line tools, Haiku 4.5 scored 41.0%, trailing only Sonnet 4.5's 50.0% among Claude models.
The model maintains a 200,000-token context window for standard users, with developers accessing theClaude Developer Platformable to use a 1-million-token context window. That expanded capacity means the model can process extremely large codebases or documents in a single request — roughly equivalent to a 1,500-page book.
When asked about the rapid succession of model releases, the Anthropic spokesperson emphasized the company's focus on execution rather than competitive positioning.
""We're focused on shipping the best possible products for our customers — and our shipping velocity speaks for itself,"" the spokesperson said. ""What was state-of-the-art just five months ago is now faster, cheaper, and more accessible.""
That velocity stands in contrast to the company's earlier, more measured release schedule. Anthropic appeared to have paused development of its Haiku line after releasing version 3.5 at the end of last year, leading some observers to speculate the company had deprioritized smaller models.
That rapid price-performance improvement validates a core promise of artificial intelligence: that capabilities will become dramatically cheaper over time as the technology matures and companies optimize their models. For enterprises, it suggests that today's budget constraints around AI deployment may ease considerably in coming years.
The practical applications ofHaiku 4.5span a wide range of enterprise functions, from customer service to financial analysis to software development. The model's combination of speed and intelligence makes it particularly suited for real-time, low-latency tasks like chatbot conversations and customer support interactions, where delays of even a few seconds can degrade user experience.
In financial services, the multi-agent architecture enabled by pairingSonnet 4.5withHaiku 4.5could transform how firms monitor markets and manage risk. Anthropic envisions Haiku 4.5 monitoring thousands of data streams simultaneously — tracking regulatory changes, market signals and portfolio risks — while Sonnet 4.5 handles complex predictive modeling and strategic analysis.
For research organizations, the division of labor could compress timelines dramatically.Sonnet 4.5might orchestrate a comprehensive analysis while multipleHaiku 4.5agents parallelize literature reviews, data gathering and document synthesis across dozens of sources, potentially ""compressing weeks of research into hours,"" according to Anthropic's use case descriptions.
Several companies have already integrated Haiku 4.5 and reported positive results. Guy Gur-Ari, co-founder of coding startup Augment, said the model ""hit a sweet spot we didn't think was possible: near-frontier coding quality with blazing speed and cost efficiency."" In Augment's internal testing, Haiku 4.5 achieved 90% of Sonnet 4.5's performance while matching much larger models.
Jeff Wang, CEO of Windsurf, another coding-focused startup, said Haiku 4.5 ""is blurring the lines"" on traditional trade-offs between speed, cost and quality. ""It's a fast frontier model that keeps costs efficient and signals where this class of models is headed.""
Jon Noronha, co-founder of presentation software company Gamma, reported that Haiku 4.5 ""outperformed our current models on instruction-following for slide text generation, achieving 65% accuracy versus 44% from our premium tier model — that's a game-changer for our unit economics.""
For enterprises evaluating AI strategies,Haiku 4.5presents both opportunity and challenge. The opportunity lies in accessing sophisticated AI capabilities at dramatically lower costs, potentially making viable entire categories of applications that were previously too expensive to deploy at scale.
The challenge is keeping pace with a technology landscape that is evolving faster than most organizations can absorb. As Krieger noted in his recent podcast appearance, companies are moving beyond ""AI FOMO"" to demand concrete metrics and demonstrated value. But establishing those metrics and evaluation frameworks takes time — time that may be in short supply as competitors race ahead.
The shift from single-model deployments to multi-agent architectures also requires new ways of thinking about AI systems. Rather than viewing AI as a monolithic assistant, enterprises must learn to orchestrate multiple specialized agents, each optimized for particular tasks — more akin to managing a team than operating a tool.
The fundamental economics of AI are shifting with remarkable speed. Five months ago, Sonnet 4's capabilities commanded premium pricing and represented the cutting edge. Today, Haiku 4.5 delivers similar performance at a third of the cost. If that trajectory continues — and both Anthropic's release schedule and competitive pressure from OpenAI and Google suggest it will — the AI capabilities that seem remarkable today may be routine and inexpensive within a year.
For Anthropic, the challenge will be translating technical achievements into sustainable business growth while maintaining the safety-focused approach that differentiates it from competitors. The company's projected revenue growth to as much as $26 billion by 2026 suggests strong market traction, but achieving those targets will require continued innovation and successful execution across an increasingly complex product portfolio.
Whether enterprises will choose Claude over increasingly capable alternatives from OpenAI, Google and a growing field of competitors remains an open question. But Anthropic is making a clear bet: that the future of AI belongs not to whoever builds the single most powerful model, but to whoever can deliver the right intelligence, at the right speed, at the right price — and make it accessible to everyone.
In an industry where the promise of artificial intelligence has long outpaced reality, Anthropic is betting that delivering on that promise, faster and cheaper than anyone expected, will be enough to win. And with pricing dropping by two-thirds in just five months while performance holds steady, that promise is starting to look like reality.
"
Startup & Tech Media,Google releases new AI video model Veo 3.1 in Flow and API: what it means for enterprises.,https://venturebeat.com/ai/google-releases-new-ai-video-model-veo-3-1-in-flow-and-api-what-it-means-for,"As expected after days of leaks and rumors online, Google hasunveiled Veo 3.1, its latest AI video generation model, bringing a suite of creative and technical upgrades aimed at improving narrative control, audio integration, and realism in AI-generated video.
While the updates expand possibilities for hobbyists and content creators using Google’s online AI creation app,Flow, the release also signals a growing opportunity for enterprises, developers, and creative teams seeking scalable, customizable video tools.
The quality is higher, the physics better, the pricing the same as before, and the control and editing features more robust and varied.
Myinitial testsshowed it to be a powerful and performant model that immediately delights with each generation. However, the look is more cinematic, polished and a little more ""artificial"" than by default than rivals such asOpenAI's new Sora 2, released late last month, which may or may not be what a particular user is going after (Sora excels at handheld and ""candid"" style videos).
Veo 3.1 builds on its predecessor, Veo 3 (released back in May 2025) with enhanced support for dialogue, ambient sound, and other audio effects.
Native audio generation is now available across several key features in Flow, including “Frames to Video,” “Ingredients to Video,” and “Extend,"" which give users the ability to, respectively: turn still images into video; use items, characters and objects from multiple images in a single video; and generate longer clips than the initial 8 seconds, to more than 30 seconds or even 1+ plus when continuing from a prior clip's final frame.
Before, you had to add audio manually after using these features.
This addition gives users greater command over tone, emotion, and storytelling — capabilities that have previously required post-production work.
In enterprise contexts, this level of control may reduce the need for separate audio pipelines, offering an integrated way to create training content, marketing videos, or digital experiences with synchronized sound and visuals.
Google noted ina blog postthat the updates reflect user feedback calling for deeper artistic control and improved audio support. Gallegos emphasizes the importance of making edits and refinements possible directly in Flow, without reworking scenes from scratch.
With Veo 3.1, Google introduces support for multiple input types and more granular control over generated outputs. The model accepts text prompts, images, and video clips as input, and also supports:
Reference images (up to three)to guide appearance and style in the final output
First and last frame interpolationto generate seamless scenes between fixed endpoints
Scene extensionthat continues a video’s action or motion beyond its current duration
These tools aim to give enterprise users a way to fine-tune the look and feel of their content—useful for brand consistency or adherence to creative briefs.
Additional capabilities like “Insert” (add objects to scenes) and “Remove” (delete elements or characters) are also being introduced, though not all are immediately available through the Gemini API.
Veo 3.1 is accessible through several of Google’s existing AI services:
Flow, Google’s own interface for AI-assisted filmmaking
Gemini API, targeted at developers building video capabilities into applications
Vertex AI, where enterprise integration will soon support Veo’s “Scene Extension” and other key features
Availability through these platforms allows enterprise customers to choose the right environment—GUI-based or programmatic—based on their teams and workflows.
The Veo 3.1 model is currently inpreviewand available only on thepaid tierof the Gemini API. The cost structure is the same as Veo 3, the preceding generation of AI video models from Google.
Standard model: $0.40 per second of video
Fast model: $0.15 per second
There is no free tier, and users are charged only if a video is successfully generated. This model is consistent with previous Veo versions and provides predictable pricing for budget-conscious enterprise teams.
Veo 3.1 outputs video at720p or 1080p resolution, with a24 fps frame rate.
Duration options include4, 6, or 8 secondsfrom a text prompt or uploaded images, with the ability to extend videos up to148 seconds (more than 2 and half minutes!)when using the “Extend” feature.
New functionality also includes tighter control over subjects and environments. For example, enterprises can upload a product image or visual reference, and Veo 3.1 will generate scenes that preserve its appearance and stylistic cues across the video. This could streamline creative production pipelines for retail, advertising, and virtual content production teams.
The broader creator and developer community has responded to Veo 3.1’s launch with a mix of optimism and tempered critique—particularly when comparing it to rival models like OpenAI’s Sora 2.
Matt Shumer,an AI founder of Otherside AI/Hyperwrite, and early adopter, described his initial reaction as “disappointment,” noting that Veo 3.1 is “noticeably worse than Sora 2” and also “quite a bit more expensive.”
However, he acknowledged that Google’s tooling—such as support for references and scene extension—is a bright spot in the release.
Travis Davids, a 3D digital artist and AI content creator, echoed some of that sentiment. While he noted improvements in audio quality, particularly in sound effects and dialogue, he raised concerns about limitations that remain in the system.
These include the lack of custom voice support, an inability to select generated voices directly, and the continued cap at 8-second generations—despite some public claims about longer outputs.
Davids also pointed out that character consistency across changing camera angles still requires careful prompting, whereas other models like Sora 2 handle this more automatically. He questioned the absence of 1080p resolution for users on paid tiers like Flow Pro and expressed skepticism over feature parity.
On the more positive end,@kimmonismus,an AI newsletter writer, stated that “Veo 3.1 is amazing,” though still concluded that OpenAI’s latest model remains preferable overall.
Collectively, these early impressions suggest that while Veo 3.1 delivers meaningful tooling enhancements and new creative control features, expectations have shifted as competitors raise the bar on both quality and usability.
Since launching Flow five months ago, Google says over275 million videoshave been generated across various Veo models.
The pace of adoption suggests significant interest not only from individuals but also from developers and businesses experimenting with automated content creation.
Thomas Iljic, Director of Product Management at Google Labs, highlights that Veo 3.1’s release brings capabilities closer to how human filmmakers plan and shoot. These include scene composition, continuity across shots, and coordinated audio—all areas that enterprises increasingly look to automate or streamline.
Videos generated with Veo 3.1 are watermarked using Google’sSynthIDtechnology, which embeds an imperceptible identifier to signal that the content is AI-generated.
Google applies safety filters and moderation across its APIs to help minimize privacy and copyright risks. Generated content is stored temporarily and deleted after two days unless downloaded.
For developers and enterprises, these features provide reassurance around provenance and compliance—critical in regulated or brand-sensitive industries.
Veo 3.1 is not just an iteration on prior models—it represents a deeper integration of multimodal inputs, storytelling control, and enterprise-level tooling. While creative professionals may see immediate benefits in editing workflows and fidelity, businesses exploring automation in training, advertising, or virtual experiences may find even greater value in the model’s composability and API support.
The early user feedback highlights that while Veo 3.1 offers valuable tooling, expectations around realism, voice control, and generation length are evolving rapidly. As Google expands access through Vertex AI and continues refining Veo, its competitive positioning in enterprise video generation will hinge on how quickly these user pain points are addressed."
Startup & Tech Media,Visa just launched a protocol to secure the AI shopping boom — here’s what it means for merchants.,https://venturebeat.com/ai/visa-just-launched-a-protocol-to-secure-the-ai-shopping-boom-heres-what-it,"Visais introducing a new security framework designed to solve one of the thorniest problems emerging in artificial intelligence-powered commerce: how retailers can tell the difference between legitimate AI shopping assistants and the malicious bots that plague their websites.
The payments giant unveiled itsTrusted Agent Protocolon Tuesday, establishing what it describes as foundational infrastructure for ""agentic commerce"" — a term for the rapidly growing practice of consumers delegating shopping tasks to AI agents that can search products, compare prices, and complete purchases autonomously.
The protocol enables merchants to cryptographically verify that an AI agent browsing their site is authorized and trustworthy, rather than a bot designed to scrape pricing data, test stolen credit cards, or carry out other fraudulent activities.
The launch comes as AI-driven traffic to U.S. retail websites hasexploded by more than 4,700%over the past year, according to data from Adobe cited by Visa. That dramatic surge has created an acute challenge for merchants whose existing bot detection systems — designed to block automated traffic — now risk accidentally blocking legitimate AI shoppers along with bad actors.
""Merchants need additional tools that provide them with greater insight and transparency into agentic commerce activities to ensure they can participate safely,"" said Rubail Birwadker, Visa's Global Head of Growth, in an exclusive interview with VentureBeat. ""Without common standards, potential risks include ecosystem fragmentation and the proliferation of closed loop models.""
The stakes are substantial. While 85% of shoppers who have used AI to shop report improved experiences, merchants face the prospect of either turning away legitimate AI-powered customers or exposing themselves to sophisticated bot attacks. Visa's own data shows the companyprevented $40 billion in fraudulent activitybetween October 2022 and September 2023, nearly double the previous year, much of it involving AI-powered enumeration attacks where bots systematically test combinations of card numbers until finding valid credentials.
Visa'sTrusted Agent Protocoloperates through what Birwadker describes as a ""cryptographic trust handshake"" between merchants and approved AI agents. The system works in three steps:
First, AI agents must be approved and onboarded through Visa'sIntelligent Commerceprogram, where they undergo vetting to meet trust and reliability standards. Each approved agent receives a unique digital signature key — essentially a cryptographic credential that proves its identity.
When an approved agent visits a merchant's website, it creates a digital signature using its key and transmits three categories of information: Agent Intent (indicating the agent is trusted and intends to retrieve product details or make a purchase), Consumer Recognition (data showing whether the underlying consumer has an existing account with the merchant), and Payment Information (optional payment data to support checkout).
Merchants or their infrastructure providers, such as content delivery networks, then validate these digital signatures against Visa's registry of approved agents. ""Upon proper validation of these fields, the merchant can confirm the signature is a trusted agent,"" Birwadker explained.
Crucially, Visa designed the protocol to require minimal changes to existing merchant infrastructure. Built on theHTTP Message Signature standardand aligned withWeb Both Auth, the protocol works with existing web infrastructure without requiring merchants to overhaul their checkout pages. ""This is no-code functionality,"" Birwadker emphasized, though merchants may need to integrate with Visa's Developer Center to access the verification system.
Visa developed the protocol in collaboration withCloudflare, the web infrastructure and security company that already provides bot management services to millions of websites. The partnership reflects Visa's recognition that solving bot verification requires cooperation across the entire web stack, not just the payments layer.
""Trusted Agent Protocol supplements traditional bot management by providing merchants insights that enable agentic commerce,"" Birwadker said. ""Agents are providing additional context they otherwise would not, including what it intends to do, who the underlying consumer is, and payment information.""
The protocol arrives as multiple technology giants race to establish competing standards for AI commerce. Google recently introduced itsAgent Protocol for Payments (AP2), whileOpenAIandStripehave discussed their own approaches to enabling AI agents to make purchases. Microsoft, Shopify, Adyen, Ant International, Checkout.com, Cybersource, Elavon, Fiserv, Nuvei, and Worldpay provided feedback during Trusted Agent Protocol's development, according to Visa.
When asked how Visa's protocol relates to these competing efforts, Birwadker struck a collaborative tone. ""Both Google's AP2 and Visa's Trusted Agent Protocol are working toward the same goal of building trust in agent-initiated payments,"" he said. ""We are engaged with Google, OpenAI, and Stripe and are looking to create compatibility across the ecosystem.""
Visa says it is working with global standards bodies including theInternet Engineering Task Force (IETF),OpenID Foundation, andEMVCoto ensure the protocol can eventually become interoperable with other emerging standards. ""While these specifications apply to the Visa network in this initial phase, enabling agents to safely and securely act on a consumer's behalf requires an open, ecosystem-wide approach,"" Birwadker noted.
The protocol raises important questions about authorization and liability when AI agents make purchases on behalf of consumers. If an agent completes an unauthorized transaction — perhaps misunderstanding a user's intent or exceeding its delegated authority — who bears responsibility?
Birwadker emphasized that the protocol helps merchants ""leverage this information to enable experiences tied to existing consumer relationships and more secure checkout,"" but he did not provide specific details about how disputes would be handled when agents make unauthorized purchases. Visa's existing fraud protection and chargeback systems would presumably apply, though the company has not yet published detailed guidance on agent-initiated transaction disputes.
The protocol also places Visa in the position of gatekeeper for the emerging agentic commerce ecosystem. Because Visa determines which AI agents get approved for theIntelligent Commerceprogram and receive cryptographic credentials, the company effectively controls which agents merchants can easily trust. ""Agents are approved and onboarded through the Visa Intelligent Commerce program, ensuring they meet our standards for trust and reliability,"" Birwadker said, though he did not detail the specific criteria agents must meet or whether Visa charges fees for approval.
This gatekeeping role could prove contentious, particularly if Visa's approval process favors large technology companies over startups, or if the company faces pressure to block agents from competitors or politically controversial entities. Visa declined to provide details about how many agents it has approved so far or how long the vetting process typically takes.
The protocol launch comes at a complex moment for Visa, which continues to navigate significant legal and regulatory challenges even as its core business remains robust. The company's latestearnings reportfor the third quarter of fiscal year 2025 showed a 10% increase in net revenues to $9.2 billion, driven by resilient consumer spending and strong growth in cross-border transaction volume. For the full fiscal year ending September 30, 2024, Visa processed 289 billion transactions, with a total payments volume of $15.2 trillion.

However, the company's legal headwinds have intensified. In July 2025, a federal judge rejected a landmark$30 billion settlementthat Visa and Mastercard had reached with merchants over long-disputed credit card swipe fees, sending the parties back to the negotiating table and extending the long-running legal battle.
Simultaneously,Visa remains under investigation by the Department of Justiceover its rules for routing debit card transactions, with regulators scrutinizing whether the company's practices unlawfully limit merchant choice and stifle competition. These domestic challenges are mirrored abroad, where European regulators have continued their own antitrust investigations into the fee structures of both Visa and its primary competitor, Mastercard.
Against this backdrop of regulatory pressure, Birwadker acknowledged that adoption of the Trusted Agent Protocol will take time. ""As agentic commerce continues to rise, we recognize that consumer trust is still in its early stages,"" he said. ""That's why our focus through 2025 is on building foundational credibility and demonstrating real-world value.""
The protocol is available immediately inVisa's Developer Centerand onGitHub, with agent onboarding already active and merchant integration resources available. But Birwadker declined to provide specific targets for how many merchants might adopt the protocol by the end of 2026. ""Adoption is aligned with the momentum we're already seeing,"" he said. ""The launch of our protocol marks another big step — it's not just a technical milestone, but a signal that the industry is beginning to unify.""
Industry analysts say merchant adoption will likely depend on how quickly agentic commerce grows as a percentage of overall e-commerce. While AI-driven traffic has surged dramatically, much of that consists of agents browsing and researching rather than completing purchases. If AI agents begin accounting for a significant share of completed transactions, merchants will face stronger incentives to adopt verification systems like Visa's protocol.
Visa's move reflects broader strategic bets on AI across the financial services industry. The company hasinvested $10 billion in technologyover the past five years to reduce fraud and increase network security, with AI and machine learning central to those efforts. Visa's fraud detection system analyzes over 500 different attributes for each transaction, using AI models to assign real-time risk scores to the 300 billion annual transactions flowing through its network.
""Every single one of those transactions has been processed by AI,"" James Mirfin, Visa's global head of risk and identity solutions, said in aJuly 2024 CNBC interviewdiscussing the company's fraud prevention efforts. ""If you see a new type of fraud happening, our model will see that, it will catch it, it will score those transactions as high risk and then our customers can decide not to approve those transactions.""
The company has also moved aggressively into new payment territories beyond its core card business. In January 2025,Visa partnered with Elon Musk's X(formerly Twitter) to provide the infrastructure for a digital wallet and peer-to-peer payment service called the X Money Account, competing with services like Venmo and Zelle. That deal marked Visa's first major partnership in the social media payments space and reflected the company's recognition that payment flows are increasingly happening outside traditional e-commerce channels.
The agentic commerce protocol represents an extension of this strategy — an attempt to ensure Visa remains central to payment flows even as the mechanics of shopping shift from direct human interaction to AI intermediation.Jack Forestell, Visa's Chief Product & Strategy Officer, framed the protocol in expansive terms: ""We believe the entire payments ecosystem has a responsibility to ensure sellers trust AI agents with the same confidence they place in their most valued customers and networks.""
The real test for Visa's protocol won't be technical — it will be political. As AI agents become a larger force in retail, whoever controls the verification infrastructure controls access to hundreds of billions of dollars in commerce. Visa's position as gatekeeper gives it enormous leverage, but also makes it a target.
Merchants chafing under Visa's existing fee structure and facing multiple antitrust investigations may resist ceding even more power to the payments giant. Competitors like Google and OpenAI, each with their own ambitions in commerce, have little incentive to let Visa dictate standards. Regulators already scrutinizing Visa's market dominance will surely examine whether its agent approval process unfairly advantages certain players.
And there's a deeper question lurking beneath the technical specifications and corporate partnerships: In an economy increasingly mediated by AI, who decides which algorithms get to spend our money? Visa is making an aggressive bid to be that arbiter, wrapping its answer in the language of security and interoperability. Whether merchants, consumers, and regulators accept that proposition will determine not just the fate of the Trusted Agent Protocol, but the structure of AI-powered commerce itself.
For now, Visa is moving forward with the confidence of a company that has weathered disruption before. But in the emerging world of agentic commerce, being too trusted might prove just as dangerous as not being trusted enough.
"
Startup & Tech Media,Salesforce bets on AI 'agents' to fix what it calls a $7 billion problem in enterprise software.,https://venturebeat.com/ai/salesforce-bets-on-ai-agents-to-fix-what-it-calls-a-usd7-billion-problem-in,"As 50,000 attendees descend on Salesforce'sDreamforce conferencethis week, the enterprise software giant is making its most aggressive bet yet on artificial intelligence agents, positioning itself as the antidote to what it calls an industry-wide ""pilot purgatory"" where95% of enterprise AI projects never reach production.
The company on Monday launchedAgentforce 360, a sweeping reimagination of its entire product portfolio designed to transform businesses into what it calls ""agentic enterprises"" — organizations where AI agents work alongside humans to handle up to 40% of work across sales, service, marketing, and operations.
""We are truly in the agentic AI era, and I think it's probably the biggest revolution, the biggest transition in technology I've ever experienced in my career,"" said Parker Harris, Salesforce's co-founder and chief technology officer, during a recent press briefing. ""In the future, 40% of the work in the Fortune 1000 is probably going to be done by AI, and it's going to be humans and AI actually working together.""
The announcement comes at a pivotal moment for Salesforce, which has deployed more than 12,000 AI agent implementations over the past year while building what Harris called a ""$7 billion business"" around its AI platform. Yet the launch also arrives amid unusual turbulence, as CEO Marc Benioff facesfierce backlashfor recent commentssupporting President Trumpand suggesting National Guard troops should patrol San Francisco streets.
The stakes are enormous. While companies have rushed to experiment with AI following ChatGPT's emergence two years ago, most enterprise deployments have stalled before reaching production, according torecent MIT researchthat Salesforce executives cited extensively.
""Customers have invested a lot in AI, but they're not getting the value,"" said Srini Tallapragada, Salesforce's president and chief engineering and customer success officer. ""95% of enterprise AI pilots fail before production. It's not because of lack of intent. People want to do this. Everybody understands the power of the technology. But why is it so hard?""
The answer, according to Tallapragada, is that AI tools remain disconnected from enterprise workflows, data, and governance systems. ""You're writing prompts, prompts, you're getting frustrated because the context is not there,"" he said, describing what he called a ""prompt doom loop.""
Salesforce's solution is a deeply integrated platform connecting what it calls four ingredients: the Agentforce 360 agent platform, Data 360 for unified data access, Customer 360 apps containing business logic, and Slack as the ""conversational interface"" where humans and agents collaborate.
Perhaps the most significant strategic shift is the elevation ofSlack— acquired by Salesforce in 2019 for$27.7 billion— as the primary interface for Salesforce itself. The company is effectively reimagining its traditional Lightning interface around Slack channels, where sales deals, service cases, and data insights will surface conversationally rather than through forms and dashboards.
""Imagine that you maybe don't log into Salesforce, you don't see Salesforce, but it's there. It's coming to you in Slack, because that's where you're getting your work done,"" Harris explained.
The strategy includes embedding Salesforce's Agentforce agents for sales, IT service, HR service, and analytics directly into Slack, alongside a completely rebuilt Slackbot that acts as a personal AI companion. The company is also launching ""Channel Expert,"" an always-on agent that provides instant answers from channel conversations.
To enable third-party AI tools to access Slack's conversational data, Salesforce is releasing aReal-Time Search APIandModel Context Protocol server. Partners including OpenAI, Anthropic, Google, Perplexity, Writer, Dropbox, Notion, and Cursor are building agents that will live natively in Slack.
""The best way to see the power of the platform is through the AI apps and agents already being built,"" Rob Seaman, a Salesforce executive, said during a technical briefing, citing examples of startups ""achieving tens of thousands of customers that have it installed in 120 days or less.""
Beyond Slack integration, Salesforce announced major expansions into voice-based interactions and employee service.Agentforce Voice, now generally available, transforms traditional IVR systems into natural conversations that can update CRM records, trigger workflows, and seamlessly hand off to human agents.
The IT Service offering represents Salesforce's most direct challenge toServiceNow, the market leader. Mudhu Sudhakar, who joined Salesforce two months ago as senior vice president for IT and HR Service, positioned the product as a fundamental reimagining of employee support.
""Legacy IT service management is very portals, forms, tickets focused, manual process,"" Sudhakar said. ""What we had a few key tenets: conversation first and agent first, really focused on having a conversational experience for the people requesting the support and for the people providing the support.""
The IT Service platform includes what Salesforce describes as 25+ specialized agents and 100+ pre-built workflows and connectors that can handle everything from password resets to complex incident management.
Customer results suggest the approach is gaining traction.Redditreduced average support resolution time from 8.9 minutes to 1.4 minutes — an 84% improvement — while deflecting 46% of cases entirely to AI agents. ""This efficiency has allowed us to provide on-demand help for complex tasks and boost advertiser satisfaction scores by 20%,"" said John Thompson, Reddit's VP of sales strategy and operations, in a statement.
Engine, a travel management company, reduced average handle time by 15%, saving over $2 million annually.OpenTableresolved 70% of restaurant and diner inquiries autonomously. And 1-800Accountant achieved a 90% case deflection rate during the critical tax week period.
Salesforce's own internal deployments may be most telling. Tallapragada's customer success organization now handles 1.8 million AI-powered conversations weekly, with metrics published athelp.salesforce.comshowing how many agents answer versus escalating to humans.
Even more significantly, Salesforce has deployed AI-powered sales development representatives to follow up on leads that would previously have gone uncontacted due to cost constraints. ""Now, Agentforce has an SDR which is doing thousands of leads following up,"" Tallapragada explained. The company also increased proactive customer outreach by 40% by shifting staff from reactive support.
Given enterprise concerns about AI reliability, Salesforce has invested heavily in what it calls the ""trust layer"" — audit trails, compliance checks, and observability tools that let organizations monitor agent behavior at scale.
""You should think of an agent as a human. Digital labor. You need to manage performance just like a human. And you need these audit trails,"" Tallapragada explained.
The company encountered this challenge firsthand when its own agent deployment scaled. ""When we started at Agentforce at Salesforce, we would track every message, which is great until 1,000, 3,000,"" Tallapragada said. ""Once you have a million chats, there's no human, we cannot do it.""
The platform now includes ""Agentforce Grid"" for searching across millions of conversations to identify and fix problematic patterns. The company also introduced Agent Script, a new scripting language that allows developers to define precise guardrails and deterministic controls for agent behavior.
Underlying the agent capabilities is significant infrastructure investment. Salesforce'sData 360includes ""Intelligent Context,"" which automatically extracts structured information from unstructured content like PDFs, diagrams, and flowcharts using what the company describes as ""AI-powered unstructured data pipelines.""
The company is also collaborating withDatabricks,dbt Labs, andSnowflakeon the ""Universal Semantic Interchange,"" an attempt to standardize how different platforms define business metrics. The pending $8 billion acquisition of Informatica, expected to close soon, will expand metadata management capabilities across the enterprise.
Salesforce's aggressive AI agent push comes as virtually every major enterprise software vendor pursues similar strategies. Microsoft has embedded Copilot across its product line, Google offers agent capabilities through Vertex AI and Gemini, and ServiceNow has launched its own agentic offerings.
When asked how Salesforce's announcement compared toOpenAI's recent releases, Tallapragada emphasized that customers will use multiple AI tools simultaneously. ""Most of the time I'm seeing they're using OpenAI, they're using Gemini, they're using Anthropic, just like Salesforce, we use all three,"" he said.
The real differentiation, executives argued, lies not in the AI models but in the integration with business processes and data. Harris framed the competition in terms familiar from Salesforce's founding: ""26 years ago, we just said, let's make Salesforce automation as easy as buying a book on Amazon.com. We're doing that same thing. We want to make agentic AI as easy as buying a book on Amazon.""
The company's customer success stories are impressive but remain a small fraction of its customer base. With 150,000 Salesforce customers and one million Slack customers, the 12,000 Agentforce deployments represent roughly 8% penetration — strong for a one-year-old product line, but hardly ubiquitous.
The company's stock, downroughly 28% year to datewith aRelative Strength rating of just 15, suggests investors remain skeptical. This week's Dreamforce demonstrations — and the months of customer deployments that follow — will begin to provide answers to whether Salesforce can finally move enterprise AI from pilots to production at scale, or whether the ""$7 billion business"" remains more aspiration than reality.
"
Startup & Tech Media,The most important OpenAI announcement you probably missed at DevDay 2025.,https://venturebeat.com/ai/the-most-important-openai-announcement-you-probably-missed-at-devday-2025,"OpenAI’s annual developer conference on Monday was a spectacle of ambitious AI product launches, from anapp store for ChatGPTto a stunningvideo-generation APIthat brought creative concepts to life. But for the enterprises and technical leaders watching closely, the most consequential announcement was the quietgeneral availability of Codex, the company's AI software engineer. This release signals a profound shift in how software—and by extension, modern business—is built.
While other announcements captured the public’s imagination, the production-ready release ofCodex, supercharged by anew specialized modeland asuite of enterprise-grade tools, is the engine behind OpenAI’s entire vision. It is the tool that builds the tools, the proven agent in a world buzzing with agentic potential, and the clearest articulation of the company's strategy to win the enterprise.
Thegeneral availability of Codexmoves it from a ""research preview"" to a fully supported product, complete with a newsoftware development kit (SDK), aSlack integration, and administrative controls for security and monitoring.This transition declares that Codex is ready for mission-critical work inside the world’s largest companies.
""We think this is the best time in history to be a builder; it has never been faster to go from idea to product,"" said OpenAI CEO Sam Altman during theopening keynotepresentation. ""Software used to take months or years to build. You saw that it can take minutes now to build with AI.""
That acceleration is not theoretical. It's a reality born from OpenAI’s own internal use — a massive ""dogfooding"" effort that serves as the ultimate case study for enterprise customers.
At the heart of the Codex upgrade isGPT-5-Codex, a version of OpenAI's latest flagship model that has been ""purposely trained for Codex and agentic coding."" The new model is designed to function as an autonomous teammate, moving far beyond simple code autocompletion.
""I personally like to think about it as a little bit like a human teammate,"" explained Tibo Sottiaux, an OpenAI engineer, during a technical session on Codex. ""You can pair a program with it on your computer, you can delegate to it, or as you'll see, you can give it a job without explicit prompting.""
This new model enables ""adaptive thinking,"" allowing it to dynamically adjust the time and computational effort spent on a task based on its complexity.For simple requests, it's fast and efficient, but for complex refactoring projects, it can work for hours.
One engineer during the technical session noted, ""I've seen the GPT-5-Codex model work for over seven hours productively... on a marathon session."" This capability to handle long-running, complex tasks is a significant leap beyond the simple, single-shot interactions that define most AI coding assistants.
The results inside OpenAI have been dramatic. The company reported that 92% of its technical staff now uses Codex daily, and those engineers complete 70% more pull requests (a measure of code contribution) each week. Usage has surged tenfold since August.
""When we as a team see the stats, it feels great,"" Sottiaux shared. ""But even better is being at lunch with someone who then goes 'Hey I use Codex all the time. Here's a cool thing that I do with it. Do you want to hear about it?'""
Perhaps the most compelling argument for Codex’s importance is that it is the foundational layer upon which OpenAI’s other flashy announcements were built. During theDevDay event, the company showcased custom-built arcade games and a dynamic, AI-powered website for the conference itself, all developed usingCodex.
In one session, engineers demonstrated how they built ""Storyboard,"" a custom creative tool for the film industry, in just 48 hours during an internal hackathon. ""We decided to test Codex, our coding agent... we would send tasks to Codex in between meetings. We really easily reviewed and merged PRs into production, which Codex even allowed us to do from our phones,"" said Allison August, a solutions engineering leader at OpenAI.
This reveals a critical insight: the rapid innovation showcased at DevDay is a direct result of the productivity flywheel created byCodex. The AI is a core part of the manufacturing process for all other AI products.
A key enterprise-focused feature is the new, more robust code review capability. OpenAI said it ""purposely trained GPT-5-Codex to be great at ultra thorough code review,"" enabling it to explore dependencies and validate a programmer's intent against the actual implementation to find high-quality bugs.Internally, nearly every pull request at OpenAI is now reviewed by Codex, catching hundreds of issues daily before they reach a human reviewer.
""It saves you time, you ship with more confidence,"" Sottiaux said. ""There's nothing worse than finding a bug after we actually ship the feature.""
The maturation ofCodexis central to OpenAI’s broader strategy to conquer the enterprise market, a move essential to justifying its massive valuation and unprecedented compute expenditures. During a press conference, CEO Sam Altman confirmed the strategic shift.
""The models are there now, and you should expect a huge focus from us on really winning enterprises with amazing products, starting here,"" Altman said during a private press conference.
OpenAI President and Co-founder Greg Brockman immediately added, ""And you can see it already with Codex, which I think has been just an incredible success and has really grown super fast.""
For technical decision-makers, the message is clear. While consumer-facing agents that book dinner reservations are still finding their footing,Codexis a proven enterprise agent delivering substantial ROI today. Companies like Cisco have already rolled out Codex to their engineering organizations, cutting code review times by 50% and reducing project timelines from weeks to days.
With the newCodex SDK, companies can now embed this agentic power directly into their own custom workflows, such as automating fixes in a CI/CD pipeline or even creating self-evolving applications. During a live demo, an engineer showcased a mobile app that updated its own user interface in real-time based on a natural language prompt, all powered by the embedded Codex SDK.
While the launch of anapp ecosystem in ChatGPTand the breathtaking visuals of theSora 2 APIrightfully generated headlines, thegeneral availability of Codexmarks a more fundamental and immediate transformation. It is the quiet but powerful engine driving the next era of software development, turning the abstract promise of AI-driven productivity into a tangible, deployable reality for businesses today."
Startup & Tech Media,Salesforce launches AI 'trust layer' to tackle enterprise deployment failures plaguing 80% of projects.,https://venturebeat.com/ai/salesforce-launches-ai-trust-layer-to-tackle-enterprise-deployment-failures,"Salesforce Inc.is expanding its artificial intelligence platform with new data management and governance capabilities, aiming to address what the company says is a crisis in enterprise AI adoption where more than 80% of projects fail to deliver meaningful business value.
The San Francisco-based software giant announced Thursday a suite of new tools designed to create what it calls a ""trusted AI foundation"" for enterprises struggling with fragmented data, weak governance, and security concerns that have hampered AI deployments across corporate America.
""We're seeing a lot of these AI projects really failing, and a lot of it's because customers still have fragmented data, they still have weak governance, they still have poor security,"" said Desiree Motamedi, Salesforce's senior vice president and chief marketing officer, in an exclusive interview with VentureBeat. ""They really want a way that they can bring AI at scale that has the accuracy, the context and the control.""
The timing of Salesforce's announcement comes as the company prepares for its annualDreamforce conferencenext week, where CEO Marc Benioff is expected to showcase the company's vision for what he calls the ""agentic enterprise"" — workplaces where AI agents work alongside humans across every business function.
The scale of AI project failures has become a significant concern for enterprise technology leaders. According to aRAND Corporation study, poor data quality, inadequate governance frameworks, and fragmented system integration are the primary culprits behind the high failure rate of corporate AI initiatives.
This challenge has created both pressure and opportunity for enterprise software providers. While companies face mounting pressure to deploy AI capabilities, many are discovering that their existing data infrastructure isn't equipped to support reliable AI applications at scale.
Salesforce's response centers on what Motamedi describes as three core capabilities: ensuring AI outputs are grounded in unified business data, embedding security and compliance controls into every workflow, and connecting AI agents across different platforms and data sources.
""The Salesforce platform is a $7 billion business,"" Motamedi noted, highlighting the significant revenue opportunity the company sees in addressing enterprise AI infrastructure needs. ""This is a significant opportunity where we're seeing meaningful differentiation from other vendors in the market.""
The company's latest announcements include several technically sophisticated solutions aimed at different aspects of the enterprise AI challenge:
Data Cloud Context Indexingrepresents Salesforce's approach to handling unstructured content like contracts, technical diagrams, and decision trees. The system uses what the company calls a ""business-aware lens"" to help AI agents interpret complex documents within their proper business context.
""A good example is a field engineer who uploads a schematic for guided troubleshooting,"" Motamedi explained. ""Now they have that capability at their disposal, because it's right there in that view.""
Data Cloud Clean Rooms, now generally available, allows organizations to securely share and analyze data with partners without exposing sensitive information. Using Salesforce's ""zero copy"" technology, companies can collaborate on data analysis without actually moving or duplicating datasets.
The clean room technology extends beyond traditional advertising applications to sectors like banking, where institutions could ""detect fraud, and they want to be able to do it with some of their partners. They could now do it in hours versus weeks,"" according to Motamedi.
Tableau Semanticsaddresses one of the most persistent challenges in enterprise data management: ensuring consistent definitions of business metrics across different systems and teams. The AI-powered semantic layer translates raw data into standardized business language.
""We use terms like ACV or churn that have specific definitions within our organization,"" Motamedi said. ""Making sure AI understands those definitions, and then having a standardized layer across organizations, really makes this seamless for enterprises.""
MuleSoft Agent Fabrictackles what Salesforce calls ""agent sprawl"" — the proliferation of AI agents across different platforms and vendors within large organizations. The system provides centralized registration, orchestration, and governance for AI agents regardless of where they were built.
Salesforce's comprehensive approach to AI infrastructure positions the company in direct competition withMicrosoft,Google,Amazon, andServiceNow, all of which are vying to become the dominant platform for enterprise AI deployment.
The company's strategy relies heavily on integration advantages that come from building AI capabilities into an existing platform used by thousands of enterprises. ""The power of the platform"" lies in the fact that ""all of this is natively into the platform. So these capabilities are just there, and they work and they work seamlessly together,"" Motamedi emphasized.
This integrated approach contrasts with point solutions that require custom integration work. ""Some of these point solutions, if you want these things to work together, you got to build those integrations. You got to have developer teams to make that happen,"" she noted.
The company's pending $8 billion acquisition of data management company Informatica, expected to close soon, will significantly expand Salesforce's capabilities in enterprise metadata management — a critical component for AI accuracy.
""For the last 26 years, Salesforce has been rooted in our platform approach — we've built the metadata layer from day one,"" Motamedi said. ""But with Informatica, we're going to see metadata across the entire enterprise, and that gives us another layer of accuracy for AI responses.""
Despite the technical capabilities, Salesforce acknowledges that enterprise AI adoption remains in early stages. The company reports having ""over 12,000 live deployments of Agentforce"" — its AI agent platform — but Motamedi describes a wide range of organizational readiness.
""Every company has a mandate right now to figure out how they can incorporate AI,"" she said. ""We see very interesting ranges from people who are just getting started to people who are like, we're going to build like 80 different agents within their organization.""
Early customer implementations includeAAA Washington, which is using Salesforce's unified data foundation to improve member experiences across roadside assistance, insurance, and travel services.UChicago Medicineis leveraging the platform to ensure reliable patient interactions while enabling healthcare staff to focus on complex, human-centered care.
The maturity curve for enterprise AI adoption means ""it's going to take a couple years to see it fully, fully embraced, but we already see the path,"" according to Motamedi.
The broader implications of Salesforce's strategy extend beyond technical capabilities to fundamental questions about how enterprises will manage AI risk and governance. The company's emphasis on built-in security and compliance reflects growing corporate awareness that AI deployment without proper controls can create significant business liability.
Recent incidents involving AI agents accessing sensitive information or providing unreliable outputs have made corporate leaders more cautious about scaling AI initiatives. Salesforce's approach of embedding security directly into AI workflows — including automated threat detection partnerships withCrowdStrikeandOkta, and built-in HIPAA compliance for healthcare applications — represents an attempt to address these concerns while accelerating adoption.
However, market skepticism remains. CNBC's Jim Cramerrecently noted concernsabout Salesforce's performance despite strong quarterly reports, suggesting that investor expectations for AI-driven growth may be outpacing actual business results.
The company's success will ultimately depend on whether it can help enterprises bridge the gap between AI experimentation and production-scale deployment. As Motamedi framed it: ""We really believe that we have a trust layer for enterprise AI with all of these new announcements, and we're really helping companies move from cautious pilots to transformative action.""
Whether that vision becomes reality will depend on Salesforce's ability to prove that integrated platforms can solve enterprise AI's trust problem better than the patchwork of point solutions most companies rely on today. In an industry where80% of projects fail, the company that finally cracks the code on reliable, scalable enterprise AI could reshape how business gets done — or discover that the technical challenges run deeper than any single platform can solve.
"
Startup & Tech Media,Visa just launched a protocol to secure the AI shopping boom — here’s what it means for merchants.,https://venturebeat.com/ai/visa-just-launched-a-protocol-to-secure-the-ai-shopping-boom-heres-what-it,"Visais introducing a new security framework designed to solve one of the thorniest problems emerging in artificial intelligence-powered commerce: how retailers can tell the difference between legitimate AI shopping assistants and the malicious bots that plague their websites.
The payments giant unveiled itsTrusted Agent Protocolon Tuesday, establishing what it describes as foundational infrastructure for ""agentic commerce"" — a term for the rapidly growing practice of consumers delegating shopping tasks to AI agents that can search products, compare prices, and complete purchases autonomously.
The protocol enables merchants to cryptographically verify that an AI agent browsing their site is authorized and trustworthy, rather than a bot designed to scrape pricing data, test stolen credit cards, or carry out other fraudulent activities.
The launch comes as AI-driven traffic to U.S. retail websites hasexploded by more than 4,700%over the past year, according to data from Adobe cited by Visa. That dramatic surge has created an acute challenge for merchants whose existing bot detection systems — designed to block automated traffic — now risk accidentally blocking legitimate AI shoppers along with bad actors.
""Merchants need additional tools that provide them with greater insight and transparency into agentic commerce activities to ensure they can participate safely,"" said Rubail Birwadker, Visa's Global Head of Growth, in an exclusive interview with VentureBeat. ""Without common standards, potential risks include ecosystem fragmentation and the proliferation of closed loop models.""
The stakes are substantial. While 85% of shoppers who have used AI to shop report improved experiences, merchants face the prospect of either turning away legitimate AI-powered customers or exposing themselves to sophisticated bot attacks. Visa's own data shows the companyprevented $40 billion in fraudulent activitybetween October 2022 and September 2023, nearly double the previous year, much of it involving AI-powered enumeration attacks where bots systematically test combinations of card numbers until finding valid credentials.
Visa'sTrusted Agent Protocoloperates through what Birwadker describes as a ""cryptographic trust handshake"" between merchants and approved AI agents. The system works in three steps:
First, AI agents must be approved and onboarded through Visa'sIntelligent Commerceprogram, where they undergo vetting to meet trust and reliability standards. Each approved agent receives a unique digital signature key — essentially a cryptographic credential that proves its identity.
When an approved agent visits a merchant's website, it creates a digital signature using its key and transmits three categories of information: Agent Intent (indicating the agent is trusted and intends to retrieve product details or make a purchase), Consumer Recognition (data showing whether the underlying consumer has an existing account with the merchant), and Payment Information (optional payment data to support checkout).
Merchants or their infrastructure providers, such as content delivery networks, then validate these digital signatures against Visa's registry of approved agents. ""Upon proper validation of these fields, the merchant can confirm the signature is a trusted agent,"" Birwadker explained.
Crucially, Visa designed the protocol to require minimal changes to existing merchant infrastructure. Built on theHTTP Message Signature standardand aligned withWeb Both Auth, the protocol works with existing web infrastructure without requiring merchants to overhaul their checkout pages. ""This is no-code functionality,"" Birwadker emphasized, though merchants may need to integrate with Visa's Developer Center to access the verification system.
Visa developed the protocol in collaboration withCloudflare, the web infrastructure and security company that already provides bot management services to millions of websites. The partnership reflects Visa's recognition that solving bot verification requires cooperation across the entire web stack, not just the payments layer.
""Trusted Agent Protocol supplements traditional bot management by providing merchants insights that enable agentic commerce,"" Birwadker said. ""Agents are providing additional context they otherwise would not, including what it intends to do, who the underlying consumer is, and payment information.""
The protocol arrives as multiple technology giants race to establish competing standards for AI commerce. Google recently introduced itsAgent Protocol for Payments (AP2), whileOpenAIandStripehave discussed their own approaches to enabling AI agents to make purchases. Microsoft, Shopify, Adyen, Ant International, Checkout.com, Cybersource, Elavon, Fiserv, Nuvei, and Worldpay provided feedback during Trusted Agent Protocol's development, according to Visa.
When asked how Visa's protocol relates to these competing efforts, Birwadker struck a collaborative tone. ""Both Google's AP2 and Visa's Trusted Agent Protocol are working toward the same goal of building trust in agent-initiated payments,"" he said. ""We are engaged with Google, OpenAI, and Stripe and are looking to create compatibility across the ecosystem.""
Visa says it is working with global standards bodies including theInternet Engineering Task Force (IETF),OpenID Foundation, andEMVCoto ensure the protocol can eventually become interoperable with other emerging standards. ""While these specifications apply to the Visa network in this initial phase, enabling agents to safely and securely act on a consumer's behalf requires an open, ecosystem-wide approach,"" Birwadker noted.
The protocol raises important questions about authorization and liability when AI agents make purchases on behalf of consumers. If an agent completes an unauthorized transaction — perhaps misunderstanding a user's intent or exceeding its delegated authority — who bears responsibility?
Birwadker emphasized that the protocol helps merchants ""leverage this information to enable experiences tied to existing consumer relationships and more secure checkout,"" but he did not provide specific details about how disputes would be handled when agents make unauthorized purchases. Visa's existing fraud protection and chargeback systems would presumably apply, though the company has not yet published detailed guidance on agent-initiated transaction disputes.
The protocol also places Visa in the position of gatekeeper for the emerging agentic commerce ecosystem. Because Visa determines which AI agents get approved for theIntelligent Commerceprogram and receive cryptographic credentials, the company effectively controls which agents merchants can easily trust. ""Agents are approved and onboarded through the Visa Intelligent Commerce program, ensuring they meet our standards for trust and reliability,"" Birwadker said, though he did not detail the specific criteria agents must meet or whether Visa charges fees for approval.
This gatekeeping role could prove contentious, particularly if Visa's approval process favors large technology companies over startups, or if the company faces pressure to block agents from competitors or politically controversial entities. Visa declined to provide details about how many agents it has approved so far or how long the vetting process typically takes.
The protocol launch comes at a complex moment for Visa, which continues to navigate significant legal and regulatory challenges even as its core business remains robust. The company's latestearnings reportfor the third quarter of fiscal year 2025 showed a 10% increase in net revenues to $9.2 billion, driven by resilient consumer spending and strong growth in cross-border transaction volume. For the full fiscal year ending September 30, 2024, Visa processed 289 billion transactions, with a total payments volume of $15.2 trillion.

However, the company's legal headwinds have intensified. In July 2025, a federal judge rejected a landmark$30 billion settlementthat Visa and Mastercard had reached with merchants over long-disputed credit card swipe fees, sending the parties back to the negotiating table and extending the long-running legal battle.
Simultaneously,Visa remains under investigation by the Department of Justiceover its rules for routing debit card transactions, with regulators scrutinizing whether the company's practices unlawfully limit merchant choice and stifle competition. These domestic challenges are mirrored abroad, where European regulators have continued their own antitrust investigations into the fee structures of both Visa and its primary competitor, Mastercard.
Against this backdrop of regulatory pressure, Birwadker acknowledged that adoption of the Trusted Agent Protocol will take time. ""As agentic commerce continues to rise, we recognize that consumer trust is still in its early stages,"" he said. ""That's why our focus through 2025 is on building foundational credibility and demonstrating real-world value.""
The protocol is available immediately inVisa's Developer Centerand onGitHub, with agent onboarding already active and merchant integration resources available. But Birwadker declined to provide specific targets for how many merchants might adopt the protocol by the end of 2026. ""Adoption is aligned with the momentum we're already seeing,"" he said. ""The launch of our protocol marks another big step — it's not just a technical milestone, but a signal that the industry is beginning to unify.""
Industry analysts say merchant adoption will likely depend on how quickly agentic commerce grows as a percentage of overall e-commerce. While AI-driven traffic has surged dramatically, much of that consists of agents browsing and researching rather than completing purchases. If AI agents begin accounting for a significant share of completed transactions, merchants will face stronger incentives to adopt verification systems like Visa's protocol.
Visa's move reflects broader strategic bets on AI across the financial services industry. The company hasinvested $10 billion in technologyover the past five years to reduce fraud and increase network security, with AI and machine learning central to those efforts. Visa's fraud detection system analyzes over 500 different attributes for each transaction, using AI models to assign real-time risk scores to the 300 billion annual transactions flowing through its network.
""Every single one of those transactions has been processed by AI,"" James Mirfin, Visa's global head of risk and identity solutions, said in aJuly 2024 CNBC interviewdiscussing the company's fraud prevention efforts. ""If you see a new type of fraud happening, our model will see that, it will catch it, it will score those transactions as high risk and then our customers can decide not to approve those transactions.""
The company has also moved aggressively into new payment territories beyond its core card business. In January 2025,Visa partnered with Elon Musk's X(formerly Twitter) to provide the infrastructure for a digital wallet and peer-to-peer payment service called the X Money Account, competing with services like Venmo and Zelle. That deal marked Visa's first major partnership in the social media payments space and reflected the company's recognition that payment flows are increasingly happening outside traditional e-commerce channels.
The agentic commerce protocol represents an extension of this strategy — an attempt to ensure Visa remains central to payment flows even as the mechanics of shopping shift from direct human interaction to AI intermediation.Jack Forestell, Visa's Chief Product & Strategy Officer, framed the protocol in expansive terms: ""We believe the entire payments ecosystem has a responsibility to ensure sellers trust AI agents with the same confidence they place in their most valued customers and networks.""
The real test for Visa's protocol won't be technical — it will be political. As AI agents become a larger force in retail, whoever controls the verification infrastructure controls access to hundreds of billions of dollars in commerce. Visa's position as gatekeeper gives it enormous leverage, but also makes it a target.
Merchants chafing under Visa's existing fee structure and facing multiple antitrust investigations may resist ceding even more power to the payments giant. Competitors like Google and OpenAI, each with their own ambitions in commerce, have little incentive to let Visa dictate standards. Regulators already scrutinizing Visa's market dominance will surely examine whether its agent approval process unfairly advantages certain players.
And there's a deeper question lurking beneath the technical specifications and corporate partnerships: In an economy increasingly mediated by AI, who decides which algorithms get to spend our money? Visa is making an aggressive bid to be that arbiter, wrapping its answer in the language of security and interoperability. Whether merchants, consumers, and regulators accept that proposition will determine not just the fate of the Trusted Agent Protocol, but the structure of AI-powered commerce itself.
For now, Visa is moving forward with the confidence of a company that has weathered disruption before. But in the emerging world of agentic commerce, being too trusted might prove just as dangerous as not being trusted enough.
"
Startup & Tech Media,Weaponized AI can dismantle patches in 72 hours — but Ivanti's kernel defense can help.,https://venturebeat.com/security/weaponized-ai-can-dismantle-patches-in-72-hours-but-ivantis-kernel-defense,"Adversaries from cybercrime gangs to nation-state cyberattack squads are fine-tuning weaponized AI with the goal of defeating new patches in 3 days or less.
The quicker the attack, the more time to explore a victim’s network, exfiltrate data, install ransomware or set up reconnaissance that will last for months or years. Traditional, manual patching is now a liability, rendering enter organizations defenseless against weaponized AI attacks
""Threat actors are reverse engineering patches, and the speed at which they're doing it has been enhanced greatly by AI,"" Mike Riemer, SVP of Network Security Group and Field CISO atIvantitold VentureBeat in a recent interview. ""They're able to reverse engineer a patch within 72 hours. So if I release a patch and a customer doesn't patch within 72 hours of that release, they're open to exploit.""
This isn't theoretical speculation. It's the hard reality forcing vendors to rearchitect their security infrastructure from the kernel up completely. Last week, Ivanti released Connect Secure (ICS) version 25.X, marking what Riemer calls ""tangible evidence"" of the company's commitment to meeting this threat head-on.
AtDEF CON 33researchers fromAmberWolfproved this threat real, demonstrating complete authentication bypasses inZscaler,Netskope, and Check Point by exploiting vulnerabilities that existed for months, including Zscaler's failure to validate SAML assertions (CVE-2025-54982), Netskope’s credential-free OrgKey access, and Check Point’s hard-coded SFTP keys exposing tenant logs were all flaws left open and exploitable more than 16 months after initial disclosure.
The kernel is the central orchestrator of everything that happens in a computing device, controlling memory, processes, and hardware.
If an attacker compromises the kernel, they've seized total control of a device that can scale to compromising an entire network. Any other security layer or application, platform or safeguard is immediately bypassed with attackers take control of the kernel.
Nearly all operating systems rely on the concept ofenforcing rings of privilege. Applications run in user mode with limited access. The kernel operates in kernel mode with complete control. When adversaries break that barrier, they’ve gained access to what many security researchers consider the holy grail of a systems and entire networks’ vulnerabilities.
Ivanti's new release directly addresses this reality. Connect Secure 25.X runs on an enterprise-grade Oracle Linux operating system with strong Security-Enhanced Linux (SELinux) enforcement that can limit a threat actor's abilities within the system. The solution includes Secure Boot protection, disk encryption, key management, secure factory reset, a modern secure web server, and Web Application Firewall (WAF), all designed to secure key aspects of the system and significantly deter external threats.
""In the past year, we've significantly advanced our Secure by Design strategy, translating our commitment into real action through substantial investments and an expanded security team,"" Riemer explained. ""This release stands as tangible evidence of our commitment. We listened to our customers, invested in both technology and talent, and modernized the security of Ivanti Connect Secure to provide the resilience and peace of mind our customers expect and deserve.""
While operating system rings define privilege levels, modern patch management has adopted its own ring strategy to combat the 72-hour exploit window.
Ring deploymentprovides a phased, automated patching strategy that rolls out updates incrementally: a Test Ring for core IT validation, an Early Adopter Ring for compatibility testing, and a Production Ring for enterprise-wide rollout.
This approach directly addresses the speed crisis. Ring deployment achieves 99% patch success within 24 hours for up to 100,000 PCs, according toGartner research.Ponemon Instituteresearch shows organizations take an alarming average of 43 days to detect cyberattacks even after a patch is released.
Jesse Miller, SVP and director of IT atSouthstar Bank, emphasized: ""When judging how impactful something can be, you have to take everything from current events, your industry, your environment and more into the equation."" His teamuses ring deployment to reduce their attack surface as quickly as possible.
Attackers aggressively exploit legacy vulnerabilities with 76% of vulnerabilities leveraged by ransomware were reported between 2010 and 2019. When kernel access is at stake, every hour of delay multiplies the risk exponentially.
At CrowdStrike's FalCon conference, Chief Technology Innovation Officer Alex Ionescu laid out the problem: ""By now, it's clear that if you want to protect against bad actors, you need to operate in the kernel. But to do that, the reliability of your machine is put at risk.""
The industry is responding with fundamental shifts:
Microsoft's WISP
mandates multi-year changes for every Windows security vendor
Linux embraced eBPF
for safer kernel instrumentation
Apple's Endpoint Security Framework
enables user-mode operation
AmberWolfresearchers spent seven months analyzingZTNAproducts.Zscalerfailed to validateSAMLassertions (CVE-2024-54982).Netskope'sauthentication could be bypassed using non-revocable OrgKey values.Check Pointhad hard-codedSFTPkeys (CVE-2025-3831).
These vulnerabilities existed for months. Some vendors patched quietly without CVEs. As of August 2025, 16 months after disclosure, many organizations still used exploitable configurations.
When nation-state attackers exploited Ivanti Connect Secure in January 2024, it validated Ivanti’s decision to rapidly advance its kernel-level security strategy, compressing a three-year project into just 18 months. As Riemer explained, ""We had already completed phase one of the kernel-hardening project before the attack. That allowed us to quickly pivot and accelerate our roadmap.”
Key accomplishments included:
Migration to 64-bit Oracle Linux:
Ivanti replaced an outdated 32-bit CentOS OS with Oracle Linux 9, significantly reducing known vulnerabilities tied to legacy open-source components.
Custom SELinux enforcement:
Implementing strict SELinux policies initially broke a significant number of product features, requiring careful refactoring without compromising security parameters. The resulting solution now runs in permanent enforcement mode, Riemer explained.
Process de-privileging and secure boot with TPM:
Ivanti eliminated root privileges from critical processes and integrated TPM-based secure boot and RSA encryption, ensuring continuous integrity checks, aligning with AmberWolf’s research recommendations and findings.
There were also a series of independent penetration testing initiatives, and each confirmed zero successful compromises, with threat actors typically abandoning attempts within three days.
Riemer explained to VentureBeat that global intelligence community customers actively watched threat actors probe the hardened systems. ""They tried oldTTPs, pivoted to web server exploits. They pretty much gave up after about three days,"" Riemer said.
The decision to go kernel-level wasn't a panic response. ""We actually had plans in place in 2023 to address this before we ever got attacked,"" Riemer said. The conversation that sealed the decision happened in Washington, DC. ""I sat down with the CIO of a federal agency, and I asked him flat out: Is there going to be a need for the U.S. government to have an L3 VPN solution on-prem in the future?"" Riemer recalled. ""His response was that there would always be a mission need for an L3 VPN on-prem type solution in order to give encrypted communication access to the warfighter.""
Gartner'sEmerging Tech Impact Radar: Cloud Security reportrateseBPFas having ""high"" mass with 1-3 years to early majority adoption. ""The use of eBPF allows for enhanced visibility and security without relying solely on kernel-level agents,"" Gartner notes.
The majority of cybersecurity security vendors are investing heavily in eBPF. ""Today, almost our entire customer base runsFalcon sensoron top ofeBPF,"" Ionescu said during his keynote at this year’s Fal.Con. ""We've been part of that journey aseBPF foundationmembers.""
Palo Alto Networkshas also emerged as a major player in eBPF-based security, investing heavily in the technology for theirCortex XDRandPrisma Cloudplatforms. This architectural shift allowsPalo Alto Networksto provide deep visibility into system calls, network traffic, and process execution while maintaining system reliability.
The convergence ofCrowdStrike,Palo Alto Networks, and other major vendors on eBPF technology signals a fundamental transformation—providing the visibility security teams need without catastrophic failure risks.
Patching is often relegated to one of those tasks that gets procrastinated about because so many security teams are short-handed, facing chronic time shortages. Those are the conditions that adversaries bank on when they choose victims.
It’s a sure bet that if a company is not prioritizing cybersecurity, they will be months or even years back on their patching. That’s what adversaries look for. Patterns emerge from different industries of victims and they share a common trait of procrastinating about system maintenance in general and security patterns specifically.
Based on interviewing victims of breaches that started with patches sometimes years old, VentureBeat has seen the following immediate steps they take to reduce the probabilo9ty of being hit again:
Automate patching immediately.Monthly cycles are obsolete. Tony Miller,Ivanti'sVP of enterprise services, confirmedring deployment eliminates the reactive patching chaosthat leaves organizations vulnerable during the critical 72-hour window.
Audit kernel-level security.Ask vendors abouteBPF/ESF/WISPmigration plans and timelines.
Layer defenses.This is table stakes for any cybersecurity strategy but critical to get right. ""Whether it wasSELinuxprofiling, root privilege avoidance, an updated web server, or theWAF—each layer stopped attacks,"" Riemer said.
Demand transparency.""Another vendor had been attacked in November 2023. That information didn't come available until August 2024,"" Riemer revealed. ""This is whyIvantihas been so public about transparency.""
Kernel-level transformation isn't optional. It's survival when AI weaponizes vulnerabilities in three days.
Ivanti Connect Secure 25.X represents what's possible when a vendor commits fully to kernel-level security, not as a reactive measure, but as a fundamental architectural principle.Gartner'sstrategic planning assumption is sobering: ""By 2030, at least 80% of enterpriseWindowsendpoints will still rely on hybrid endpoint protection agents, increasing the attack surface and requiring rigorous validation.""
Organizations must harden what they can now, automate immediately, and prepare for architectural upheaval. AsGartneremphasizes,combining ring deployment with integrated compensating controlsincludingendpoint protection platforms,multifactor authentication, andnetwork segmentationas part of a broaderzero-trust frameworkensures security teams can shrink exposure windows."
Startup & Tech Media,MCP stacks have a 92% exploit probability: How 10 plugins became enterprise security's biggest blind spot.,https://venturebeat.com/security/mcp-stacks-have-a-92-exploit-probability-how-10-plugins-became-enterprise,"The same connectivity that madeAnthropic's Model Context Protocol (MCP)the fastest-adopted AI integration standard in 2025 has created enterprise cybersecurity's most dangerous blind spot.
Recent research from Pyntquantifies the growing threat in clear, unambiguous terms. Their analysis exposes the startling network effect of vulnerabilities that escalate the more MCP plugins are used. Deploying just ten MCP plugins creates a92% probability of exploitation. At three interconnected servers, riskexceeds 50%. Even a single MCP plugin presents a 9% exploit probability, and the threat compounds exponentially with each addition.
The design premise for MCP began with a commendable goal of solving AI's integration chaos. Anthropic chose to standardize how large language models (LLMs) connect to external tools and data sources, delivering what every organization working with AI models and resources desperately needed: a universal interface for AI agents to access everything from APIs, cloud services, databases, and more.
Anthropic's launch was so well orchestratedthat MCP immediately gained traction with many of the leading AI companies in the industry, including Google and Microsoft, who both quickly adopted the standard. Now, a short ten months after the launch, there areover 16,000 MCP servers deployedacross Fortune 500 companies this year alone.
At the core of MCP's security paradox is its greatest strength, which isfrictionless connectivityandpervasive integrationwith as little friction as possible. That aspect of the protocol is itsgreatest weakness.Securitywasn't built into the protocol's core design.Authenticationremains optional.Authorization frameworksarrived just six months ago in updates, months after the protocol had seen widespread deployments. Combined, these two factors are fueling a quicklysprawling attack surfacewhere every new connection multiplies risk, creating anetwork effectof vulnerabilities.
""MCP is shipping with the same mistake we've seen in every major protocol rollout: insecure defaults,"" warns Merritt Baer, Chief Security Officer atEnkrypt AIand advisor to companies including Andesite and AppOmni told VentureBeat in a recent interview. ""If we don't build authentication and least privilege in from day one, we'll be cleaning up breaches for the next decade.""
Source:Pynt, Quantifying Risk Exposure Across 281 MCPs Report
Pynt's analysisof 281 MCP servers provides the data needed to illustrate the mathematical principles that are core to compositional risk.
According to their analysis, 72% of MCPs expose sensitive capabilities that include dynamic code execution, file system access, and privileged API calls, while 13% accept untrusted inputs like web scraping, Slack messages, email, or RSS feeds. When these two risk factors intersect, as they do in 9% of real-world MCP setups, attackers gain direct pathways to prompt injections, command execution, and data exfiltration, often without a single human approval required. These aren't hypothetical vulnerabilities; they're live, measurable exploit paths hidden within everyday MCP configurations.
""When you plug into an MCP server, you're not just trusting your own security, you're inheriting the hygiene of every tool, every credential, every developer in that chain,"" Baer warns. ""That's a supply chain risk in real time.""
Source:Pynt, Quantifying Risk Exposure Across 281 MCPs Report
Security research teams from many of the industry's leading companies continue their work to identify real-world exploits that MCP is currently seeing in the wild, in addition to those that are theoretical in nature. The MCP protocol continues to show increased vulnerabilities in different scenarios, with the main ones including the following:
CVE-2025-6514(CVSS 9.6):The MCP-remote package, downloaded over 500,000 times, carries a critical vulnerability allowing arbitrary OS command execution. ""The vulnerability allows attackers to trigger arbitrary OS command execution on the machine running MCP-remote when it initiates a connection to an untrusted MCP server, launching a full system compromise,"" warnsJFrog'ssecurity team.
The Postmark MCP Backdoor:Koi Securityuncovered that thepostmark-mcp npm packagehad been trojanized to grant attackers implicit ""god-mode"" access within AI workflows. In version 1.0.16, the malicious actor inserted a single line of code that silently BCC'd every outbound email to their domain (e.g., phan@giftshop.club), effectively exfiltrating internal memos, invoices, and password resets, all without raising alerts. AsKoi researchersput it: ""These MCP servers run with the same privileges as the AI assistants themselves — full email access, database connections, API permissions — yet they don't appear in any asset inventory, skip vendor risk assessments, and bypass every security control from DLP to email gateways.""
Idan Dardikman, co-founder and CTO at Koi Security,writes in a recent blog postexposing just how lethal the postmark-mcp npm package is, ""Let me be really clear about something: MCP servers aren't like regular npm packages. These are tools specifically designed for AI assistants to use autonomously.""
""If you're using postmark-mcp version 1.0.16 or later, you're compromised. Remove it immediately and rotate any credentials that may have been exposed through email. But more importantly, audit every MCP server you're using. Ask yourself: Do you actually know who built these tools you're trusting with everything? "" Dardikman writes. He ends thepostwith solid advice: ""Stay paranoid. With MCPs, paranoia is just good sense.""
CVE-2025-49596:Oligo Securityexposed a critical RCE vulnerability in Anthropic's MCP Inspector, enabling browser-based attacks. ""With code execution on a developer's machine, attackers can steal data, install backdoors, and move laterally across networks,"" explains Avi Lumelsky, security researcher
Trail of Bits'""Line Jumping"" Attack:Researchers demonstrated how maliciousMCP serversinject prompts throughtool descriptionsto manipulate AI behavior without ever being explicitly invoked. ""This vulnerability exploits the faulty assumption that humans provide a reliable defense layer,""the team notes.
Additional vulnerabilities includeprompt injection attackshijacking AI behavior,tool poisoning, manipulating server metadata,authentication weaknesseswhere tokens pass through untrusted proxies, andsupply chain attacks through compromised npm packages.
Authentication and authorization were initially optional in MCP. The protocol prioritized interoperability over security, assuming enterprises would add their own controls. They haven't.OAuth 2.0authorization finally arrived in March 2025, refined toOAuth 2.1by June. But thousands of MCP servers deployed without authentication remain in production.
Academic research fromQueen's Universityanalyzed 1,899 open-source MCP servers and found 7.2% contain general vulnerabilities and 5.5% exhibit MCP-specific tool poisoning.Gartner's survey (via IBM's Human–Machine Identity Blur paper)reveals organizations deploy 45 cybersecurity tools but effectively manage only 44% of machine identities, meaning half the identities in enterprise ecosystems could be invisible and unmanaged.
Defining a multilayer MCP defense strategy helps to close the gaps left in the original protocol's structure. The layers defined here look to bring together architectural safeguards and immediate operational measures to reduce an organization's threat surface.
Improving authentication and access controls needs to start with enforcingOAuth 2.1for each MCP gateway across an organization.Gartnernotes that enterprises enforcing these measures report 48% fewer vulnerabilities, 30% better user adoption, and centralized MCP server monitoring. ""MCP gateways serve as essential security intermediaries,""writesthe research firm, by providing unified server catalogs and real-time monitoring.
Semantic layers are essential for bringing greater context to each access decision, ensuring AI agents work only with standardized, trusted, and verifiable data. Deploying semantic layers helps reduce operational overhead, improves natural language query accuracy, and delivers the real-time traceability security leaders need. VentureBeat is seeing the practice of embedding security policies directly into data access contribute to reduced breach risks and more secure agentic analytics workflows.
By definition, knowledge graphs connect entities, analytics assets, and business processes, enabling AI agents to operate transparently and securely within an organizational context. Gartner highlights this capability as critical for regulatory compliance, auditability, and trust, especially in complex queries and workflows. Merritt Baer underscores the urgency: ""If you're using MCP today, you already need security. Guardrails, monitoring, and audit logs aren't optional — they're the difference between innovation with and without risk mitigation,"" advises Baer.
VentureBeat recommends security leaders who have MCP-based integrations active in their organizations take the following five precautionary actions to secure their infrastructure:
Make it a practice of implementing MCP Gateways by first enforcingOAuth 2.1andOpenID Connectwhile centralizing MCP server registration.
Define how your infrastructure can support a layered security architecture with semantic layers and knowledge graphs alongside gateways.
Turn the activity of conducting regular MCP audits through threat modeling, continuous monitoring, and red-teaming into the muscle memory of your security teams, so it's done by reflex.
Limit MCP plugin usage to essential plugins only—remember:3 plugins = 52% risk, 10 plugins = 92% risk.
Invest in AI-specific security as a distinct risk category within your cybersecurity strategy.
"
Startup & Tech Media,"Stopping breaches at machine speed demands agents, not alerts.",https://venturebeat.com/security/stopping-breaches-at-machine-speed-demands-agents-not-alerts,"Presented by DXC Technology
The sheer volume and sophistication of incoming threats today has dwarfed attacks from just six months ago, let alone two years ago, because adversaries have leveled up with AI.
Naturally, security operations and analysts are under pressure, facing mounting alert volumes and false positives, while organizations scramble to support them amidst a widening talent gap and an old model that doesn't stand up, says Chris Drumgoole, president, global infrastructure services at DXC Technology.
""The traditional, linear SOC [Security Operations Center] method was built very much like the rest of information technology service management — ticket, investigate threat — but the math just doesn’t add up given the volume,"" Drumgoole says. ""You would need a SOC bigger than your customer call center just to deal with all the incoming tickets. And that pure volume question is coupled with the increasing sophistication of tools and attacks. When you put those things in a blender, you end up with an old model that doesn’t work anymore.""
To combat alert fatigue and slow investigation cycles, organizations are fighting fire with fire: agentic security, or intelligent AI agents, that are capable of independently triaging, investigating, and responding to incidents at scale. DXC has partnered with 7AI to launch DXC Agentic Security Operations Center (SOC) integrating fully autonomous AI agents into its end-to-end managed security operations.
But before rolling this out globally to customers, DXC put the technology to the test, Drumgoole adds, using 7AI's agentic platform to optimize its own internal SOC capabilities. They immediately saw an 80% reduction in tier-1 SOC analyst time and a 95% reduction in the number of tickets that humans must analyze, which means a 67% reduction in mean time to respond in the tier-1 and tier-2 SOC.
This isn't just upgraded automation, but a major shift in threat response, analogous to the earlier shift from static defense to dynamic response. Agentic security isn't rule-based — it's adaptive, contextual, and end-to-end. And though humans will stay in the loop over the long term, agentic AI has the potential to move from reactive triage to proactive, self-directed defense.
""The real difference is that the AI model gives every alert the side eye, so to speak,"" Drumgoole says. ""While automation responds to the same alert the same way every time, the AI agent approaches each situation uniquely, recognizes the nuances and can learn from what it saw the last time and the time before. What we expect from our new Agentic SOC is going to be evolutionarily different just in terms of the amount they handle and how fast they handle it going forward.""
DXC Agentic SOC eliminates the traditional bottlenecks of manual alert processing, expecting to save customers 30 minutes to 2.5 hours per investigation by reducing false positive rates that can consume analyst resources. Average response time has gone from about 74 minutes to 24 minutes, a 70% improvement over the average human capability.
""The data speaks for itself. The math is the math,"" Drumgoole says. ""In the first 40 days of running our own Agentic SOC, we saved 165 human days of analyst work time. It’s only going up from there.
Even though the math speaks for itself in terms of accuracy, many organizations are still nervous about AI in general, and in particular, relying on it to transform their processes. Most workflows are built around humans, and bringing AI into the mix means disrupting that workflow, which adds time and material costs, and even takes an emotional toll.
""That’s a big adjustment for people,"" Drumgoole says. ""It’s really not a technical barrier, but an emotional, operational, and process barrier, underlined by 'this is the way we’ve always done it' thinking.""
But according to Drumgoole, it’s an evolve-or-die moment, and organizations need to push through the uncertainty. The mandate has to come from the top level, with executive-level sponsorship and clear mission. The beauty of the solution is that it's straightforward to implement and easy to scale, because it doesn't need to be considered a technology transformation, necessarily. Instead, it should be treated as equivalent to adding a stable of tier-1 security analysts. It doesn't require data storage or access to personal information, just needs to have the same security and access controls that an analyst would.
""Those who embrace it will grow their business,"" he explains. ""The more you can invest in training your agents, in building and deploying them, the better they’re going to get. I think if you don’t, you’re going to find yourself a dinosaur real fast.""
Sponsored articles are content produced by a company that is either paying for the post or has a business relationship with VentureBeat, and they’re always clearly marked. For more information, contactsales@venturebeat.com."
Startup & Tech Media,Doron Therapeutics Announces First Patients Dosed in Phase 3 Study of MOTYS (PTP-001) for the Treatment of Osteoarthritis of the Knee.,https://venturebeat.com/business/doron-therapeutics-announces-first-patients-dosed-in-phase-3-study-of-motys-ptp-001-for-the-treatment-of-osteoarthritis-of-the-knee,"
— Global Trial to Assess Injectable Biologic Therapy for Pain Relief and Functional Improvement in Patients with Knee Osteoarthritis —

CHAPEL HILL, N.C.--(BUSINESS WIRE)--October 16, 2025--
Doron Therapeutics, a clinical-stage biotechnology company developing novel biologics for musculoskeletal diseases, today announced that the initial patients have been dosed in its Phase 3 clinical trial ofMOTYS(PTP-001), the company’s lead biologic candidate for the treatment of knee osteoarthritis. MOTYS has been granted FastTrack and Regenerative Medicine Advanced Therapy (RMAT) designations by the U.S. Food and Drug Administration (FDA). The Phase 3 study of this investigational therapy follows the successful completion of an End-of-Phase 2 meeting held with the FDA in May.
“The initiation of our Phase 3 study represents a critical inflection point for Doron Therapeutics,” said Alessandra Pavesio, Chief Executive Officer of Doron Therapeutics. “We are advancing a highly differentiated therapy with the potential to address one of the largest areas of unmet need in medicine. We believe MOTYS, our innovative drug designed to deliver prolonged relief from pain and restored mobility in osteoarthritis patients, is well-positioned to create meaningful and durable clinical benefit for this underserved and growing population.”
Professor David Hunter, Chair of Rheumatology at the University of Sydney and Royal North Shore Hospital, Australia, and Principal Investigator in the Phase 3 trial added, ""Osteoarthritis remains the leading cause of pain and disability worldwide, yet treatment options for people living with this disease are woefully limited. This trial is an important step toward a biologic therapy that could transform care and bring real hope to millions of patients.""
The Phase 3 study is a randomized, placebo-controlled trial enrolling approximately 300 patients across multiple sites and countries, with the goal of evaluating improvements in both pain and function for up to 12 months following a single MOTYS injection.
About MOTYS
MOTYS is Doron Therapeutics’ investigational biologic therapy designed as an off-the-shelf, intra-articular knee injection to address the complex underlying biology of osteoarthritis. MOTYS' rigorously controlled and consistently potent secretome, growth factor, and extracellular matrix components are derived from donated placental tissues, following healthy at-term births. The therapy is designed to display a multifactorial mechanism of action that promotes a beneficial combination of anti-inflammatory, anti-catabolic and pro-anabolic effects. By targeting key pathways involved in inflammation and tissue damage, MOTYS has the potential to provide both immediate and long-lasting relief to patients from a single injection. MOTYS is currently in Phase 3 clinical development and has not been approved for sale by any regulatory authority.
About Doron Therapeutics
Doron Therapeutics is a clinical-stage biotechnology company headquartered in Chapel Hill, North Carolina, dedicated to developing innovative biologic therapies for musculoskeletal diseases. To learn more, please visit:https://dorontherapeutics.com/.

View source version on businesswire.com:https://www.businesswire.com/news/home/20251016185916/en/
Contact Information:Sarah CaleyHead of Business Developmentinfo@dorontherapeutics.com"
Startup & Tech Media,ThetaRho.ai Joins athenahealth's Marketplace Program to Deliver Instant AI-Powered Clinical Data Access.,https://venturebeat.com/business/thetarhoai-joins-athenahealths-marketplace-program-to-deliver-instant-ai-powered-clinical-data-access,"
Revolutionary AI assistant RISA eliminates the pain of hunting for patient data across multiple systems, enabling clinicians to focus more on patient care

SAN FRANCISCO--(BUSINESS WIRE)--October 16, 2025--
ThetaRho.ai, a provider of AI-powered clinical data intelligence solutions, today announced an agreement withathenahealth, Inc. through the award-winningathenahealth® Marketplaceprogram. Now available to athenahealth's growing network of healthcare providers, the newly integrated application, RISA, enables instant access to comprehensive patient information across multiple healthcare systems through natural language queries. The athenahealth Marketplace empowers athenahealth customers to easily discover and implement solutions that meet their unique needs, fostering a more efficient and patient-centered approach to care.
RISA transforms clinical workflows by allowing healthcare providers to simply ask questions such as, “Show me the discharge summary from the latest hospital visit,” or “Is this patient on a beta blocker?” and receive immediate, complete responses. The AI assistant seamlessly integrates with CommonWell Health Alliance, TEFCA, and Carequality networks to provide comprehensive patient data visibility beyond the primary EHR.
Key benefits for athenahealth customers include:
Early adopters have reported significant improvements in clinical efficiency:
“RISA is designed to give time back to physicians by making patient information instantly usable, not hidden behind tabs and PDFs,”said Kannan Govindarajan CEO, Co-Founder ofThetaRho.ai.“Through our Marketplace partnership with athenahealth, we’re excited to empower thousands of providers on the athenaOne network with a tool that reduces burnout and helps them focus on what matters most—the patient.”
As a new Marketplace partner, ThetaRho.ai joins an expansive, open ecosystem that enables integration with third-party applications, services, and systems to make innovative solutions available to more than 160,000 providers on the athenaOne network. The Marketplace allows athenaOne customers the ability to access and efficiently integrate with solutions that augment the functionality of the athenaOne platform, helping providers and administrators eliminate friction for patients while working to improve practice outcomes and financial performance.
To learn more aboutThetaRho.ai’s new integrated application on the Marketplace, please visit theproduct listing page.
About ThetaRho.ai
ThetaRho.ai develops AI-powered solutions that eliminate healthcare data silos and enhance clinical decision-making. The company’s product, RISA, serves healthcare organizations nationwide, enabling instant access to comprehensive patient information through natural language queries. With proven results including 90-minute daily time savings and 20% increases in patient capacity, ThetaRho.ai is committed to making healthcare data more accessible, actionable, and valuable for both clinicians and patients. Learn more aboutThetaRho.ai.
About the athenahealth Marketplace
The athenahealth Marketplace, one of the largest healthcare app stores, offers customers a diverse range of integrated solutions and innovative applications designed to enhance the athenaOne experiences. Nearly 75% of athenahealth customers use Marketplace partner solutions to boost practice efficiency and engage patients in their own care. The Marketplace has over 500 solutions across more than 60 medical specialties and capabilities that are integrated with athenaOne, athenahealth's all-in-one solution that improves clinical effectiveness, patient experience, and financial performance. Learn more about our comprehensiveMarketplace program.

View source version on businesswire.com:https://www.businesswire.com/news/home/20251007087671/en/
Ramani NarayanNara@thetarho.ai408.386.0844"
Startup & Tech Media,RAD Intel Appoints Aaron Vandeford as Director of Investor Relations.,https://venturebeat.com/business/rad-intel-appoints-aaron-vandeford-as-director-of-investor-relations,"LOS ANGELES--(BUSINESS WIRE)--October 15, 2025--
RAD Intelannounced today that Aaron Vandeford has joined as Director of Investor Relations. In this role, Vandeford will lead and investor relations strategy and execution, strengthening engagement across the company’s growing investor community and helping position the company for its next stage of growth.
Vandeford brings nearly 20 years of experience in investor relations, strategic finance, and capital markets. He has advised and led investor communications for public, private, and PE-backed companies through IPOs, SPACs, and M&A transactions totaling more than $20 billion in value. His background spans energy, technology, and consumer-facing sectors, with deep experience building and scaling best-in-class investor relations programs and partnering with executive teams to design capital allocation strategies, strengthen investor trust, and craft compelling financial narratives.
“Since day one, we’ve focused on a direct and open dialogue with our entire investor base as an important driver of success. As we scale the business, Aaron’s capital-markets experience will allow us to keep that personal access whilealsoraising the bar on metrics, cadence, and investor value overall,” said Jeremy Barnett, Co-Founder and CEO of RAD Intel.
“I’m thrilled to join RAD Intel at such an exciting stage in its journey,” said Vandeford. “The company’s commitment to innovation and impact, combined with its unique approach to democratizing investment access, is incredibly compelling. I look forward to building deep relationships with our investors and helping to amplify RAD Intel’s story as we grow.”
Aaron received his MBA in Finance from the University of Denver and B.A. in Economics and Spanish from Willamette University.
About RAD Intel
RAD Intel is a holding company with one shared AI decision layer that accelerates go-to-market for high-traction brands. Portfolio companies plug into one unified platform that ingests live market data, builds high-intent audiences, and directs creative and media toward measurable outcomes — shortening time to market and compounding growth across the portfolio. Learn more atwww.radintel.ai.

View source version on businesswire.com:https://www.businesswire.com/news/home/20251015797274/en/
Maria Brown, RAD Intelmariabrown@Radintel.ai"
Startup & Tech Media,CIBC Innovation Banking Provides $1.5 Million in Growth Capital to Vessel.,https://venturebeat.com/business/cibc-innovation-banking-provides-15-million-in-growth-capital-to-vessel,"MONTREAL--(BUSINESS WIRE)--October 15, 2025--
CIBC Innovation Banking announced today that it has provided a $1.5 million debt facility in growth capital to Vessel Funds Inc. (Vessel), a leading platform that simplifies investor relations, fund reporting and compliance tasks. The financing will support Vessel’s continued global expansion and product innovation.
Vessel’s platform enables fund managers and limited partners with real-time insights, streamlined workflows, and enhanced transparency, helping drive operational efficiency and improved decision-making across the investment lifecycle.
“We are excited to support Vessel as it continues to innovate in the fund management space,” said Eric Laflamme, Managing Director, CIBC Innovation Banking. “Vessel’s technology is transforming how private market funds and their LPs operate, and we look forward to supporting its continued growth.”
“This relationship with CIBC Innovation Banking is a significant milestone for Vessel,” said Thomas Terrats, CEO and Co-Founder, Vessel. “The growth capital will enable us to accelerate our product development and expand our reach, serving more funds globally while helping more General Partners and Limited Partners unlock greater value from their investments.”
Vessel is supported by leading investors and tech-forward venture capital firms including Inovia, Afore Capital, BY Venture Partners, FJ Labs and Golden Ventures.
About CIBC Innovation Banking
CIBC Innovation Banking has 25 years of specialized experience in growth-stage tech and life science companies across North America – a longer track record than most banks. CIBC Innovation Banking now has over $11 billion in funds managed including life sciences, health care, cleantech companies, investors, and entrepreneurs, and has assisted over 700 venture and private equity-backed businesses over the past six and a half years. The bank operates out of 14 global locations in San Francisco, Menlo Park, New York, Toronto, London, Austin, Boston, Chicago, Seattle, Vancouver, Montreal, Atlanta, Reston, and Durham. Connect with us today to start the conversation.www.innovationbanking.cibc.com
About Vessel
Vessel is a leading platform designed for funds and their limited partners, providing advanced tools to manage, track, and optimize fund operations. By leveraging real-time data and automation, Vessel enables fund managers and limited partners to make smarter decisions, increase transparency, and improve operational efficiency. For more information, visithttps://vessel.co

View source version on businesswire.com:https://www.businesswire.com/news/home/20251003080432/en/
Katarina Milicevickatarina.milicevic@cibc.com416-362-3458"
Startup & Tech Media,"Dfinity launches Caffeine, an AI platform that builds production apps from natural language prompts.",https://venturebeat.com/ai/dfinity-launches-caffeine-an-ai-platform-that-builds-production-apps-from,"TheDfinity Foundationon Wednesday releasedCaffeine, an artificial intelligence platform that allows users to build and deploy web applications through natural language conversation alone, bypassing traditional coding entirely. The system, which became publicly available today, represents a fundamental departure from existing AI coding assistants by building applications on a specialized decentralized infrastructure designed specifically for autonomous AI development.
UnlikeGitHub Copilot,Cursor, or other ""vibe coding"" tools that help human developers write code faster, Caffeine positions itself as a complete replacement for technical teams. Users describe what they want in plain language, and an ensemble of AI models writes, deploys, and continually updates production-grade applications — with no human intervention in the codebase itself.
""In the future, you as a prospective app owner or service owner… will talk to AI. AI will give you what you want on a URL,"" said Dominic Williams, founder and chief scientist at the Dfinity Foundation, in an exclusive interview with VentureBeat. ""You will use that, completely interact productively, and you'll just keep talking to AI to evolve what that does. The AI, or an ensemble of AIs, will be your tech team.""
The platform has attracted significant early interest: more than 15,000 alpha users tested Caffeine before its public release, with daily active users representing 26% of those who received access codes — ""early Facebook kind of levels,"" according to Williams. The foundation reports some users spending entire days building applications on the platform, forcing Dfinity to consider usage limits due to underlying AI infrastructure costs.
Caffeine's most significant technical claim addresses a problem that has plagued AI-generated code: data loss during application updates. The platform builds applications usingMotoko, a programming language developed by Dfinity specifically for AI use, which provides mathematical guarantees that upgrades cannot accidentally delete user data.
""When AI is updating apps and services in production, a mistake cannot lose data. That's a guarantee,"" Williams said. ""It's not like there are some safeguards to try and stop it losing data. This language framework gives it rails that guarantee if an upgrade, an update to its app's underlying logic, would cause data loss, the upgrade fails and the AI just tries again.""
This addresses what Williams characterizes as critical failures in competing platforms. User forums for tools likeLovableandReplit, he notes, frequently report three major problems: applications that become irreparably broken as complexity increases, security vulnerabilities that allow unauthorized access, and mysterious data loss during updates.
Traditional tech stacks evolved to meet human developer needs — familiarity with SQL databases, preference for known programming languages, existing skill investments. ""That's how the traditional tech stacks evolved. It's really evolved to meet human needs,"" Williams explained. ""But in the future, it's going to be different. You're not going to care how the AI did it. Instead, for you, AI is the tech stack.""
Caffeine's architecture reflects this philosophy. Applications run entirely on theInternet Computer Protocol (ICP), a blockchain-based network that Dfinity launched in May 2021 after raising over $100 million from investors includingAndreessen HorowitzandPolychain Capital. The ICP uses what Dfinity calls ""chain-key cryptography"" to create what Williams describes as ""tamper-proof"" code — applications that are mathematically guaranteed to execute their written logic without interference from traditional cyberattacks.
""The code can't be affected by ransomware, so you don't have to worry about malware in the same way you do,"" Williams said. ""Configuration errors don't result in traditional cyber attacks. That passive traditional cyber attacks isn't something you need to worry about.""
At the heart of Caffeine's technical approach is a concept called ""orthogonal persistence,"" which fundamentally reimagines how applications store and manage data. In traditional development, programmers must write extensive code to move data between application logic and separate database systems — marshaling data in and out of SQL servers, managing connections, handling synchronization.
Motokoeliminates this entirely. Williams demonstrated with a simple example: defining a blog post data type and declaring a variable to store an array of posts requires just two lines of code. ""This declaration is all that's necessary to have the blog maintain its list of posts,"" he explained during a presentation on the technology. ""Compare that to traditional IT where in order to persist the blog posts, you'd have to marshal them in and out of a database server. This is quite literally orders of magnitude more simple.""
This abstraction allows AI to work at a higher conceptual level, focusing on application logic rather than infrastructure plumbing. ""Logic and data are kind of the same,"" Williams said. ""This is one of the things that enables AI to build far more complicated functionality than it could otherwise do.""
The system also employs what Dfinity calls ""loss-safe data migration."" When AI needs to modify an application's data structure — adding a ""likes"" field to blog posts, for example — it must write migration logic in two passes. The framework automatically verifies that the transformation won't result in data loss, refusing to compile or deploy code that could delete information unless explicitly instructed.
Williams positionsCaffeineas particularly transformative for enterprise IT, where he claims costs could fall to ""1% of what they were before"" while time-to-market shrinks to similar fractions. The platform targets a spectrum from individual creators to large corporations, all of whom currently face either expensive development teams or constraining low-code templates.
""A corporation or government department might want to create a corporate portal or CRM, ERP functionality,"" Williams said, referring to customer relationship management and enterprise resource planning systems. ""They will otherwise have to obtain this by signing up for some incredibly expensive SaaS service where they become locked in, their data gets stuck, and they still have to spend a lot of money on consultants customizing the functionality.""
Applications built throughCaffeineare owned entirely by their creators and cannot be shut down by centralized parties — a consequence of running on the decentralizedInternet Computernetwork rather than traditional cloud providers likeAmazon Web Services. ""When someone says built on the internet computer, it actually means built on the internet computer,"" Williams emphasized, contrasting this with blockchain projects that merely host tokens while running actual applications on centralized infrastructure.
The platform demonstrated this versatility during a July 2025 hackathon in San Francisco, where participants created applications ranging from a ""Will Maker"" tool for generating legal documents, to ""Blue Lens,"" a voice-AI water quality monitoring system, to ""Road Patrol,"" a gamified community reporting app for infrastructure problems. Critically, many of these came from non-technical participants with no coding background.
""I'm from a non-technical background, I'm actually a quality assurance professional,"" said the creator of Blue Lens in a video testimonial. ""Through Caffeine I can build something really intuitive and next-gen to the public."" The application integrated multiple external services — Eleven Labs for voice AI, real-time government water data through retrieval-augmented generation, and Midjourney-generated visual assets — all coordinated through conversational prompts.
Caffeineenters a crowded market of AI-assisted development tools, but Williams argues the competition isn't truly comparable.GitHub Copilot,Cursor, and similar tools serve human developers working with traditional technology stacks. Platforms likeReplitandLovableoccupy a middle ground, offering ""vibe coding"" that mixes AI generation with human editing.
""If you're a Node.js developer, you know you're working with the traditional stack, and you might want to do your coding with Copilot or using Claude or using Cursor,"" Williams said. ""That's a very different thing to what Caffeine is offering. There'll always be cases where you probably wouldn't want to hand over the logic of the control system for a new nuclear missile silo to AI. But there's going to be these holdout areas, right? And there's all the legacy stuff that has to be maintained.""
The key distinction, according to Williams, lies in production readiness. Existing AI coding tools excel at rapid prototyping but stumble when applications grow complex or require guaranteed reliability. Reddit forums for these platforms document users hitting insurmountable walls where applications break irreparably, or where AI-generated code introduces security vulnerabilities.
""As the demands and the requirements become more complicated, eventually you can hit a limit, and when you hit that limit, not only can you not go any further, but sometimes your app will get broken and there's no way of going back to where you were before,"" Williams said. ""That can't happen with productive apps, and it also can't be the case that you're getting hacked and losing data, because once you go hands-free, if you like, and there's no tech team, there's no technical people involved, who's going to run the backups and restore your app?""
The Internet Computer's architecture addresses this through Byzantine fault tolerance — even if attackers gain physical control over some network hardware, they cannot corrupt applications or their data. ""This is the beginning of a compute revolution and it's also the perfect platform for AI to build on,"" Williams said.
Dfinity framesCaffeinewithin a broader vision it calls the ""self-writing internet,"" where the web literally programs itself through natural language interaction. This represents what Williams describes as a ""seismic shift coming to tech"" — from human developers selecting technology stacks based on their existing skills, to AI selecting optimal implementations invisible to users.
""You don't care about whether some human being has learned all of the different platforms and Amazon Web Services or something like that. You don't care about that. You just care: Is it secure? Do you get security guarantees? Is it resilient? What's the level of resilience?"" Williams said. ""Those are the new parameters.""
The platform demonstrated this during live demonstrations, including at theWorld Computer Summit 2025in Zurich. Williams created a talent recruitment application from scratch in under two minutes, then modified it in real-time while the application ran with users already interacting with it. ""You will continue talking to the AI and just keep on refreshing the URL to see the changes,"" he explained.
This capability extends to complex scenarios. During demonstrations, Williams showed building a tennis lesson booking system, an e-commerce platform, and an event registration system — all simultaneously, working on multiple applications in parallel. ""We predict that as people get very proficient withCaffeine, they could be working on even 10 apps in parallel,"" he said.
The system writes substantial code: a simple personal blog generated 700 lines of code in a couple of minutes. More complex applications can involve thousands of lines across frontend and backend components, all abstracted away from the user who only describes desired functionality.
Caffeine's economic model differs fundamentally from traditional software-as-a-service platforms. Applications run on theInternet Computer Protocol, which uses a ""reverse gas model"" where developers pay for computation rather than users paying transaction fees. The platform includes an integrated App Market where creators can publish applications for others to clone and adapt — creating what Dfinity envisions as a new economic ecosystem.
""App stores today obviously operate on gatekeeping,"" said Pierre Samaties, chief business officer at Dfinity, during the World Computer Summit. ""That's going to erode."" Rather than purchasing applications, users can clone them and modify them for their own purposes — fundamentally different from Apple'sApp StoreorGoogle Playmodels.
Williams acknowledges thatCaffeineitself currently runs on centralized infrastructure, despite building applications on the decentralized Internet Computer. ""Caffeine itself actually is centralized. It uses aspects of the Internet Computer. We want Caffeine itself to run on the Internet Computer in the future, but it's not there now,"" he said. The platform leverages commercially available foundation models from companies like Anthropic, whoseClaude Sonnetmodel powers much of Caffeine's backend logic.
This pragmatic approach reflects Dfinity's strategy of using best-in-class AI models while focusing its own development on the specialized infrastructure and programming language designed for AI use. ""These content models have been developed by companies with enormous budgets, absolutely enormous budgets,"" Williams said. ""I don't think in the near future we'll run AI on the Internet Computer for that reason, unless there's a special case.""
TheDfinity Foundationhas pursued this vision since Williams began researching decentralized networks in late 2013. After involvement with Ethereum before its 2015 launch, Williams became fascinated with the concept of a ""world computer""—a public blockchain network that could host not just tokens but entire applications and services.
""By 2015 I was talking about network-focused drivers, Dfinity back then, and that could really operate as an alternative tech stack, and eventually host even things like social networks and massive enterprise systems,"" Williams said. The foundation launched the Internet Computer Protocol in May 2021, initially focusing on Web3 developers. Despite not being among the highest-valued blockchain projects, ICP consistently ranks in the top 10 for developer numbers.
The pivot to AI-driven development came from recognizing that ""in the future, the tech stack will be AI,"" according to Williams. This realization led to Caffeine's development, announced on Dfinity's public roadmap in March 2025 and demonstrated at theWorld Computer Summitin June 2025.
One successful example of the Dfinity vision running in production isOpenChat, a messaging application that runs entirely on the Internet Computer and is governed by a decentralized autonomous organization (DAO) with tens of thousands of participants voting on source code updates through algorithmic governance. ""The community is actually controlling the source code updates,"" Williams explained. ""Developers propose updates, community reads the updates, and if the community is happy, OpenChat updates itself.""
The platform faces several challenges. Dfinity's crypto industry roots may create perception problems in enterprise markets, Williams acknowledges. ""The Web3 industry's reputation is a bit tarnished and probably rightfully so,"" he said during the World Computer Summit. ""Now people can, for themselves, experience what a decentralized network is. We're going to see self-writing take over the enterprise space because the speed and efficiency are just incredible.""
The foundation's history includes controversy: ICP's token launched in 2021 at over $100 per token with an all-time high around $700, thencrashed below $3 in 2023before recovering. The project has faced legal challenges, including class action lawsuitsalleging misleading investors, andDfinity filed defamation claimsagainst industry critics.
Technical limitations also remain. Caffeine cannot yet compile React front-ends on the Internet Computer itself, requiring some off-chain processing. Complex integrations with traditional systems — payment processing through Stripe, for example — still require centralized components. ""Your app is running end-to-end on the Internet Computer, then when it needs to actually accept payment, it's going to hand over to your Stripe account,"" Williams explained.
The platform's claims about data loss prevention and security guarantees, while technically grounded in the Motoko language design and Internet Computer architecture, remain to be tested at scale with diverse real-world applications. The 26% daily active user rate from alpha testing is impressive but comes from a self-selected group of early adopters.
Williams rejects concerns that AI-driven development will eliminate software engineering jobs, arguing instead for market expansion. ""The self-writing internet empowers eight billion non-technical people,"" he said. ""Some of these people will enter roles in tech, becoming prompt engineers, tech entrepreneurs, or helping run online communities. Humanity will create millions of new custom apps and services, and a subset of those will require professional human assistance.""
During hisWorld Computer Summit demonstration, Williams was explicit about the scale of transformation Dfinity envisions. ""Today there are about 35,000 Web3 engineers in the world. Worldwide there are about 15 million full-stack engineers,"" he said. ""But tomorrow with the self-writing internet, everyone will be a builder. Today there are already about five billion people with internet-connected smartphones and they'll all be able to use Caffeine.""
The hackathon results suggest this isn't pure hyperbole. A dentist built ""Dental Tracks"" to help patients manage their dental records. A transportation industry professional created ""Road Patrol"" for gamified infrastructure reporting. A frustrated knitting student built ""Skill Sprout,"" a garden-themed app for learning new hobbies, complete with material checklists and step-by-step skill breakdowns—all without writing a single line of code.
""I was learning to knit. I got irritated because I had the wrong materials,"" the creator explained in a video interview. ""I don't know how to do the stitches, so I have to individually search, and it's really intimidating when you're trying to learn something you don't—you don't even know what you don't know.""
WhetherCaffeinesucceeds depends on factors still unknown: how production applications perform under real-world stress, whether the Internet Computer scales to millions of applications, whether enterprises can overcome their skepticism of blockchain-adjacent technology. But if Williams is right about the fundamental shift — that AI will be the tech stack, not just a tool for human developers — then someone will build what Caffeine promises.
The question isn't whether the future looks like this. It's who gets there first, and whether they can do it without losing everyone's data along the way.
"
Startup & Tech Media,EAGLET boosts AI agent performance on longer-horizon tasks by generating custom plans.,https://venturebeat.com/ai/eaglet-boosts-ai-agent-performance-on-longer-horizon-tasks-by-generating,"2025 was supposed to bethe year of ""AI agents,""according to Nvidia CEO Jensen Huang, and other AI industry personnel. And it has been, in many ways, with numerous leading AI model providers such asOpenAI,Google, and even Chinese competitors likeAlibaba releasingfine-tuned AI models or applications designed to focus on a narrow set of tasks, such as web search and report writing.
But one big hurdle to a future of highly performant, reliable, AI agents remains: getting them to stay on task when the task extends over a number of steps.Third-party benchmark testsshow even the most powerful AI models experience higher failure rates the more steps they take to complete a task, and the longer time they spend on it (exceeding hours).
Anew academic framework called EAGLETproposes a practical and efficient method to improve long-horizon task performance in LLM-based agents — without the need for manual data labeling or retraining.
Developed by researchers from Tsinghua University, Peking University, DeepLang AI, and the University of Illinois Urbana-Champaign,EAGLET offers a ""global planner"" that can be integrated into existing agent workflows to reduce hallucinations and improve task efficiency.
EAGLET is a fine-tuned language model that interprets task instructions — typically provided as prompts by the user or the agent's operating environment — and generates a high-level plan for the agent (powered by its own LLM). It does not intervene during execution, but its up-front guidance helps reduce planning errors and improve task completion rates.
Many LLM-based agents struggle with long-horizon tasks because they rely on reactive, step-by-step reasoning. This approach often leads to trial-and-error behavior, planning hallucinations, and inefficient trajectories.
EAGLET tackles this limitation by introducing aglobal planning modulethat works alongside the executor agent.
Instead of blending planning and action generation in a single model, EAGLET separates them, enabling more coherent, task-level strategies.
EAGLET’s planner is trained using a two-stage process that requires no human-written plans or annotations.
The first stage involves generating synthetic plans with high-capability LLMs, such as GPT-5 and DeepSeek-V3.1-Think.
These plans are then filtered using a novel strategy called homologous consensus filtering, which retains only those that improve task performance for both expert and novice executor agents.
In the second stage, a rule-based reinforcement learning process further refines the planner, using a custom-designed reward function to assess how much each plan helps multiple agents succeed.
One of EAGLET’s key innovations is the Executor Capability Gain Reward (ECGR).
This reward measures the value of a generated plan by checking whether it helps both high- and low-capability agents complete tasks more successfully and with fewer steps.
It also includes a decay factor to favor shorter, more efficient task trajectories. This approach avoids over-rewarding plans that are only useful to already-competent agents and promotes more generalizable planning guidance.
The EAGLET planner is designed to be modular and ""plug-and-play,"" meaning it can be inserted into existing agent pipelines without requiring executor retraining.
In evaluations, the planner boosted performance across a variety of foundational models, including GPT-4.1, GPT-5, Llama-3.1, and Qwen2.5.
It also proved effective regardless of prompting strategy, working well with standard ReAct-style prompts as well as approaches like Reflexion.
EAGLET was tested on three widely used benchmarks for long-horizon agent tasks: ScienceWorld, which simulates scientific experiments in a text-based lab environment; ALFWorld, which tasks agents with completing household activities through natural language in a simulated home setting; and WebShop, which evaluates goal-driven behavior in a realistic online shopping interface.
Across all three, executor agents equipped with EAGLET outperformed their non-planning counterparts and other planning baselines, including MPO and KnowAgent.
In experiments with the open source Llama-3.1-8B-Instruct model, EAGLET boosted average performance from 39.5 to 59.4, a +19.9 point gain across tasks.
On ScienceWorld unseen scenarios, it raised performance from 42.2 to 61.6.
In ALFWorld seen scenarios, EAGLET improved outcomes from 22.9 to 54.3, a more than 2.3× increase in performance.
Even stronger gains were seen with more capable models.
For instance, GPT-4.1 improved from 75.5 to 82.2 average score with EAGLET, and GPT-5 rose from 84.5 to 88.1, despite already being strong performers.
In some benchmarks, performance gains were as high as +11.8 points, such as when combining EAGLET with the ETO executor method on ALFWorld unseen tasks.
Compared to other planning baselines like MPO, EAGLET consistently delivered higher task completion rates. For example, on ALFWorld unseen tasks with GPT-4.1, MPO achieved 79.1, while EAGLET scored 83.6—a +4.5 point advantage.
Additionally, the paper reports that agents using EAGLET complete tasks in fewer steps on average. With GPT-4.1 as executor, average step count dropped from 13.0 (no planner) to 11.1 (EAGLET). With GPT-5, it dropped from 11.4 to 9.4, supporting the claim of improved execution efficiency.
Compared to RL-based methods like GiGPO, which can require hundreds of training iterations, EAGLET achieved better or comparable results with roughly one-eighth the training effort.
This efficiency also carries over into execution: agents using EAGLET typically needed fewer steps to complete tasks. This translates into reduced inference time and compute cost in production scenarios.
As of the version submitted to arXiv, the authors have not released an open-source implementation of EAGLET. It is unclear if or when the code will be released, under what license, or how it will be maintained, which may limit the near-term utility of the framework for enterprise deployment.
VentureBeat has reached out to the authors to clarify these points and will update this piece when we hear back.
While the planner is described as plug-and-play, it remains unclear whether EAGLET can be easily integrated into popular enterprise agent frameworks such as LangChain or AutoGen, or if it requires a custom stack to support plan-execute separation.
Similarly, the training setup leverages multiple executor agents, which may be difficult to replicate in enterprise environments with limited model access. VentureBeat has asked the researchers whether the homologous consensus filtering method can be adapted for teams that only have access to one executor model or limited compute resources.
EAGLET’s authors report success across model types and sizes, but it is not yet known what the minimal viable model scale is for practical deployment. For example, can enterprise teams use the planner effectively with sub-10B parameter open models in latency-sensitive environments? Additionally, the framework may offer industry-specific value in domains like customer support or IT automation, but it remains to be seen how easily the planner can be fine-tuned or customized for such verticals.
Another open question is how EAGLET is best deployed in practice. Should the planner operate in real-time alongside executors within a loop, or is it better used offline to pre-generate global plans for known task types? Each approach has implications for latency, cost, and operational complexity. VentureBeat has posed this question to the authors and will report any insights that emerge.
For technical leaders at medium-to-large enterprises, EAGLET represents a compelling proof of concept for improving the reliability and efficiency of LLM agents. But without public tooling or implementation guidelines, the framework still presents a build-versus-wait decision. Enterprises must weigh the potential gains in task performance and efficiency against the costs of reproducing or approximating the training process in-house.
For enterprises developing agentic AI systems—especially in environments requiring stepwise planning, such as IT automation, customer support, or online interactions—EAGLET offers a template for how to incorporate planning without retraining. Its ability to guide both open- and closed-source models, along with its efficient training method, may make it an appealing starting point for teams seeking to improve agent performance with minimal overhead."
Startup & Tech Media,"How Rose Rock Bridge is building the future of energy in Tulsa, Oklahoma.",https://venturebeat.com/energy/how-rose-rock-bridge-is-building-the-future-of-energy-in-tulsa-oklahoma,"Presented by Tulsa Innovation Labs
Tulsa was once called ""the oil capital of the world,” and since its launch in 2022, Rose Rock Bridge (RRB), a Tulsa-based non-profit startup incubator led by Tulsa Innovation Labs, has been capitalizing on this heritage, aiming to source and support emerging technologies targeting the energy sector. To create a tech economy that becomes foundational to the future of the sustainable energy industry, and competes on the world stage, they're marrying the expertise and industry that already exists in Tulsa with promising entrepreneurial talent.
""Places like Tulsa, we’re tailor-made for tech excellence,"" says Jennifer Hankins, managing director, Tulsa Innovation Labs. ""Our legacy as an oil and gas leader means we know how to build things, and we know how to capture big industries, and we're positioned to be a leader in energy innovation.""
RRB, in partnership with major stakeholders, is helping put the region's strong corporate, academic, and workforce resources in the hands of innovative, early-stage startups developing the next-generation solutions that are solving pressing energy industry problems and opening up new markets.
""We're building the next generation of big energy companies that tackle global challenges in a way that's authentic to Tulsa's local expertise, and not one that feels more extractive to it,"" she adds. ""
RRB has already accelerated 33 companies, initiated 22 active pilots with industry partners, and secured 11 customer contracts, resulting in over $50 million in funding raised by its member companies.
RRB's Rose Rock Bridge Showcase is a showcase and pitch competition presented in partnership with four local energy industry partners: Williams, ONEOK, Devon Energy, and Helmerich and Payne. These partners identify white space problems they're aiming to solve — this year, low carbon natural gas solutions — and RRB finds the startups that can solve them.
From a competitive pool of more than 50 applications, fourteen companies are selected to pitch for pilot opportunities and potential investment from leading Oklahoma energy companies. While most pitch competitions are seen as pathways to venture capital, the RRB model is designed to accelerate commercialization; instead of vying for funding alone, these companies are competing for the chance to put their technology into practice, Hankins explains.
""What sets the winners apart is the way they're solving big challenges with game-changing ideas in the energy space,"" Hankins says. ""But above and beyond just a great idea, it has to be an idea that’s commercial. We can say that our companies have already demonstrated the technology. They’ve already validated it. They’ve secured a big customer, gained traction, are on the path to secure follow-on funding. Those are things that hold back most startups, and our program brings all of those three things together to accelerate commercialization.""
Each startup receives $100,000 in non-dilutive funding to grow their business in Tulsa, along with support services and pilot opportunities through industry partners, equipping them with both the resources and real-world experience needed for long-term market integration—and a solid foothold in Tulsa.
This year's cohort is comprised of companies that are driving innovation in low carbon natural gas through technologies that enhance operations, control and reduce emissions, and turn waste from energy production into valuable materials:
Developing artificial intelligence/machine learning-assisted Raman Spectroscopy for real-time chemical analysis, which helps energy providers process their product more efficiently.
Erdin Guma, Eigen Control
Increasing the reliability of equipment while reducing methane emissions with spring-loaded electric valve actuators
Dean Pick, Kinitics Automation
Dean Pick, Kinitics Automation
Converting wastewater and stranded gas into clean methanol
Brian Worfolk, Lukera Energy
Brian Worfolk, Lukera Energy
Making hazardous, high-risk environments safer with robotic inspection platforms.
Connor Crawford, Pike Robotics
Connor Crawford, Pike Robotics
""We talk a lot about stickiness,"" Hankins says. ""Tulsa Innovation Labs, in addition to the Rose Rock Bridge initiative, is really focused on creating that supportive ecosystem in the region.""
For example, ensuring these companies have lab space if necessary, connecting them to university partners to sharpen research and development, and helping them establish relationships and follow-on funding with other energy-related funds, and embedding them into the Tulsa energy tech landscape. The RRB entrepreneur in residence and executive in residence offer in-depth mentoring as well.
""I call it polishing the startups,"" Hankins explains. ""You go through our program, get a pilot, get insight from the corporate perspective. That’s probably the highest value. But along the way, all the support to help you operationalize your company and your idea faster. We’re going to find a way that you’ll leave our program more ready to get to market, whether that be through some of those auxiliary supports, or we’re going to make sure that direct connection to the customer happens.""
Sponsored articles are content produced by a company that is either paying for the post or has a business relationship with VentureBeat, and they’re always clearly marked. For more information, contactsales@venturebeat.com.
"
Startup & Tech Media,Self-improving language models are becoming reality with MIT's updated SEAL technique.,https://venturebeat.com/ai/self-improving-language-models-are-becoming-reality-with-mits-updated-seal,"Researchers at the Massachusetts Institute of Technology (MIT) are gaining renewed attention for developing andopen sourcinga technique that allows large language models (LLMs) — like those underpinning ChatGPT and most modern AI chatbots — to improve themselves by generating synthetic data to fine-tune upon.
The technique, known as SEAL (Self-Adapting LLMs), was first described in a paper published back in June andcovered by VentureBeat at the time.
A significantly expanded andupdated version of the paper was released last month, as well asopen source code posted on Github(under an MIT License, allowing for commercial and enterprise usage), and is making new waves among AI power users on the social network X this week.
SEAL allows LLMs to autonomously generate and apply their own fine-tuning strategies. Unlike conventional models that rely on fixed external data and human-crafted optimization pipelines, SEAL enables models to evolve by producing their own synthetic training data and corresponding optimization directives.
The development comes from a team affiliated with MIT’s Improbable AI Lab, including Adam Zweiger, Jyothish Pari, Han Guo, Ekin Akyürek, Yoon Kim, and Pulkit Agrawal. Their research was recently presented at the 39th Conference on Neural Information Processing Systems (NeurIPS 2025).
Earlier this year, VentureBeat first reported on SEAL as an early-stage framework that allowed language models to generate and train on their own synthetic data — a potential remedy for the stagnation of pretrained models once deployed.
At that stage, SEAL was framed as a proof-of-concept that could let enterprise AI agents continuously learn in dynamic environments without manual retraining.
Since then, the research has advanced considerably. The new version expands on the prior framework by demonstrating that SEAL’s self-adaptation ability scales with model size, integrates reinforcement learning more effectively to reduce catastrophic forgetting, and formalizes SEAL’s dual-loop structure (inner supervised fine-tuning and outer reinforcement optimization) for reproducibility.
The updated paper also introduces evaluations across different prompting formats, improved stability during learning cycles, and a discussion of practical deployment challenges at inference time.
While LLMs have demonstrated remarkable capabilities in text generation and understanding, their adaptation to new tasks or knowledge is often manual, brittle, or dependent on context.
SEAL challenges this status quo by equipping models with the ability to generate what the authors call “self-edits” — natural language outputs that specify how the model should update its weights.
These self-edits may take the form of reformulated information, logical implications, or tool configurations for augmentation and training. Once generated, the model fine-tunes itself based on these edits. The process is guided by reinforcement learning, where the reward signal comes from improved performance on a downstream task.
The design mimics how human learners might rephrase or reorganize study materials to better internalize information. This restructuring of knowledge before assimilation serves as a key advantage over models that passively consume new data “as-is.”
SEAL has been tested across two main domains: knowledge incorporation and few-shot learning.
In the knowledge incorporation setting, the researchers evaluated how well a model could internalize new factual content from passages similar to those in the SQuAD dataset, a benchmark reading comprehension dataset introduced by Stanford University in 2016, consisting of over 100,000 crowd-sourced question–answer pairs based on Wikipedia articles (Rajpurkar et al., 2016).
Rather than fine-tuning directly on passage text,the model generated synthetic implications of the passageand then fine-tuned on them.
After two rounds of reinforcement learning, the model improved question-answering accuracy from 33.5% to 47.0% on a no-context version of SQuAD — surpassing results obtained using synthetic data generated by GPT-4.1.
In the few-shot learning setting, SEAL was evaluated using a subset of the ARC benchmark, where tasks require reasoning from only a few examples. Here, SEAL generated self-edits specifying data augmentations and hyperparameters.
After reinforcement learning,the success rate in correctly solving held-out tasks jumped to 72.5%, up from 20% using self-edits generated without reinforcement learning.Models that relied solely on in-context learning without any adaptation scored 0%.
SEAL operates using a two-loop structure: an inner loop performs supervised fine-tuning based on the self-edit, while an outer loop uses reinforcement learning to refine the policy that generates those self-edits.
The reinforcement learning algorithm used is based on ReSTEM, which combines sampling with filtered behavior cloning. During training, only self-edits that lead to performance improvements are reinforced. This approach effectively teaches the model which kinds of edits are most beneficial for learning.
For efficiency, SEAL applies LoRA-based fine-tuning rather than full parameter updates, enabling rapid experimentation and low-cost adaptation.
The researchers report that SEAL can produce high-utility training data with minimal supervision, outperforming even large external models like GPT-4.1 in specific tasks.
They also demonstrate that SEAL generalizes beyond its original setup: it continues to perform well when scaling from single-pass updates to multi-document continued pretraining scenarios.
However, the framework is not without limitations. One issue is catastrophic forgetting, where updates to incorporate new information can degrade performance on previously learned tasks.
In response to this concern, co-author Jyo Pari told VentureBeat via email that reinforcement learning (RL) appears to mitigate forgetting more effectively than standard supervised fine-tuning (SFT), citing a recent paper on the topic. He added that combining this insight with SEAL could lead to new variants where SEAL learns not just training data, but reward functions.
Another challenge is computational overhead: evaluating each self-edit requires fine-tuning and performance testing, which can take 30–45 seconds per edit — significantly more than standard reinforcement learning tasks.
As Jyo explained, “Training SEAL is non-trivial because it requires 2 loops of optimization, an outer RL one and an inner SFT one. At inference time, updating model weights will also require new systems infrastructure.” He emphasized the need for future research into deployment systems as a critical path to making SEAL practical.
Additionally, SEAL’s current design assumes the presence of paired tasks and reference answers for every context, limiting its direct applicability to unlabeled corpora. However, Jyo clarified that as long as there is a downstream task with a computable reward, SEAL can be trained to adapt accordingly—even in safety-critical domains. In principle, a SEAL-trained model could learn to avoid training on harmful or malicious inputs if guided by the appropriate reward signal.
The AI research and builder community has reacted with a mix of excitement and speculation to the SEAL paper. On X, formerly Twitter, several prominent AI-focused accounts weighed in on the potential impact.
User@VraserX, a self-described educator and AI enthusiast, called SEAL “the birth of continuous self-learning AI” and predicted that models like OpenAI's GPT-6 could adopt similar architecture.
In their words, SEAL represents “the end of the frozen-weights era,” ushering in systems that evolve as the world around them changes.
They highlighted SEAL's ability to form persistent memories, repair knowledge, and learn from real-time data, comparing it to a foundational step toward models that don’t just use information but absorb it.
Meanwhile,@alex_prompter, co-founder of an AI-powered marketing venture, framed SEAL as a leap toward models that literally rewrite themselves. “MIT just built an AI that can rewrite its own code to get smarter,” he wrote.Citing the paper’s key results — a 40% boost in factual recall and outperforming GPT-4.1 using self-generated data— he described the findings as confirmation that “LLMs that finetune themselves are no longer sci-fi.”
The enthusiasm reflects a broader appetite in the AI space for models that can evolve without constant retraining or human oversight — particularly in rapidly changing domains or personalized use cases.
In response to questions about scaling SEAL to larger models and tasks, Jyo pointed to experiments (Appendix B.7) showing that as model size increases, so does their self-adaptation ability. He compared this to students improving their study techniques over time — larger models are simply better at generating useful self-edits.
When asked whether SEAL generalizes to new prompting styles, he confirmed it does, citing Table 10 in the paper. However, he also acknowledged that the team has not yet tested SEAL’s ability to transfer across entirely new domains or model architectures.
“SEAL is an initial work showcasing the possibilities,” he said. “But it requires much more testing.” He added that generalization may improve as SEAL is trained on a broader distribution of tasks.
Interestingly, the team found that only a few reinforcement learning steps already led to measurable performance gains. “This is exciting,” Jyo noted, “because it means that with more compute, we could hopefully get even more improvements.” He suggested future experiments could explore more advanced reinforcement learning methods beyond ReSTEM, such as Group Relative Policy Optimization (GRPO).
SEAL represents a step toward models that can autonomously improve over time, both by integrating new knowledge and by reconfiguring how they learn. The authors envision future extensions where SEAL could assist in self-pretraining, continual learning, and the development of agentic systems — models that interact with evolving environments and adapt incrementally.
In such settings, a model could use SEAL to synthesize weight updates after each interaction, gradually internalizing behaviors or insights. This could reduce the need for repeated supervision and manual intervention, particularly in data-constrained or specialized domains.
As public web text becomes saturated and further scaling of LLMs becomes bottlenecked by data availability, self-directed approaches like SEAL could play a critical role in pushing the boundaries of what LLMs can achieve.
You can access the SEAL project, including code and further documentation, at:https://jyopari.github.io/posts/seal"
Startup & Tech Media,Researchers find that retraining only small parts of AI models can cut costs and prevent forgetting.,https://venturebeat.com/ai/researchers-find-that-retraining-only-small-parts-of-ai-models-can-cut-costs,"Enterprises often find that whenthey fine-tune models, one effective approach to making a large language model (LLM) fit for purpose and grounded in data is to have the model lose some of its abilities. After fine-tuning, some models “forget” how to perform certain tasks or other tasks they already learned.
Research from the University of Illinois Urbana-Champaign proposes a new method for retraining models that avoids “catastrophic forgetting,” in which the model loses some of its prior knowledge. The paper focuses on two specific LLMs that generate responses from images: LLaVA and Qwen 2.5-VL.
The approach encourages enterprises to retrain only narrow parts of an LLM to avoid retraining the entire model and incurring a significant increase in compute costs. The team claims that catastrophic forgetting isn’t true memory loss, but rather a side effect of bias drift.
“Training a new LMM can cost millions of dollars, weeks of time, and emit hundreds of tons of CO2, so finding ways to more efficiently and effectively update existing models is a pressing concern,” the team wrote in thepaper. “Guided by this result, we explore tuning recipes that preserve learning while limiting output shift.”
The researchers focused on a multi-layer perceptron (MLP), the model's internal decision-making component.
The researchers wanted first to verify the existence and the cause of catastrophic forgetting in models.
To do this, they created a set of target tasks for the models to complete. The models were then fine-tuned and evaluated to determine whether they led to substantial forgetting. But as the process went on, the researchers found that the models were recovering some of their abilities.
“We also noticed a surprising result, that the model performance would drop significantly in held out benchmarks after training on the counting task, it would mostly recover on PathVQA, another specialized task that is not well represented in the benchmarks,” they said. “Meanwhile, while performing the forgetting mitigation experiments, we also tried separately tuning only the self-attention projection (SA Proj) or MLP layers, motivated by the finding that tuning only the LLM was generally better than tuning the full model. This led to another very surprising result – that tuning only self-attention projection layers led to very good learning of the target tasks with no drop in performance in held out tasks, even after training all five target tasks in a sequence.”
The researchers said they believe that “what looks like forgetting or interference after fine-tuning on a narrow target task is actually bias in the output distribution due to the task distribution shift.”
That finding turned out to be the key to the experiment. The researchers noted that tuning the MLP increases the likelihood of “outputting numeric tokens and a highly correlated drop in held out task accuracy.” What it showed is that a model forgetting some of its knowledge is only temporary and not a long-term matter.
“To avoid biasing the output distribution, we tune the MLP up/gating projections while keeping the down projection frozen, and find that it achieves similar learning to full MLP tuning with little forgetting,” the researchers said.
This allows for a more straightforward and more reproducible method for fine-tuning a model.
By focusing on a narrow segment of the model, rather than a wholesale retraining, enterprises can cut compute costs. It also allows better control of output drift.
However, the research focuses only on two models, specifically those dealing with vision and language. The researchers noted that due to limited resources, they are unable to try the experiment with other models.
Their findings, however, can be extended to other LLMs, especially for different modalities.
"
Startup & Tech Media,"This new AI technique creates ‘digital twin’ consumers, and it could kill the traditional survey industry.",https://venturebeat.com/ai/this-new-ai-technique-creates-digital-twin-consumers-and-it-could-kill-the,"A newresearch paperquietly published last week outlines a breakthrough method that allows large language models (LLMs) to simulate human consumer behavior with startling accuracy, a development that could reshape the multi-billion-dollarmarket research industry. The technique promises to create armies of synthetic consumers who can provide not just realistic product ratings, but also the qualitative reasoning behind them, at a scale and speed currently unattainable.
For years, companies have sought to use AI for market research, but have been stymied by a fundamental flaw: when asked to provide a numerical rating on a scale of 1 to 5, LLMs produce unrealistic and poorly distributed responses. A new paper, ""LLMs Reproduce Human Purchase Intent via Semantic Similarity Elicitation of Likert Ratings,"" submitted to the pre-print server arXiv on October 9th proposes an elegant solution that sidesteps this problem entirely.
The international team of researchers, led by Benjamin F. Maier, developed a method they callsemantic similarity rating (SSR). Instead of asking an LLM for a number, SSR prompts the model for a rich, textual opinion on a product. This text is then converted into a numerical vector — an ""embedding"" — and its similarity is measured against a set of pre-defined reference statements. For example, a response of ""I would absolutely buy this, it's exactly what I'm looking for"" would be semantically closer to the reference statement for a ""5"" rating than to the statement for a ""1.""
The results are striking. Tested against a massive real-world dataset from a leading personal care corporation — comprising 57 product surveys and 9,300 human responses — the SSR method achieved 90% of human test-retest reliability. Crucially, the distribution of AI-generated ratings was statistically almost indistinguishable from the human panel. The authors state, ""This framework enables scalable consumer research simulations while preserving traditional survey metrics and interpretability.""
This development arrives at a critical time, as the integrity of traditional online survey panels is increasingly under threat from AI. A 2024 analysis from theStanford Graduate School of Businesshighlighted a growing problem of human survey-takers using chatbots to generate their answers. These AI-generated responses were found to be ""suspiciously nice,"" overly verbose, and lacking the ""snark"" and authenticity of genuine human feedback, leading to what researchers called a ""homogenization"" of data that could mask serious issues like discrimination or product flaws.
Maier's research offers a starkly different approach: instead of fighting to purge contaminated data, it creates a controlled environment for generating high-fidelity synthetic data from the ground up.
""What we're seeing is a pivot from defense to offense,"" said one analyst not affiliated with the study. ""The Stanford paper showed the chaos of uncontrolled AI polluting human datasets. This new paper shows the order and utility of controlled AI creating its own datasets. For a Chief Data Officer, this is the difference between cleaning a contaminated well and tapping into a fresh spring.""
The technical validity of the new method hinges on the quality of the text embeddings, a concept explored in a 2022 paper inEPJ Data Science. That research argued for a rigorous ""construct validity"" framework to ensure that text embeddings — the numerical representations of text — truly ""measure what they are supposed to.""
The success of theSSR methodsuggests its embeddings effectively capture the nuances of purchase intent. For this new technique to be widely adopted, enterprises will need to be confident that the underlying models are not just generating plausible text, but are mapping that text to scores in a way that is robust and meaningful.
The approach also represents a significant leap from prior research, which has largely focused on using text embeddings to analyze and predict ratings from existing online reviews. A2022 study, for example, evaluated the performance of models like BERT and word2vec in predicting review scores on retail sites, finding that newer models like BERT performed better for general use. The new research moves beyond analyzing existing data to generating novel, predictive insights before a product even hits the market.
For technical decision-makers, the implications are profound. The ability to spin up a ""digital twin"" of a target consumer segment and test product concepts, ad copy, or packaging variations in a matter of hours could drastically accelerate innovation cycles.
As the paper notes, these synthetic respondents also provide ""rich qualitative feedback explaining their ratings,"" offering a treasure trove of data for product development that is both scalable and interpretable. While the era of human-only focus groups is far from over, this research provides the most compelling evidence yet that their synthetic counterparts are ready for business.
But the business case extends beyond speed and scale. Consider the economics: a traditional survey panel for a national product launch might cost tens of thousands of dollars and take weeks to field. An SSR-based simulation could deliver comparable insights in a fraction of the time, at a fraction of the cost, and with the ability to iterate instantly based on findings. For companies in fast-moving consumer goods categories — where the window between concept and shelf can determine market leadership — this velocity advantage could be decisive.
There are, of course, caveats. The method was validated on personal care products; its performance on complex B2B purchasing decisions, luxury goods, or culturally specific products remains unproven. And while the paper demonstrates that SSR can replicate aggregate human behavior, it does not claim to predict individual consumer choices. The technique works at the population level, not the person level — a distinction that matters greatly for applications like personalized marketing.
Yet even with these limitations, the research is a watershed. While the era of human-only focus groups is far from over, this paper provides the most compelling evidence yet that their synthetic counterparts are ready for business. The question is no longer whether AI can simulate consumer sentiment, but whether enterprises can move fast enough to capitalize on it before their competitors do."
Startup & Tech Media,Breaking the bottleneck: Why AI demands an SSD-first future.,https://venturebeat.com/ai/breaking-the-bottleneck-why-ai-demands-an-ssd-first-future,"Presented by Solidigm
As AI adoption surges, data centers face a critical bottleneck in storage — and traditional HDDs are at the center of it. Data that once sat idle as cold archives is now being pulled into frequent use to build more accurate models and deliver better inference results. This shift from cold data to warm data demands low-latency, high-throughput storage that can handle parallel computations. HDDs will remain the workhorse for low-cost cold storage, but without rethinking their role, the high-capacity storage layer risks becoming the weakest link in the AI factory.
""Modern AI workloads, combined with data center constraints, have created new challenges for HDDs,"" says Jeff Janukowicz, research vice president at IDC. ""While HDD suppliers are addressing data storage growth by offering larger drives, this often comes at the expense of slower performance. As a result, the concept of 'nearline SSDs' is becoming an increasingly relevant topic of discussion within the industry.""
Today, AI operators need to maximize GPU utilization, manage network-attached storage efficiently, and scale compute — all while cutting costs on increasingly scarce power and space. In an environment where every watt and every square inch counts, says Roger Corell, senior director of AI and leadership marketing at Solidigm, success requires more than a technical refresh. It calls for a deeper realignment.
“It speaks to the tectonic shift in the value of data for AI,” Corell says. “That’s where high-capacity SSDs come into play. Along with capacity, they bring performance and efficiency -- enabling exabyte-scale storage pipelines to keep pace with the relentless pace of data set size. All of that consumes power and space, so we need to do it as efficiently as possible to enable more GPU scale in this constrained environment.”
High-capacity SSDs aren’t just displacing HDDs — they’re removing one of the biggest bottlenecks on the AI factory floor. By delivering massive gains in performance, efficiency, and density, SSDs free up the power and space needed to push GPU scale further. It’s less a storage upgrade than a structural shift in how data infrastructure is designed for the AI era.
HDDs have impressive mechanical designs, but they're made up of many moving parts that at scale use more energy, take up more space, and fail at a higher rate than solid state drives. The reliance on spinning platters and mechanical read/write heads inherently limits Input/Output Operations Per Second (IOPS), creating bottlenecks for AI workloads that demand low latency, high concurrency, and sustained throughput.
HDDs also struggle with latency-sensitive tasks, as the physical act of seeking data introduces mechanical delays unsuited for real-time AI inference and training. Moreover, their power and cooling requirements increase significantly under frequent and intensive data access, reducing efficiency as data scales and warms.
In contrast, the SSD-based VAST storage solution reduces energy usage by ~$1M a year, and in an AI environment where every watt matters, this is a huge advantage for SSDs. To demonstrate, Solidigm and VAST Data completed a study examining the economics of data storage at exabyte scale — a quadrillion bytes, or a billion gigabytes, with an analysis of storage power consumption versus HDDs over a 10-year period.
As a starting reference point, you’d need four 30TB HDDs to equal the capacity of a single 122TB Solidigm SSD. After factoring in VAST’s data reduction techniques made possible by the superior performance of SSDs, the exabyte solution comprises 3,738 Solidigm SSDs vs over 40,000 high-capacity HDDs. The study found that the SSD-based VAST solution consumes 77% less storage energy.
""We’re shipping 122-terabyte drives to some of the top OEMs and leading AI cloud service providers in the world,"" Corell says. ""When you compare an all-122TB SSD to hybrid HDD + TLC SSD configuration, they're getting a nine-to-one savings in data center footprint. And yes, it’s important in these massive data centers that are building their own nuclear reactors and signing hefty power purchase agreements with renewable energy providers, but it’s increasingly important as you get to the regional data centers, the local data centers, and all the way out to your edge deployments where space can come at a premium.""
That nine-to-one savings goes beyond space and power — it lets organizations fit infrastructure into previously unavailable spaces, expand GPU scale, or build smaller footprints.
""If you’re given X amount of land and Y amount of power, you’re going to use it. You’re AI"" Corell explains, “where every watt and square inch counts, so why not use it in the most efficient way? Get the most efficient storage possible on the planet and enable greater GPU scale within that envelope that you have to fit in. On an ongoing basis, it’s going to save you operational cost as well. You have 90 percent fewer storage bays to maintain, and the cost associated with that is gone.""
Another often-overlooked element, the (much) larger physical footprint of data stored on mechanical HDDs results in a greater construction materials footprint. Collectively, concrete and steel production accounts for over 15% of global greenhouse gas emissions. By reducing the physical footprint of storage, high-capacity SSDs can help reduce embodied concrete and steel-based emissions by more than 80% compared to HDDs. And in the last phase of the sustainability life cycle, which is drive end-of-life, there will be 90% percent fewer drives to disposition. .
The move to SDD isn't just a storage upgrade; it's a fundamental realignment of data infrastructure strategy in the AI era, and it's picking up speed.
""Big hyperscalers are looking to wring the most out of their existing infrastructure, doing unnatural acts, if you will, with HDDs like overprovisioning them to near 90% to try to wring out as many IOPS per terabyte as possible, but they’re beginning to come around,"" Corell says. ""Once they turn to a modern all high-capacity storage infrastructure, the industry at large will be on that trajectory. Plus, we're starting to see these lessons learned on the value of modern storage in AI applied to other segments as well, such as big data analytics, HPC, and many more.""
While all-flash solutions are being embraced almost universally, there will always be a place for HDDs, he adds. HDDs will persist in usages like archival, cold storage, and scenarios where pure cost per gigabyte concerns outweigh the need for real-time access. But as the token economy heats up and enterprises realize value in monetizing data, the warm and warming data segments will continue to grow.
Now in its 4th generation, with more than 122 cumulative exabytes shipped to date, Solidigm’s QLC (Quad-Level Cell) technology has led the industry in balancing higher drive capacities with cost efficiency.
""We don’t think of storage as just storing bits and bytes. We think about how we can develop these amazing drives that are able to deliver benefits at a solution level,"" Corell says. ""The shining star on that is our recently launched, E1.S, designed specifically for dense and efficient storage in direct attach storage configurations for the next-generation fanless GPU server.""
The Solidigm D7-PS1010 E1.S is a breakthrough, the industry’s first eSSD with single-sided direct-to-chip liquid cooling technology. Solidigm worked with NVIDIA to address the dual challenges of heat management and cost efficiency, while delivering the high performance required for demanding AI workloads.
""We’re rapidly moving to an environment where all critical IT components will be direct-to-chip liquid-cooled on the direct attach side,"" he says. ""I think the market needs to be looking at their approach to cooling, because power limitations, power challenges are not going to abate in my lifetime, at least. They need to be applying a neocloud mindset to how they’re architecting the most efficient infrastructure.""
Increasingly complex inference is pushing against a memory wall, which makes storage architecture a front-line design challenge, not an afterthought. High-capacity SSDs, paired with liquid cooling and efficient design, are emerging as the only path to meet AI’s escalating demands. The mandate now is to build infrastructure not just for efficiency, but for storage that can efficiently scale as data grows. The organizations that realign storage now will be the ones able to scale AI tomorrow.
Sponsored articles are content produced by a company that is either paying for the post or has a business relationship with VentureBeat, and they’re always clearly marked. For more information, contactsales@venturebeat.com.
"
Startup & Tech Media,"We keep talking about AI agents, but do we ever know what they are?.",https://venturebeat.com/ai/we-keep-talking-about-ai-agents-but-do-we-ever-know-what-they-are,"Imagine you do two things on a Monday morning.
First, you ask a chatbot to summarize your new emails. Next, you ask an AI tool to figure out why your top competitor grew so fast last quarter. TheAI silently gets to work. It scours financial reports, news articles and social media sentiment. It cross-references that data with your internal sales numbers, drafts a strategy outlining three potential reasons for the competitor's success and schedules a 30-minute meeting with your team to present its findings.
We're calling both of these ""AI agents,"" but they represent worlds of difference in intelligence, capability and the level of trust we place in them. This ambiguity creates a fog that makes it difficult to build, evaluate, and safely govern these powerful new tools. If we can't agree on what we're building, how can we know when we've succeeded?
This post won't try to sell you on yet another definitive framework. Instead, think of it as a survey of the current landscape of agent autonomy, a map to help us all navigate the terrain together.
Before we can measure an agent's autonomy, we need to agree on what an ""agent"" actually is. The most widely accepted starting point comes from the foundational textbook on AI, Stuart Russell and Peter Norvig’s“Artificial Intelligence: A Modern Approach.”
They define an agent as anything that can be viewed as perceiving its environment through sensors and acting upon that environment through actuators. A thermostat is a simple agent: Its sensor perceives the room temperature, and its actuator acts by turning the heat on or off.
The ReAct model for AI agents (Credit: Confluent)
ReAct Model for AI Agents (Credit: Confluent)
That classic definition provides a solid mental model. For today's technology, we can translate it into four key components that make up a modern AI agent:
Perception(the ""senses""): This is how an agent takes in information about its digital or physical environment. It's the input stream that allows the agent to understand the current state of the world relevant to its task.
Reasoning engine(the ""brain""): This is the core logic that processes the perceptions and decides what to do next. For modern agents, this is typically powered by a large language model (LLM). The engine is responsible for planning, breaking down large goals into smaller steps, handling errors and choosing the right tools for the job.
Action(the ""hands""): This is how an agent affects its environment to move closer to its goal. The ability to take action via tools is what gives an agent its power.
Goal/objective: This is the overarching task or purpose that guides all of the agent's actions. It is the ""why"" that turns a collection of tools into a purposeful system. The goal can be simple (""Find the best price for this book"") or complex (""Launch the marketing campaign for our new product"")
Putting it all together, a true agent is a full-body system. The reasoning engine is the brain, but it’s useless without the senses (perception) to understand the world and the hands (actions) to change it. This complete system, all guided by a central goal, is what creates genuine agency.
With these components in mind, the distinction we made earlier becomes clear. A standard chatbot isn't a true agent. It perceives your question and acts by providing an answer, but it lacks an overarching goal and the ability to use external tools to accomplish it.
An agent, on the other hand, is software that has agency.
It has the capacity to act independently and dynamically toward a goal. And it's this capacity that makes a discussion about the levels of autonomy so important.
The dizzying pace of AI can make it feel like we're navigating uncharted territory. But when it comes to classifying autonomy, we’re not starting from scratch. Other industries have been working on this problem for decades, and their playbooks offer powerful lessons for the world ofAI agents.
The core challenge is always the same: How do you create a clear, shared language for the gradual handover of responsibility from a human to a machine?
Perhaps the most successful framework comes from the automotive industry. TheSAE J3016 standarddefines six levels of driving automation, from Level 0 (fully manual) to Level 5 (fully autonomous).
The SAE J3016 Levels of Driving Automation (Credit: SAE International)
The SAE J3016 Levels of Driving Automation (Credit: SAE International)
What makes this model so effective isn't its technical detail, but its focus on two simple concepts:
Dynamic driving task (DDT):This is everything involved in the real-time act of driving: steering, braking, accelerating and monitoring the road.
Operational design domain (ODD):These are the specific conditions under which the system is designed to work. For example, ""only on divided highways"" or ""only in clear weather during the daytime.""
The question for each level is simple: Who is doing the DDT, and what is the ODD?
At Level 2, the human must supervise at all times. At Level 3, the car handles the DDT within its ODD, but the human must be ready to take over. At Level 4, the car can handle everything within its ODD, and if it encounters a problem, it can safely pull over on its own.
The key insight for AI agents:A robust framework isn't about the sophistication of the AI ""brain."" It's about clearly defining the division of responsibility between human and machine under specific, well-defined conditions.
While the SAE’s six levels are great for broad classification, aviation offers a more granular model for systems designed for close human-machine collaboration. TheParasuraman, Sheridan, and Wickens modelproposes a detailed 10-level spectrum of automation.
Levels of Automation of Decision and Action Selection for Aviation (Credit: The MITRE Corporation)
Levels of Automation of Decision and Action Selection for Aviation (Credit: The MITRE Corporation)
This framework is less about full autonomy and more about the nuances of interaction. For example:
AtLevel 3, the computer ""narrows the selection down to a few"" for the human to choose from.
AtLevel 6, the computer ""allows the human a restricted time to veto before it executes"" an action.
AtLevel 9, the computer ""informs the human only if it, the computer, decides to.""
The key insight for AI agents:This model is perfect for describing the collaborative ""centaur"" systems we're seeing today. Most AI agents won't be fully autonomous (Level 10) but will exist somewhere on this spectrum, acting as a co-pilot that suggests, executes with approval or acts with a veto window.
Finally, the world of robotics brings in another critical dimension: context. The National Institute of Standards and Technology's (NIST)Autonomy Levels for Unmanned Systems (ALFUS) frameworkwas designed for systems like drones and industrial robots.
The Three-Axis Model for ALFUS (Credit: NIST)
The Three-Axis Model for ALFUS (Credit: NIST)
Its main contribution is adding context to the definition of autonomy, assessing it along three axes:
Human independence:How much human supervision is required?
Mission complexity:How difficult or unstructured is the task?
Environmental complexity:How predictable and stable is the environment in which the agent operates?
The key insight for AI agents:This framework reminds us that autonomy isn't a single number. An agent performing a simple task in a stable, predictable digital environment (like sorting files in a single folder) is fundamentally less autonomous than an agent performing a complex task across the chaotic, unpredictable environment of the open internet, even if the level of human supervision is the same.
Having looked at the lessons from automotive, aviation and robotics, we can now examine the emerging frameworks designed forAI agents. While the field is still new and no single standard has won out, most proposals fall into three distinct, but often overlapping, categories based on the primary question they seek to answer.
These frameworks classify agents based on their underlying technical architecture and what they are capable of achieving. They provide a roadmap for developers, outlining a progression of increasingly sophisticated technical milestones that often correspond directly to code patterns.
A prime example of this developer-centric approach comes from Hugging Face. Theirframeworkuses a star rating to show the gradual shift in control from human to AI:
Five Levels of AI Agent Autonomy, as proposed by HuggingFace (Credit: Hugging Face)
Five Levels of AI Agent Autonomy, as proposed by HuggingFace (Credit: Hugging Face)
Zero stars (simple processor):The AI has no impact on the program's flow. It simply processes information and its output is displayed, like a print statement. The human is in complete control.
One star (router):The AI makes a basic decision that directs program flow, like choosing between two predefined paths (if/else). The human still defineshoweverything is done.
Two stars (tool call):The AI chooses which predefined tool to use and what arguments to use with it. The human has defined the available tools, but the AI decides how to execute them.
Three stars (multi-step agent):The AI now controls the iteration loop. It decides which tool to use, when to use it and whether to continue working on the task.
Four stars (fully autonomous):The AI can generate and execute entirely new code to accomplish a goal, going beyond the predefined tools it was given.
Strengths:This model is excellent for engineers. It's concrete, maps directly to code and clearly benchmarks the transfer of executive control to the AI.
Weaknesses:It is highly technical and less intuitive for non-developers trying to understand an agent's real-world impact.
This second category defines autonomy not by the agent’s internal skills, but by the nature of its relationship with the human user. The central question is: Who is in control, and how do we collaborate?
This approach often mirrors the nuance we saw in the aviation models. For instance, a framework detailed in the paperLevels of Autonomy for AI Agentsdefines levels based on the user's role:
L1 - user as an operator:The human is in direct control (like a person using Photoshop with AI-assist features).
L4 - user as an approver:The agent proposes a full plan or action, and the human must give a simple ""yes"" or ""no"" before it proceeds.
L5 - user as an observer:The agent has full autonomy to pursue a goal and simply reports its progress and results back to the human.
Levels of Autonomy for AI Agents
Levels of Autonomy for AI Agents
Strengths:These frameworks are highly intuitive and user-centric. They directly address the critical issues of control, trust, and oversight.
Weaknesses:An agent with simple capabilities and one with highly advanced reasoning could both fall into the ""Approver"" level, so this approach can sometimes obscure the underlying technical sophistication.
The final category is less concerned with how an agent works and more with what happens when it fails. These frameworks are designed to help answer crucial questions about law, safety and ethics.
Think tanks like Germany's Stiftung Neue VTrantwortung haveanalyzedAI agents through the lens of legal liability. Their work aims to classify agents in a way that helps regulators determine who is responsible for an agent's actions: The user who deployed it, the developer who built it or the company that owns the platform it runs on?
This perspective is essential for navigating complex regulations like theEU's Artificial Intelligence Act, which will treat AI systems differently based on the level of risk they pose.
Strengths:This approach is absolutely essential for real-world deployment. It forces the difficult but necessary conversations about accountability that build public trust.
Weaknesses:It's more of a legal or policy guide than a technical roadmap for developers.
A comprehensive understanding requires looking at all three questions at once: An agent's capabilities, how we interact with it and who is responsible for the outcome..
Looking at the landscape of autonomy frameworks shows us that no  single model is sufficient because the true challenges lie in the gaps between them, in areas that are incredibly difficult to define and measure.
The SAE framework for self-driving cars gave us the powerful concept of an ODD, the specific conditions under which a system can operate safely. For a car, that might be ""divided highways, in clear weather, during the day."" This is a great solution for a physical environment, but what’s the ODD for a digital agent?
The ""road"" for an agent is the entire internet. An infinite, chaotic and constantly changing environment. Websites get redesigned overnight, APIs are deprecated and social norms in online communities shift.
How do we define a ""safe"" operational boundary for an agent that can browse websites, access databases and interact with third-party services? Answering this is one of the biggest unsolved problems. Without a clear digital ODD, we can't make the same safety guarantees that are becoming standard in the automotive world.
This is why, for now, the most effective and reliable agents operate within well-defined, closed-world scenarios. As I argued in a recentVentureBeat article, forgetting the open-world fantasies and focusing on ""bounded problems"" is the key to real-world success. This means defining a clear, limited set of tools, data sources and potential actions.
Today's agents are getting very good at executing straightforward plans. If you tell one to ""find the price of this item using Tool A, then book a meeting with Tool B,"" it can often succeed. But true autonomy requires much more.
Many systems today hit a technical wall when faced with tasks that require:
Long-term reasoning and planning:Agents struggle to create and adapt complex, multi-step plans in the face of uncertainty. They can follow a recipe, but they can't yet invent one from scratch when things go wrong.
Robust self-correction:What happens when an API call fails or a website returns an unexpected error? A truly autonomous agent needs the resilience to diagnose the problem, form a new hypothesis and try a different approach, all without a human stepping in.
Composability:The future likely involves not one agent, but a team of specialized agents working together. Getting them to collaborate reliably, to pass information back and forth, delegate tasks and resolve conflicts is a monumental software engineering challenge that we are just beginning to tackle.
This is the most critical challenge of all, because it's not just technical, it's deeply human. Alignment is the problem of ensuring an agent's goals and actions are consistent with our intentions and values, even when those values are complex, unstated or nuanced.
Imagine you give an agent the seemingly harmless goal of ""maximizing customer engagement for our new product."" The agent might correctly determine that the most effective strategy is to send a dozen notifications a day to every user. The agent has achieved its literal goal perfectly, but it has violated the unstated, common-sense goal of ""don't be incredibly annoying.""
This is a failure of alignment.
The core difficulty, which organizations like theAI Alignment Forumare dedicated to studying, is that it is incredibly hard to specify fuzzy, complex human preferences in the precise, literal language of code. As agents become more powerful, ensuring they are not just capable but also safe, predictable and aligned with our true intent becomes the most important challenge we face.
The path forward for AI agents is not a single leap to a god-like super-intelligence, but a more practical and collaborative journey. The immense challenges of open-world reasoning and perfect alignment mean that the future is a team effort.
We will see less of the single, all-powerful agent and more of an ""agentic mesh"" — a network of specialized agents, each operating within a bounded domain, working together to tackle complex problems.
More importantly, they will work with us. The most valuable and safest applications will keep a human on the loop, casting them as a co-pilot or strategist to augment our intellect with the speed of machine execution. This ""centaur"" model will be the most effective and responsible path forward.
The frameworks we've explored aren’t just theoretical. They’re practical tools for building trust, assigning responsibility and setting clear expectations. They help developers define limits and leaders shape vision, laying the groundwork for AI to become a dependable partner in our work and lives.
Sean Falconer isConfluent'sAI entrepreneur in residence."
